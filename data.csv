answer
"
 Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.
 AI technology is widely used throughout industry, government, and science. Some high-profile applications include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go).[2] However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[3][4]
 Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.[5] Artificial intelligence was founded as an academic discipline in 1956.[6] The field went through multiple cycles of optimism,[7][8] followed by periods of disappointment and loss of funding, known as AI winter.[9][10] Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques,[11] and after 2017 with the transformer architecture.[12] This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.[13]
 The growing use of artificial intelligence in the 21st century is influencing a societal and economic shift towards increased automation, data-driven decision-making, and the integration of AI systems into various economic sectors and areas of life, impacting job markets, healthcare, government, industry, and education. This raises questions about the long-term effects, ethical implications, and risks of AI, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. 
 The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.[a] General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field's long-term goals.[14]
 To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[15]
 The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]
 Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[16] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[17]
 Many of these algorithms are insufficient for solving large reasoning problems because they experience a ""combinatorial explosion"": They become exponentially slower as the problems grow.[18] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[19] Accurate and efficient reasoning is an unsolved problem.
 Knowledge representation and knowledge engineering[20] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[21] scene interpretation,[22] clinical decision support,[23] knowledge discovery (mining ""interesting"" and actionable inferences from large databases),[24] and other areas.[25]
 A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[26] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[27] situations, events, states, and time;[28] causes and effects;[29] knowledge about knowledge (what we know about what other people know);[30] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[31] and many other aspects and domains of knowledge.
 Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[32] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).[19] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]
 An ""agent"" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][35] In automated planning, the agent has a specific goal.[36] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the ""utility"") that measures how much the agent prefers it. For each possible action, it can calculate the ""expected utility"": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[37]
 In classical planning, the agent knows exactly what the effect of any action will be.[38] In most real-world problems, however, the agent may not be certain about the situation they are in (it is ""unknown"" or ""unobservable"") and it may not know for certain what will happen after each possible action (it is not ""deterministic""). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[39]
 In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[40] Information value theory can be used to weigh the value of exploratory or experimental actions.[41] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
 A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[42]
 Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[43]
 Machine learning is the study of programs that can improve their performance on a given task automatically.[44] It has been a part of AI from the beginning.[e]
 There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[47] Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[48]
 In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as ""good"".[49] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[50] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[51]
 Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[52]
 Natural language processing (NLP)[53] allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[54]
 Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called ""micro-worlds"" (due to the common sense knowledge problem[32]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
 Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[55] transformers (a deep learning architecture using an attention mechanism),[56] and others.[57] In 2019, generative pre-trained transformer (or ""GPT"") language models began to generate coherent text,[58][59] and by 2023 these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[60]
 Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[61]
 The field includes speech recognition,[62] image classification,[63] facial recognition, object recognition,[64] and robotic perception.[65]
 Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[67] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
 However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[68] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.[69]
 A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[14]
 AI research uses a wide variety of techniques to accomplish the goals above.[b]
 AI can solve many problems by intelligently searching through many possible solutions.[70] There are two very different kinds of search used in AI: state space search and local search.
 State space search searches through a tree of possible states to try to find a goal state.[71] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[72]
 Simple exhaustive searches[73] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[18] ""Heuristics"" or ""rules of thumb"" can help prioritize choices that are more likely to reach a goal.[74]
 Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.[75]
 Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[76]
 Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.[77]
 Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by ""mutating"" and ""recombining"" them, selecting only the fittest to survive each generation.[78]
 Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[79]
 Formal logic is used for reasoning and knowledge representation.[80]
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as ""and"", ""or"", ""not"" and ""implies"")[81] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as ""Every X is a Y"" and ""There are some Xs that are Ys"").[82]
 Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[83] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
 Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[84] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[85]
 Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[86]
 Fuzzy logic assigns a ""degree of truth"" between 0 and 1. It can therefore handle propositions that are vague and partially true.[87]
 Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[31]
Other specialized versions of logic have been developed to describe many complex domains.
 Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[88] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[89] and information value theory.[90] These tools include models such as Markov decision processes,[91] dynamic decision networks,[92] game theory and mechanism design.[93]
 Bayesian networks[94] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][96] learning (using the expectation-maximization algorithm),[h][98] planning (using decision networks)[99] and perception (using dynamic Bayesian networks).[92]
 
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[92] The simplest AI applications can be divided into two types: classifiers (e.g., ""if shiny then diamond""), on one hand, and controllers (e.g., ""if diamond then pick up""), on the other hand. Classifiers[100] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an ""observation"") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[48]
 There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]
The naive Bayes classifier is reportedly the ""most widely used learner""[103] at Google, due in part to its scalability.[104]
Neural networks are also used as classifiers.[105]
 An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]
 Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106]
Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]
 In feedforward neural networks the signal passes in only one direction.[108] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.[109]
Perceptrons[110]
use only a single layer of neurons, deep learning[111] uses multiple layers.
Convolutional neural networks strengthen the connection between neurons that are ""close"" to each other—this is especially important in image processing, where a local set of neurons must identify an ""edge"" before the network can identify an object.[112]
 Deep learning[111]
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[113]
 Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[114] and others. The reason that deep learning performs so well in so many applications is not known as of 2023.[115]
The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i]
but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]
 Generative pre-trained transformers (GPT) are large language models that are based on the semantic relationships between words in sentences (natural language processing). Text-based GPT models are pre-trained on a large corpus of text which can be from the internet. The pre-training consists in predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pre-training, GPT models accumulate knowledge about the world, and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are still prone to generating falsehoods called ""hallucinations"", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow you to ask a question or request a task in simple text.[124][125]
 Current models and services include: Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot and LLaMA.[126] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[127]
 In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[128] Historically, specialized languages, such as Lisp, Prolog, Python and others, had been used.
 AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok).
 The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[129] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.
 For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[130] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[130] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[131] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[132] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[133][134]
 Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[135] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[136] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[137] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then in 2017 it defeated Ke Jie, who was the best Go player in the world.[138] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[139] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[140] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[141] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[142]
 Various countries are deploying AI military applications.[143] The main applications enhance command and control, communications, sensors, integration and interoperability.[144] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[143] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[144] AI was incorporated into military operations in Iraq and Syria.[143]
 In November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.[145]
 In the early 2020s, generative AI gained widespread prominence. In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it.[146] The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.[147][148]
 There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated ""AI"" in some offerings or processes.[149] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
 In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
 Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
 AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to ""solve intelligence, and then use that to solve everything else"".[150] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[151] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[152]
 Machine-learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
 Technology companies collect a wide range of data from their users, including online activity, geolocation data, video and audio.[153]
For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[154] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[155]
 AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[156] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted ""from the question of 'what they know' to the question of 'what they're doing with it'.""[157]
 Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of ""fair use"". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".[158][159] Website owners who do not wish to have their content scraped can indicate it in a ""robots.txt"" file.[160] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[161][162] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[163]
 YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[164] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[165] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem.
 In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[166] AI pioneer Geoffrey Hinton expressed concern about AI enabling ""authoritarian leaders to manipulate their electorates"" on a large scale, among other risks.[167]
 Machine learning applications will be biased if they learn from biased data.[168] The developers may not be aware that the bias exists.[169]
Bias can be introduced by the way training data is selected and by the way a model is deployed.[170][168] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[171]
Fairness in machine learning is the study of how to prevent the harm caused by algorithmic bias. It has become serious area of academic study within AI. Researchers have discovered it is not always possible to define ""fairness"" in a way that satisfies all stakeholders.[172]
 On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as ""gorillas"" because they were black. The system was trained on a dataset that contained very few images of black people,[173] a problem called ""sample size disparity"".[174] Google ""fixed"" this problem by preventing the system from labelling anything as a ""gorilla"". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[175]
 COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.
In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[176] In 2017, several researchers[k] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[178]
 A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as ""race"" or ""gender""). The feature will correlate with other features (like ""address"", ""shopping history"" or ""first name""), and the program will make the same decisions based on these features as it would on ""race"" or ""gender"".[179]
Moritz Hardt said ""the most robust fact in this research area is that fairness through blindness doesn't work.""[180]
 Criticism of COMPAS highlighted that machine learning models are designed to make ""predictions"" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these ""recommendations"" will likely be racist.[181] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is necessarily descriptive and not proscriptive.[l]
 Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[174]
 At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[183]
 Many AI systems are so complex that their designers cannot explain how they reach their decisions.[184] Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[185]
 It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as ""cancerous"", because pictures of malignancies typically include a ruler to show the scale.[186] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at ""low risk"" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[187]
 People who have been harmed by an algorithm's decision have a right to an explanation.[188] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[m] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[189]
 DARPA established the XAI (""Explainable Artificial Intelligence"") program in 2014 to try and solve these problems.[190]
 There are several possible solutions to the transparency problem. SHAP tried to solve the transparency problems by visualising the contribution of each feature to the output.[191] LIME can locally approximate a model with a simpler, interpretable model.[192] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[193] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network have learned and produce output that can suggest what the network is learning.[194]
 Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
 A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[n] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[196] Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person.[196] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[197] By 2015, over fifty countries were reported to be researching battlefield robots.[198]
 AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[199] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[200][201]
 There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[202]
 Training AI systems requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller startups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.[203]
 Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[204]
 In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.[205] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[206] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"".[o][208] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[204] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[209][210]
 Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".[211] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[212]
 From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[213]
 It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, ""spell the end of the human race"".[214] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like ""self-awareness"" (or ""sentience"" or ""consciousness"") and becomes a malevolent character.[p] These sci-fi scenarios are misleading in several ways.
 First, AI does not require human-like ""sentience"" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager).[216] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that ""you can't fetch the coffee if you're dead.""[217] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"".[218]
 Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[219]
 The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[220] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk have expressed concern about existential risk from AI.[221]
AI pioneers including Fei-Fei Li, Geoffrey Hinton, Yoshua Bengio, Cynthia Breazeal, Rana el Kaliouby, Demis Hassabis, Joy Buolamwini, and Sam Altman have expressed concerns about the risks of AI. In 2023, many leading AI experts issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"".[222]
 Other researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making ""human lives longer and healthier and easier.""[223] While the tools that are now being used to improve lives can also be used by bad actors, ""they can also be used against the bad actors.""[224][225] Andrew Ng also argued that ""it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.""[226] Yann LeCun ""scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.""[227] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[228] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[229]
 Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[230]
 Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[231]
The field of machine ethics is also called computational morality,[231]
and was founded at an AAAI symposium in 2005.[232]
 Other approaches include Wendell Wallach's ""artificial moral agents""[233] and Stuart J. Russell's three principles for developing provably beneficial machines.[234]
 Active organizations in the AI open-source community include Hugging Face,[235] Google,[236] EleutherAI and Meta.[237] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[238][239] meaning that their architecture and trained parameters (the ""weights"") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[240] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism), and that once released on the Internet, they can't be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[241]
 Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:[242][243]
 Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[244] however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.[245]
 Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[246]
 The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[247] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[248] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[249][250] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[251] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[251] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[251] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[252] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[253] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[254]
 In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"".[249] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[255] In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".[256][257]
 In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[258] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[259][260]
 The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable form of mathematical reasoning.[261][5] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an ""electronic brain"".[q] 
They developed several areas of research that would become part of AI,[263]
such as McCullouch and Pitts design for ""artificial neurons"" in 1943,[264] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that ""machine intelligence"" was plausible.[265][5]
 The field of AI research was founded at a workshop at Dartmouth College in 1956.[r][6] The attendees became the leaders of AI research in the 1960s.[s] They and their students produced programs that the press described as ""astonishing"":[t] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[u][7] Artificial intelligence laboratories were set up at a number of British and U.S. Universities in the latter 1950s and early 1960s.[5]
 Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[269] Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"".[270] Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".[271] They had, however, underestimated the difficulty of the problem.[v] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[273] and ongoing pressure from the U.S. Congress to fund more productive projects.[274] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[275] The ""AI winter"", a period when obtaining funding for AI projects was difficult, followed.[9]
 In the early 1980s, AI research was revived by the commercial success of expert systems,[276] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]
 Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[277] and began to look into ""sub-symbolic"" approaches.[278] Rodney Brooks rejected ""representation"" in general and focussed directly on engineering machines that move and survive.[w] Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[88][283] But the most important development was the revival of ""connectionism"", including neural network research, by Geoffrey Hinton and others.[284] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[285]
 AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[286] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".[287]
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.[14]
 Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]
For many specific tasks, other methods were abandoned.[x]
Deep learning's success was based on both hardware improvements (faster computers,[289] graphics processing units, cloud computing[290]) and access to large amounts of data[291] (including curated datasets,[290] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[y] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[251]
 In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[229]
 In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[292] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in ""AI"" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in ""AI"".[293]
About 800,000 ""AI""-related U.S. job openings existed in 2022.[294]
 Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""[295] He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".[295] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[265] Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks""[296]
 Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'""[297] AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".[298]
 McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world"".[299] Another AI founder, Marvin Minsky similarly describes it as ""the ability to solve hard problems"".[300] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
 Another definition has been adopted by Google,[301] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
 No established unifying theory or paradigm has guided AI research for most of its history.[z] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.
 Symbolic AI (or ""GOFAI"")[303] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""[304]
 However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.[305] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.[306] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[aa][19]
 The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[308][309] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
 ""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[310] but eventually was seen as irrelevant. Modern AI has elements of both.
 Finding a provably correct or optimal solution is intractable for many important problems.[18] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
 AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[311][312] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.
 The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that ""[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.""[313] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.
 David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.[314] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[315]
 Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[316]
 Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""[ab] Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[320]
 It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[321] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[322][323] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[322] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[324]
 In 2017, the European Union considered granting ""electronic personhood"" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[325] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[326][327]
 Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[323][322]
 A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[312]
 If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an ""intelligence explosion"" and Vernor Vinge called a ""singularity"".[328]
 However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[329]
 Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[330]
 Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[331]
 Thought-capable artificial beings have appeared as storytelling devices since antiquity,[332] and have been a persistent theme in science fiction.[333]
 A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[334]
 Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[335] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[336]
 Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[337]
 The two most widely used textbooks in 2023. (See the Open Syllabus).
 These were the four of the most widely used AI textbooks in 2008:
 Later editions.
"
"Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge, in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning.
 Examples of knowledge representation formalisms include semantic nets, frames,  rules,  logic programs and ontologies. Examples of automated reasoning engines include inference engines, theorem provers,  model generators and classifiers.
 The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959 and the Advice Taker proposed by John McCarthy also in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of the predicate calculus to represent common sense reasoning.
 Many of the early approaches to knowledge represention in AI used graph representations and semantic networks, similar to knowledge graphs today. In such approaches, problem solving was a form of graph traversal[2] or path-finding, as in the A* search algorithm. Typical applications included robot plan-formation and game-playing.
 Other researchers focused on developing  automated theorem-provers for first-order logic, motivated by the use of mathematical logic to formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of the resolution method by John Alan Robinson.
 In the meanwhile, John McCarthy and Pat Hayes developed the situation calculus as a logical representation of common sense knowledge about the laws of cause and effect. Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution for question-answering and automatic programming.[3]
 In contrast, researchers at MIT rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[4] The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development of logic programming and Prolog, using SLD resolution to treat Horn clauses as goal-reduction procedures.
 The early development of logic programming was largely a European phenomenon. In North America, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth advocated the representation of domain-specific knowledge rather than general-purpose reasoning.[5]
 These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[6]
 Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, which includes facts and rules about a problem domain, and an inference engine, which applies the knowledge in the knowledge base to answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[7]
 Meanwhile, Marvin Minsky developed the concept of frame in the mid-1970s.[8] A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.
 It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.[9]
 The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed] One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation.[10] KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[11]
 Another area of knowledge representation research was the problem of common-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent. Basic principles of common-sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[12] In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[13]
 The starting point for knowledge representation is the knowledge representation hypothesis first formalized by Brian C. Smith in 1985:[14]
 Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge. One of the most active areas of knowledge representation research is the Semantic Web.[citation needed] The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.
 Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.[15][16]
 Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems.
 The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems.
 For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.
 Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.[17]
 A key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[18] First Order Logic (FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of  knowledge representation languages.
 Arguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers.
 One of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[19] In particular, this balancing act was a driving motivation for the development of IF-THEN rules in rule-based expert systems. 
 A similar balancing act was also a motivation for the development of  logic programming (LP) and the logic programming language Prolog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax of production rules. But logic programs have a well-defined logical semantics, whereas production systems do not.
 The earliest form of logic programming was based on the Horn clause subset of FOL. But later extensions of LP included the negation as failure inference rule, which turns LP into a non-monotonic logic for default reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics, [20] which includes the unique name assumption and a form of closed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL.
 In a key 1993 paper on the topic, Randall Davis of MIT outlined five distinct roles to analyze a knowledge representation framework:[21]
 Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[15] The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[22]
 The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.[23]
 In 1985,  Ron Brachman categorized the core issues for knowledge representation as follows:[24]
 In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.
 As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL.
 After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, ""Every ontology is a treaty- a social agreement among people with common motive in sharing."" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[28]
 There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[29] the lumped element model widely used in representing electronic circuits (e.g.,[30]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.
 The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.
 Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.
 The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.
"
"
 Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]
 ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.
 The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[7][8]
 From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.
 The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[9][10] The synonym self-teaching computers was also used in this time period.[11][12]
 Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[13] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[14] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[13] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[13]
 By the early 1960s an experimental ""learning machine"" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively ""trained"" by a human operator/teacher to recognize patterns and equipped with a ""goof"" button to cause it to re-evaluate incorrect decisions.[15] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[16] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[17] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[18]
 Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.""[19] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".[20]
 Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[21]
 As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488 
 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[24]: 25 
 Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[25]
 There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ""general intelligence"".[26][27][28]
 An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[29]
 According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.
 Examples of AI-powered audio/video compression software include VP9, NVIDIA Maxine, AIVC, AccMPEG.[30] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[31]
 In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[32]
 Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[33]
 Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
 Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).[35]
 The difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.
 Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[36] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[37] He also suggested the term data science as a placeholder to call the overall field.[37]
 Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[38]
 Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[39] wherein ""algorithmic model"" means more or less the machine learning algorithms like Random Forest.
 Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[40]
 Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks.[41] Statistical physics is thus finding applications in the area of medical diagnostics.[42]
 A core objective of a learner is to generalize from its experience.[6][43] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
 The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
 For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[44]
 In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.
 
Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the ""signal"" or ""feedback"" available to the learning system:
 Although each algorithm has advantages and limitations, no single algorithm works for all problems.[45][46][47]
 Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[48] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[49] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[19]
 Types of supervised-learning algorithms include active learning, classification and regression.[50] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.
 Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.
 Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[8] and density estimation.[51] Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.[52]
 Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.
 Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
 In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54]
 Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.
 Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[56] In other words, it is a process of reducing the dimension of the feature set, also called the ""number of features"". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). This results in a smaller dimension of data (2D instead of 3D), while keeping all original variables in the model without changing the data.[57]
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.
 Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[58]
 Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[59] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[60]
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 
 It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[61]
 Several learning algorithms aim at discovering better representations of the inputs provided during training.[62] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
 Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[63] and various forms of clustering.[64][65][66]
 Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[67] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[68]
 Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
 Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[69] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[70]
 In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[71] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[72]
 In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[73]
 Three broad categories of anomaly detection techniques exist.[74] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.
 Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[75][76] and finally meta-learning (e.g. MAML).
 Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of ""interestingness"".[77]
 Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves ""rules"" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[78] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
 Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[79] For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
 Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[80]
 Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
 Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[81][82][83] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[84] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.
 A machine learning model is a type of mathematical model which, after being ""trained"" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions.[85] By extension the term model can refer to several level of specifity, from a general class of models and their associated learning algorithms, to a fully trained model with all its internal parameters tuned.[86]
 Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.
 Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
 An ANN is a model based on a collection of connected units or nodes called ""artificial neurons"", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a ""signal"", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called ""edges"". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
 The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
 Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[87]
 Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.
 Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[88]  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
 Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[89]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.
 A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
 A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
 Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.
 Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.
 A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[91][92] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[93]
 The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach[clarification needed] would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and Uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[3][5][10] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead a much higher computation time when compared to other machine learning approaches.
 Typically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Bias models may result in detrimental outcomes thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably be integrated within machine learning engineering teams.
 Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[94]
 There are many applications for machine learning, including:
 In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[97] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly.[98] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[99] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[100] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[101] In 2019 Springer Nature published the first research book created using machine learning.[102] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[103] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[104] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[105][106][107] When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[108]
 Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[109]
 Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[110][111][112] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[113]
 The ""black box theory"" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[114] The House of Lords Select Committee, which claimed that such an “intelligence system” that could have a “substantial impact on an individual’s life” would not be considered acceptable unless it provided “a full and satisfactory explanation for the decisions” it makes.[114]
 In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[115] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[116][117] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[118]
 Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[119]
 Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[120]
 Language models learned from data have been shown to contain human-like biases.[121][122] In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged ""black defendants high risk twice as often as white defendants.""[123] In 2015, Google Photos would often tag black people as gorillas,[123] and in 2018, this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data and thus was not able to recognize real gorillas at all.[124] Similar issues with recognizing non-white people have been found in many other systems.[125] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[126]
 Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[127] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that ""[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.""[128]
 Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[129] It contrasts with the ""black box"" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[130] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.
 Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[131]
 Learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[132] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in ""adversarial"" images that the system misclassifies.[133][134]
 Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[135] Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.[136]
 Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories ""spam"" and well-visible ""not spam"" of posts) machine learning models which are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[137][138][139]
 Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[140]
 In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).[141]
 Machine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[142] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to be either women or had non-European sounding names.[120] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[143][144] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in “disproportionately high levels of over-policing in low-income and minority communities” after being trained with historical crime data.[123]
 While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[145] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, “female faculty merely make up 16.1%” of all faculty members who focus on AI among several universities around the world.[146] Furthermore, among the group of “new U.S. resident AI PhD graduates,” 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[146]
 AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning.[147] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[148][149]
 Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines.[150] This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[151]
 Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units.[152] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[153] OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[154][155]
 A physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. ""Physical"" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[156][157]
 Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers.[158][159][160] Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration,[161][162] using approximate computing,[163] optimization of machine learning models and many more.[164][165] Pruning, Quantization, Knowledge Distillation, Low-Rank Factorization, Network Architecture Search (NAS) & Parameter Sharing are few of the techniques used for optimization of machine learning models.
 Software suites containing a variety of machine learning algorithms include the following:
"
"Soft computing is an umbrella term used to describe types of algorithms that produce approximate solutions to unsolvable high-level problems in computer science. Typically, traditional hard-computing algorithms heavily rely on concrete data and mathematical models to produce solutions to problems. Soft computing was coined in the late 20th century.[1] During this period, revolutionary research in three fields greatly impacted soft computing. Fuzzy logic is a computational paradigm that entertains the uncertainties in data by using levels of truth rather than rigid 0s and 1s in binary. Next, neural networks which are computational models influenced by human brain functions. Finally, evolutionary computation is a term to describe groups of algorithm that mimic natural processes such as evolution and natural selection.
 In the context of artificial intelligence and machine learning, soft computing provides tools to handle real-world uncertainties. Its methods supplement preexisting methods for better solutions. Today, the combination with artificial intelligence has led to hybrid intelligence systems that merge various computational algorithms. Expanding the applications of artificial intelligence, soft computing leads to robust solutions. Key points include tackling ambiguity, flexible learning, grasping intricate data, real-world applications, and ethical artificial intelligence.[2][3]
 The development of soft computing dates back to the late 20th century. In 1965, Lotfi Zadeh introduced fuzzy logic, which laid the mathematical groundwork for soft computing. Between the 1960s and 1970s, evolutionary computation, the development of genetic algorithms that mimicked biological processes, began to emerge. These models carved the path for models to start handling uncertainty. Although neural network research began in the 1940s and 1950s, there was a new demand for research in the 1980s. Researchers invested time to develop models for pattern recognition. Between the 1980s and 1990s, hybrid intelligence systems merged fuzzy logic, neural networks, and evolutionary computation that solved complicated problems quickly. From the 1990s to the present day, Models have been instrumental and affect multiple fields handling big data, including engineering, medicine, social sciences, and finance.[4][5]
 Fuzzy logic is an aspect of computing that handles approximate reasoning. Typically, binary logic allows computers to make decisions on true or false reasons (0s and 1s); however, introducing fuzzy logic allows systems to handle the unknowns between 0 and 1.[2][6]
 Unlike classical sets that allow members to be entirely within or out, fuzzy sets allow partial membership by incorporating ""graduation"" between sets. Fuzzy logic operations include negation, conjunction, and disjunction, which handle membership between data sets.[5]
 Fuzzy rules are logical statements that map the correlation between input and output parameters. They set the rules needed to trace variable relationships linguistically, and they would not be possible without linguistic variables. Linguistic variables represent values typically not quantifiable, allowing uncertainties.[7]
 Neural networks are computational models that attempt to mimic the structure and functioning of the human brain. While computers typically use binary logic to solve problems, neural networks attempt to provide solutions for complicated problems by enabling systems to think human-like, which is essential to soft computing.[8]
 Neural networks revolve around perceptrons, which are artificial neurons structured in layers. Like the human brain, these interconnected nodes process information using complicated mathematical operations.[9]
 Through training, the network handles input and output data streams and adjusts parameters according to the provided information. Neural networks help make soft computing extraordinarily flexible and capable of handling high-level problems.
 In soft computing, neural networks aid in pattern recognition, predictive modeling, and data analysis. They are also used in image recognition, natural language processing, speech recognition, and systems.[3][10]
 Evolutionary computation is a field in soft computing that uses the principles of natural selection and evolution to solve complicated problems. It promotes the discovery of diverse solutions within a solution space, encouraging near-perfect solutions. It finds satisfactory solutions by using computational models and types of evolutionary algorithms. Evolutionary computation consists of algorithms that mimic natural selection, such as genetic algorithms, genetic programming, and evolutionary programming. These algorithms use crossover, mutation, and selection.[11]
 Crossover, or recombination, exchanges data between nodes to diversify data and handle more outcomes. Mutation is a genetic technique that helps prevent the premature conclusion to a suboptimal solution by diversifying an entire range of solutions. It helps new optimal solutions in solution sets that help the overall optimization process. Selection is an operator that chooses which solution from a current population fits enough to transition to the next phase. These drive genetic programming to find optimal solutions by ensuring the survival of only the fittest solutions in a set.
 In soft computing, evolutionary computation helps applications of data mining (using large sets of data to find patterns), robotics, optimizing, and engineering methods.[3][5]
 Hybrid intelligence systems combine the strengths of soft computing components to create integrated computational models. Artificial techniques such as fuzzy logic, neural networks, and evolutionary computation combine to solve problems efficiently. These systems improve judgment, troubleshooting, and data analysis. Hybrid intelligence systems help overcome the limitations of individual AI approaches to improve performance, accuracy, and adaptability to address dynamic problems. It advances soft computing capabilities in data analysis, pattern recognition, and systems.[12]
 Due to their dynamic versatility, soft computing models are precious tools that confront complex real-world problems. They are applicable in numerous industries and research fields:
 Soft computing fuzzy logic and neural networks help with pattern recognition, image processing, and computer vision. Its versatility is vital in natural language processing as it helps decipher human emotions and language. They also aid in data mining and predictive analysis by obtaining priceless insights from enormous datasets. Soft computing helps optimize solutions from energy, financial forecasts, environmental and biological data modeling, and anything that deals with or requires models.[12][13]
 Within the medical field, soft computing is revolutionizing disease detection, creating plans to treat patients and models of healthcare.[10]
 Soft computing methods such as neural networks and fuzzy models are complicated and may need clarification. Sometimes, it takes effort to understand the logic behind neural network algorithms' decisions, making it challenging for a user to adopt them. In addition, it takes valuable, costly resources to feed models extensive data sets, and sometimes it is impossible to acquire the computational resources necessary. There are also significant hardware limitations which limits the computational power.[8]
 Furthermore, there needs to be more backing behind soft computing algorithms, which makes them less reliable than complicated computing models. Finally, there is a considerable potential for bias because of the input data, which leads to ethical dilemmas if methods are in fields such as medicine, finance, and healthcare.
"
"
 Weak artificial intelligence (weak AI) is artificial intelligence that implements a limited part of the mind, or, as narrow AI,[1][2][3] is focused on one narrow task. 
 In John Searle's terms it “would be useful for testing hypotheses about minds, but would not be minds”.[4] Weak AI focuses on mimicking how humans perform[dubious  – discuss] basic actions such as remembering things, perceiving things, and solving simple problems.[5] As opposed to strong AI, which uses technology to be able to think and learn on its own. Computers can use methods such as algorithms and prior knowledge to develop their ways of thinking as human beings do.[5] Strong AI systems are learning how to run independently of the programmers who programmed them. Weak AI is not able to have a mind of its own, and can only imitate physical behaviors that it can observe.[dubious  – discuss][6]
 Weak AI is contrasted with strong AI, which is defined variously as: 
 Scholars like Antonio Lieto have argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak-AI hypothesis (that should not be confused with the ""general"" vs ""narrow"" AI distinction) and that the popular assumption that cognitively inspired AI systems espouse the strong AI hypothesis is ill-posed and problematic since ""artificial models of brain and mind can be used to understand mental phenomena without pretending that that they are the real phenomena that they are modelling""[7] (as, on the other hand, implied by the strong AI assumption).
 Narrow AI can be classified as being “... limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.”[8] Narrow means the robot or computer is strictly limited to only being able to solve one problem at a time. Strong AI is conversely the opposite. Strong AI is closer to the human brain. This is all believed to be the case by philosopher John Searle. This idea of strong AI is also controversial. Searle believes that the Turing test (created by Alan Turing during WW2, originally called the Imitation Game, used to test if a machine is as intelligent as a human) is not accurate or appropriate for testing strong AI.[9]
 The differences between weak AI vs. strong AI are not widely cataloged out there at the moment. Weak AI is commonly associated with basic technology like voice-recognition software such as Siri or Alexa as mentioned in Terminology. Whereas strong AI is not fully implemented or testable yet, it is only really fantasized about in movies or popular culture media.[10]
 It seems that one approach to AI moving forward is one of an assisting or aiding role to humans. There are some sets of data or numbers that even we humans cannot fully process or understand as quickly as computers can, so this is where AI will play a helping role for us.[11][relevant?]
 Some commentators[who?] think narrow AI could be dangerous because of this ""brittleness"" and fail in unpredictable ways. Narrow AI could cause disruptions in the electric grid, damage nuclear power plants, cause global economic problems, and misdirect autonomous vehicles.[1]
 Some examples of narrow AI are AlphaGo,[12] self-driving cars, robot systems used in the medical field, and diagnostic doctors. Narrow AI systems are sometimes dangerous if unreliable. Medicines could be incorrectly sorted and distributed. Also, medical diagnoses can ultimately have serious and sometimes deadly consequences if the AI is faulty or biased.[13] Another issue with narrow AI, currently, is that behavior that it follows can become inconsistent.[14] It could be difficult for the AI to grasp complex patterns and get to a solution that works reliably in various environments.
 Simple AI programs have already worked their way into our society and we just might not have noticed it yet. Autocorrection for typing, speech recognition for speech-to-text programs, and vast expansions in the data science fields are just to name a few.[15] As much as narrow and relatively general AI is slowly starting to help out societies, they are also starting to hurt them as well. AI had already unfairly put people in jail, discriminated against women in the workplace for hiring, taught some problematic ideas to millions, and even killed people with automatic cars.[16] AI might be a powerful tool that can be used for improving our lives, but it could also be a dangerous technology with the potential for things to get out of hand.  
 Facebook, and other similar social media platforms, have been able to figure out how to use AI and machine learning, or more specifically narrow AI, to predict how people will react to being shown certain images. Narrow AI systems have been able to identify what users will engage with, based on what they post, following the patterns or trends.[17]
 Twitter has started to have more advanced AI systems to figure out how to identify narrower AI forms and detect if bots may have been used for biased propaganda, or even potentially malicious intentions. These AI systems do this through filtering words and creating different layers of conditions based on what AI has had implications for in the past, and then detecting if that account may be a bot or not.[18]
 TikTok uses its ""For You"" algorithm to determine a user's interests very quickly through analyzing patterns in what videos the user initially chooses to watch. This narrow AI system uses patterns found between videos to determine what video should be shown next including the duration, who has shared or commented on it already, and music played in the videos. The ""For You"" algorithm on TikTok is so accurate, that it can figure out exactly what a user has an interest in or even really loves, in less than an hour.[19]
"
"
 The Turing test, originally called the imitation game by Alan Turing in 1950,[2] is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation was a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel, such as a computer keyboard and screen, so the result would not depend on the machine's ability to render words as speech.[3] If the evaluator could not reliably tell the machine from the human, the machine would be said to have passed the test. The test results would not depend on the machine's ability to give correct answers to questions, only on how closely its answers resembled those a human would give. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).[4]
 The test was introduced by Turing in his 1950 paper ""Computing Machinery and Intelligence"" while working at the University of Manchester.[5] It opens with the words: ""I propose to consider the question, 'Can machines think?'"" Because ""thinking"" is difficult to define, Turing chooses to ""replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.""[6] Turing describes the new form of the problem in terms of a three-person game called the ""imitation game"", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: ""Are there imaginable digital computers which would do well in the imitation game?""[2] This question, Turing believed, was one that could actually be answered. In the remainder of the paper, he argued against all the major objections to the proposition that ""machines can think"".[7]
 Since Turing introduced his test, it has been both highly influential and widely criticized, and has become an important concept in the philosophy of artificial intelligence.[8][9][10] Philosopher John Searle would comment on the Turing test in his Chinese room argument, a thought experiment that stipulates that a machine cannot have a ""mind,"" ""understanding,"" or ""consciousness,"" regardless of how intelligently or human-like the program may make the computer behave. Searle criticizes Turing's test and claims it is insufficient to detect the presence of consciousness. Searle goes on to dispute the notion that the mind (mental cognition) can exist outside of the body, a belief known as Cartesian dualism.
 
The question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind. René Descartes prefigures aspects of the Turing test in his 1637 Discourse on the Method when he writes:  [H]ow many different automata or moving machines could be made by the industry of man ... For we can easily understand a machine's being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.[11] Here Descartes notes that automata are capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. Descartes therefore prefigures the Turing test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. Descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the Turing test as such, even if he prefigures its conceptual framework and criterion.
 Denis Diderot formulates in his 1746 book Pensées philosophiques a Turing-test criterion, though with the important implicit limiting assumption maintained, of the participants being natural living beings, rather than considering created artifacts:
 If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation. This does not mean he agrees with this, but that it was already a common argument of materialists at that time.
 According to dualism, the mind is non-physical (or, at the very least, has non-physical properties)[12] and, therefore, cannot be explained in purely physical terms. According to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially.[13]
 In 1936, philosopher Alfred Ayer considered the standard philosophical question of other minds: how do we know that other people have the same conscious experiences that we do? In his book, Language, Truth and Logic, Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: ""The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined.""[14] (This suggestion is very similar to the Turing test, but it is not certain that Ayer's popular philosophical classic was familiar to Turing.) In other words, a thing is not conscious if it fails the consciousness test.
 Tests where a human judges whether a computer or an alien is intelligent were an established convention in science fiction by the 1940s, and it is likely that Turing would have been aware of these.[15] Stanley G. Weinbaum's ""A Martian Odyssey"" (1934) provides an example of how nuanced such tests could be.[15]
 Earlier examples of machines or automatons attempting to pass as human include the Ancient Greek myth of Pygmalion who creates a sculpture of a woman that is animated by Aphrodite, Carlo Collodi's novel The Adventures of Pinocchio, about a puppet who wants to become a real boy, and E. T. A. Hoffmann's 1816 story ""The Sandman,"" where the protagonist falls in love with an automaton. In all these examples, people are fooled by artificial beings that - up to a point - pass as human.[16]
 Researchers in the United Kingdom had been exploring ""machine intelligence"" for up to ten years prior to the founding of the field of artificial intelligence (AI) research in 1956.[17] It was a common topic among the members of the Ratio Club, an informal group of British cybernetics and electronics researchers that included Alan Turing.[18]
 Turing, in particular, had been running the notion of machine intelligence since at least 1941[19] and one of the earliest-known mentions of ""computer intelligence"" was made by him in 1947.[20] In Turing's report, ""Intelligent Machinery,""[21] he investigated ""the question of whether or not it is possible for machinery to show intelligent behaviour""[22] and, as part of that investigation, proposed what may be considered the forerunner to his later tests:
 It is not difficult to devise a paper machine which will play a not very bad game of chess.[23] Now get three men A, B and C as subjects for the experiment. A and C are to be rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.[24] ""Computing Machinery and Intelligence"" (1950) was the first published paper by Turing to focus exclusively on machine intelligence. Turing begins the 1950 paper with the claim, ""I propose to consider the question 'Can machines think?'""[6] As he highlights, the traditional approach to such a question is to start with definitions, defining both the terms ""machine"" and ""think."" Turing chooses not to do so; instead, he replaces the question with a new one, ""which is closely related to it and is expressed in relatively unambiguous words.""[6] In essence he proposes to change the question from ""Can machines think?"" to ""Can machines do what we (as thinking entities) can do?""[25] The advantage of the new question, Turing argues, is that it draws ""a fairly sharp line between the physical and intellectual capacities of a man.""[26]
 To demonstrate this approach Turing proposes a test inspired by a party game, known as the ""imitation game"", in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. In this game, both the man and the woman aim to convince the guests that they are the other. (Huma Shah argues that this two-human version of the game was presented by Turing only to introduce the reader to the machine-human question-answer test.[27]) Turing described his new version of the game as follows:
 We now ask the question, ""What will happen when a machine takes the part of A in this game?"" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, ""Can machines think?""[26] Later in the paper, Turing suggests an ""equivalent"" alternative formulation involving a judge conversing only with a computer and a man.[28] While neither of these formulations precisely matches the version of the Turing test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in a BBC radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.[29]
 Turing's paper considered nine putative objections, which include some of the major arguments against artificial intelligence that have been raised in the years since the paper was published (see ""Computing Machinery and Intelligence"").[7]
 In 1966, Joseph Weizenbaum created a program which appeared to pass the Turing test. The program, known as ELIZA, worked by examining a user's typed comments for keywords. If a keyword is found, a rule that transforms the user's comments is applied, and the resulting sentence is returned. If a keyword is not found, ELIZA responds either with a generic riposte or by repeating one of the earlier comments.[30] In addition, Weizenbaum developed ELIZA to replicate the behaviour of a Rogerian psychotherapist, allowing ELIZA to be ""free to assume the pose of knowing almost nothing of the real world.""[31] With these techniques, Weizenbaum's program was able to fool some people into believing that they were talking to a real person, with some subjects being ""very hard to convince that ELIZA [...] is not human.""[31] Thus, ELIZA is claimed by some to be one of the programs (perhaps the first) able to pass the Turing test,[31][32] even though this view is highly contentious (see Naïveté of interrogators below).
 Kenneth Colby created PARRY in 1972, a program described as ""ELIZA with attitude.""[33] It attempted to model the behaviour of a paranoid schizophrenic, using a similar (if more advanced) approach to that employed by Weizenbaum. To validate the work, PARRY was tested in the early 1970s using a variation of the Turing test. A group of experienced psychiatrists analysed a combination of real patients and computers running PARRY through teleprinters. Another group of 33 psychiatrists were shown transcripts of the conversations. The two groups were then asked to identify which of the ""patients"" were human and which were computer programs.[34] The psychiatrists were able to make the correct identification only 52 percent of the time – a figure consistent with random guessing.[34]
 In the 21st century, versions of these programs (now known as ""chatbots"") continue to fool people. ""CyberLover"", a malware program, preys on Internet users by convincing them to ""reveal information about their identities or to lead them to visit a web site that will deliver malicious content to their computers.""[35] The program has emerged as a ""Valentine-risk"" flirting with people ""seeking relationships online in order to collect their personal data.""[36]
 John Searle's 1980 paper Minds, Brains, and Programs proposed the ""Chinese room"" thought experiment and argued that the Turing test could not be used to determine if a machine could think. Searle noted that software (such as ELIZA) could pass the Turing test simply by manipulating symbols of which they had no understanding. Without understanding, they could not be described as ""thinking"" in the same sense people did. Therefore, Searle concluded, the Turing test could not prove that machines could think.[37] Much like the Turing test itself, Searle's argument has been both widely criticised[38] and endorsed.[39]
 Arguments such as Searle's and others working on the philosophy of mind sparked off a more intense debate about the nature of intelligence, the possibility of machines with a conscious mind and the value of the Turing test that continued through the 1980s and 1990s.[40]
 The Loebner Prize provides an annual platform for practical Turing tests with the first competition held in November 1991.[41] It is underwritten by Hugh Loebner. The Cambridge Center for Behavioral Studies in Massachusetts, United States, organised the prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing test despite 40 years of discussing it.[42]
 The first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing test and the value of pursuing it, in both the popular press[43] and academia.[44] The first contest was won by a mindless program with no identifiable intelligence that managed to fool naïve interrogators into making the wrong identification. This highlighted several of the shortcomings of the Turing test (discussed below): The winner won, at least in part, because it was able to ""imitate human typing errors"";[43] the unsophisticated interrogators were easily fooled;[44] and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research.[45]
 The silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the ""most human"" conversational behaviour among that year's entries. Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AI Jabberwacky won in 2005 and 2006.
 The Loebner Prize tests conversational intelligence; winners are typically chatterbot programs, or Artificial Conversational Entities (ACE)s. Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic,[46] thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes.
 In June 2022 the Google LaMDA (Language Model for Dialog Applications) chatbot received widespread coverage regarding claims about it having achieved sentience. Initially in an article in The Economist Google Research Fellow Blaise Agüera y Arcas said the chatbot had demonstrated a degree of understanding of social relationships.[47] Several days later, Google engineer Blake Lemoine claimed in an interview with the Washington Post that LaMDA had achieved sentience. Lemoine had been placed on leave by Google for internal assertions to this effect. Agüera y Arcas (a Google Vice President) and Jen Gennai (head of Responsible Innovation) had investigated the claims but dismissed them.[48] Lemoine's assertion was roundly rejected by other experts in the field, pointing out that a language model appearing to mimic human conversation does not indicate that any intelligence is present behind it,[49] despite seeming to pass the Turing test. Widespread discussion from proponents for and against the claim that LaMDA has reached sentience has sparked discussion across social-media platforms, to include defining the meaning of sentience as well as what it means to be human.
 OpenAI's chatbot, ChatGPT, released in November 2022, is based on GPT-3.5 and GPT-4 large language models. Celeste Biever wrote in a Nature article that ""ChatGPT broke the Turing test.""[50] Stanford researchers reported that ChatGPT passes the test; they found that ChatGPT-4 ""passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative.""[51][52]
 Saul Traiger argues that there are at least three primary versions of the Turing test, two of which are offered in ""Computing Machinery and Intelligence"" and one that he describes as the ""Standard Interpretation"".[53] While there is some debate regarding whether the ""Standard Interpretation"" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent,[53] and their strengths and weaknesses are distinct.[54]
 Turing's original article describes a simple party game involving three players. Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either gender. In the imitation game, player C is unable to see either player A or player B, and can communicate with them only through written notes. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one.[8]
 Turing then asks:
 ""What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?"" These questions replace our original, ""Can machines think?""[26] The second version appeared later in Turing's 1950 paper. Similar to the original imitation game test, the role of player A is performed by a computer. However, the role of player B is performed by a man rather than a woman.
 Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?[26] In this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision.
 The standard interpretation is not included in the original paper, but is both accepted and debated.
Common understanding has it that the purpose of the Turing test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could imitate a human.[8] While there is some dispute whether this interpretation was intended by Turing, Sterrett believes that it was[55] and thus conflates the second version with this one, while others, such as Traiger, do not[53] – this has nevertheless led to what can be viewed as the ""standard interpretation"". In this version, player A is a computer and player B a person of either sex. The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human.[56] The fundamental issue with the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. There are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable.
 Controversy has arisen over which of the alternative formulations of the test Turing intended.[55] Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, pace Turing's remark, they are not equivalent. The test that employs the party game and compares frequencies of success is referred to as the ""Original Imitation Game Test"", whereas the test consisting of a human judge conversing with a human and a machine is referred to as the ""Standard Turing Test"", noting that Sterrett equates this with the ""standard interpretation"" rather than the second version of the imitation game. Sterrett agrees that the standard Turing test (STT) has the problems that its critics cite but feels that, in contrast, the original imitation game test (OIG test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. A man can fail the OIG test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG test requires the resourcefulness associated with intelligence and not merely ""simulation of human conversational behaviour."" The general structure of the OIG test could even be used with non-verbal versions of imitation games.[57]
 According to Huma Shah, Turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions.[58] Shah argues the imitation game which Turing described could be practicalized in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator.[27]
 Still other writers[59] have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game.
 
Some writers argue that the imitation game is best understood by its social aspects. In his 1948 paper, Turing refers to intelligence as an ""emotional concept,"" and notes that  The extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. If we are able to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine intelligence. With the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behaviour.[60] Following this remark and similar ones scattered throughout Turing's publications, Diane Proudfoot[61] claims that Turing held a response-dependence approach to intelligence, according to which an intelligent (or thinking) entity is one that appears intelligent to an average interrogator. Bernardo Gonçalves shows that although Turing used the rhetoric of introducing his test as a sort of crucial experiment to decide whether machines can be said to think,[62] the actual presentation of his test satisfies well-known properties of thought experiments in the modern scientific tradition of Galileo.[63] Shlomo Danziger[64] promotes a socio-technological interpretation, according to which Turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve a change in society's attitude toward machines. According to this reading, Turing's celebrated 50-year prediction - that by the end of the 20th century his test will be passed by some machine - actually consists of two distinguishable predictions. The first is a technological prediction: I believe that in about fifty years' time it will be possible to programme computers ... to make them play the imitation game so well that an average interrogator will not have more than 70% chance of making the right identification after five minutes of questioning.[65] The second prediction Turing makes is a sociological one: I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.[65] Danziger claims further that for Turing, alteration of society's attitude towards machinery is a prerequisite for the existence of intelligent machines: Only when the term ""intelligent machine"" is no longer seen as an oxymoron the existence of intelligent machines would become logically possible.
 Saygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer.[66] The imitation game also includes a ""social hack"" not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not.[67]
 A crucial piece of any laboratory test is that there should be a control. Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. He states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement.[26] When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.[68] As Ayse Saygin, Peter Swirski,[69] and others have highlighted, this makes a big difference to the implementation and outcome of the test.[8] An experimental study looking at Gricean maxim violations using transcripts of Loebner's one-to-one (interrogator-hidden interlocutor) Prize for AI contests between 1994 and 1999, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved.[70]
 The power and appeal of the Turing test derives from its simplicity. The philosophy of mind, psychology, and modern neuroscience have been unable to provide definitions of ""intelligence"" and ""thinking"" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of the philosophy of artificial intelligence cannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic attempt to answer a difficult philosophical question.
 The format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. Turing wrote that ""the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include.""[71] John Haugeland adds that ""understanding the words is not enough; you have to understand the topic as well.""[72]
 To pass a well-designed Turing test, the machine must use natural language, reason, have knowledge and learn. The test can be extended to include video input, as well as a ""hatch"" through which objects can be passed: this would force the machine to demonstrate skilled use of well designed vision and robotics as well. Together, these represent almost all of the major problems that artificial intelligence research would like to solve.[73]
 The Feigenbaum test is designed to take advantage of the broad range of topics available to a Turing test. It is a limited form of Turing's question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry.
 As a Cambridge honours graduate in mathematics, Turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipating a more recent approach to the subject. Instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant.
 Given the status of human sexual dimorphism as one of the most ancient of subjects, it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. The challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibility—both of which qualities are on display in this snippet of dialogue which Turing has imagined:
 When Turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry:
 Turing thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an AI run amok,[74] it has been suggested[75] that this focus perhaps represents a critical intuition on Turing's part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a ""friendly AI"". It is further noted, however, that whatever inspiration Turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a ""standard interpretation"" of the Turing test—i.e., one which focuses on a discursive intelligence only—must be regarded with some caution.
 Turing did not explicitly state that the Turing test could be used as a measure of ""intelligence"", or any other human quality. He wanted to provide a clear and understandable alternative to the word ""think"", which he could then use to reply to criticisms of the possibility of ""thinking machines"" and to suggest ways that research might move forward. 
 Nevertheless, the Turing test has been proposed as a measure of a machine's ""ability to think"" or its ""intelligence"". This proposal has received criticism from both philosophers and computer scientists. The interpretation makes the assumption that an interrogator can determine if a machine is ""thinking"" by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the reliability of the interrogator's judgement, the value of comparing the machine with a human, and the value of comparing only behaviour. Because of these and other considerations, some AI researchers have questioned the relevance of the test to their field.
 In practice, the test's results can easily be dominated not by the computer's intelligence, but by the attitudes, skill, or naïveté of the questioner. Numerous experts in the field, including cognitive scientist Gary Marcus, insist that the Turing test only shows how easy it is to fool humans and is not an indication of machine intelligence.[76]
 Turing doesn't specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term ""average interrogator"": ""[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning.""[65]
 Chatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the ""interrogators"" are not even aware of the possibility that they are interacting with computers. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required.
 Early Loebner Prize competitions used ""unsophisticated"" interrogators who were easily fooled by the machines.[44] Since 2004, the Loebner Prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. Nonetheless, some of these experts have been deceived by the machines.[77]
 One interesting feature of the Turing test is the frequency of the confederate effect, when the confederate (tested) humans are misidentified by the interrogators as machines. It has been suggested that what interrogators expect as human responses is not necessarily typical of humans. As a result, some individuals can be categorised as machines. This can therefore work in favour of a competing machine. The humans are instructed to ""act themselves"", but sometimes their answers are more like what the interrogator expects a machine to say.[78] This raises the question of how to ensure that the humans are motivated to ""act human"".
 The Turing test does not directly test whether the computer behaves intelligently. It tests only whether the computer behaves like a human being. Since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways:
 The Turing test is concerned strictly with how the subject acts – the external behaviour of the machine. In this regard, it takes a behaviourist or functionalist approach to the study of the mind. The example of ELIZA suggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all.
 John Searle has argued that external behaviour cannot be used to determine if a machine is ""actually"" thinking or merely ""simulating thinking.""[37] His Chinese room argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a mind, consciousness, or intentionality. (Intentionality is a philosophical term for the power of thoughts to be ""about"" something.)
 
Turing anticipated this line of criticism in his original paper,[82] writing:  I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[83]  Mainstream AI researchers argue that trying to pass the Turing test is merely a distraction from more fruitful research.[45] Indeed, the Turing test is not an active focus of much academic or commercial effort—as Stuart Russell and Peter Norvig write: ""AI researchers have devoted little attention to passing the Turing test.""[84] There are several reasons.
 First, there are easier ways to test their programs. Most current research in AI-related fields is aimed at modest and specific goals, such as object recognition or logistics. To test the intelligence of the programs that solve these problems, AI researchers simply give them the task directly. Stuart Russell and Peter Norvig suggest an analogy with the history of flight: Planes are tested by how well they fly, not by comparing them to birds. ""Aeronautical engineering texts,"" they write, ""do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'""[84]
 Second, creating lifelike simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, a game, or a sophisticated user interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence.
 Turing did not intend for his idea to be used to test the intelligence of programs—he wanted to provide a clear and understandable example to aid in the discussion of the philosophy of artificial intelligence.[85] John McCarthy argues that we should not be surprised that a philosophical idea turns out to be useless for practical applications. He observes that the philosophy of AI is ""unlikely to have any more effect on the practice of AI research than philosophy of science generally has on the practice of science.""[86][87]
 Another well known objection raised towards the Turing Test concerns its exclusive focus on the linguistic behaviour (i.e. it is only a ""language-based"" experiment, while all the other cognitive faculties are not tested). This drawback downsizes the role of other modality-specific ""intelligent abilities"" concerning human beings that the psychologist Howard Gardner, in his ""multiple intelligence theory"", proposes to consider (verbal-linguistic abilities are only one of those).[88]
 A critical aspect of the Turing test is that a machine must give itself away as being a machine by its utterances. An interrogator must then make the ""right identification"" by correctly identifying the machine as being just that. If however a machine remains silent during a conversation, then it is not possible for an interrogator to accurately identify the machine other than by means of a calculated guess.[89]
Even taking into account a parallel/hidden human as part of the test may not help the situation as humans can often be misidentified as being a machine.[90]
 By focusing on imitating humans, rather than augmenting or extending human capabilities, the Turing Test risks directing research and implementation toward technologies that substitute for humans and thereby drive down wages and income for workers. As they lose economic power, these workers may also lose political power, making it more difficult for them to change the allocation of wealth and income. This can trap them in a bad equilibrium. Erik Brynjolfsson has called this ""The Turing Trap"" and argued that there are currently excess incentives for creating machines that imitate rather than augment humans.
 Numerous other versions of the Turing test, including those expounded above, have been raised through the years.
 A modification of the Turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse Turing test. An example is implied in the work of psychoanalyst Wilfred Bion,[91] who was particularly fascinated by the ""storm"" that resulted from the encounter of one mind by another. In his 2000 book,[69] among several other original points with regard to the Turing test, literary scholar Peter Swirski discussed in detail the idea of what he termed the Swirski test—essentially the reverse Turing test. He pointed out that it overcomes most if not all standard objections levelled at the standard version.
 Carrying this idea forward, R. D. Hinshelwood[92] described the mind as a ""mind recognizing apparatus"". The challenge would be for the computer to be able to determine if it were interacting with a human or another computer. This is an extension of the original question that Turing attempted to answer but would, perhaps, offer a high enough standard to define a machine that could ""think"" in a way that we typically define as characteristically human.
 CAPTCHA is a form of reverse Turing test. Before being allowed to perform some action on a website, the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out. This is intended to prevent automated systems from being used to abuse the site. The rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human.
 Software that could reverse CAPTCHA with some accuracy by analysing patterns in the generating engine started being developed soon after the creation of CAPTCHA.[93]
In 2013, researchers at Vicarious announced that they had developed a system to solve CAPTCHA challenges from Google, Yahoo!, and PayPal up to 90% of the time.[94]
In 2014, Google engineers demonstrated a system that could defeat CAPTCHA challenges with 99.8% accuracy.[95]
In 2015, Shuman Ghosemajumder, former click fraud czar of Google, stated that there were cybercriminal sites that would defeat CAPTCHA challenges for a fee, to enable various forms of fraud.[96]
 A further variation is motivated by the concern that modern Natural Language Processing prove to be highly successful in generating text on the basis of a huge text corpus and could eventually pass the Turing test simply by manipulating words and sentences that have been used in the initial training of the model. Since the interrogator has no precise understanding of the training data, the model might simply be returning sentences that exist in similar fashion in the enormous amount of training data. For this reason, Arthur Schwaninger proposes a variation of the Turing test that can distinguish between systems that are only capable of using language and systems that understand language. He proposes a test in which the machine is confronted with philosophical questions that do not depend on any prior knowledge and yet require self-reflection to be answered appropriately.[97]
 Another variation is described as the subject-matter expert Turing test, where a machine's response cannot be distinguished from an expert in a given field. This is also known as a ""Feigenbaum test"" and was proposed by Edward Feigenbaum in a 2003 paper.[98]
 Robert French (1990) makes the case that an interrogator can distinguish human and non-human interlocutors by posing questions that reveal the low-level (i.e., unconscious) processes of human cognition, as studied by cognitive science. Such questions reveal the precise details of the human embodiment of thought and can unmask a computer unless it experiences the world as humans do.[99]
 The ""Total Turing test""[4] variation of the Turing test, proposed by cognitive scientist Stevan Harnad,[100] adds two further requirements to the traditional Turing test. The interrogator can also test the perceptual abilities of the subject (requiring computer vision) and the subject's ability to manipulate objects (requiring robotics).[101]
 A letter published in Communications of the ACM[102] describes the concept of generating a synthetic patient population and proposes a variation of Turing test to assess the difference between synthetic and real patients. The letter states: ""In the EHR context, though a human physician can readily distinguish between synthetically generated and real live human patients, could a machine be given the intelligence to make such a determination on its own?"" and further the letter states: ""Before synthetic patient identities become a public health problem, the legitimate EHR market might benefit from applying Turing Test-like techniques to ensure greater data reliability and diagnostic value. Any new techniques must thus consider patients' heterogeneity and are likely to have greater complexity than the Allen eighth-grade-science-test is able to grade.""
 The minimum intelligent signal test was proposed by Chris McKinstry as ""the maximum abstraction of the Turing test"",[103] in which only binary responses (true/false or yes/no) are permitted, to focus only on the capacity for thought. It eliminates text chat problems like anthropomorphism bias, and does not require emulation of unintelligent human behaviour, allowing for systems that exceed human intelligence. The questions must each stand on their own, however, making it more like an IQ test than an interrogation. It is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.[104]
 The organisers of the Hutter Prize believe that compressing natural language text is a hard AI problem, equivalent to passing the Turing test.
 The data compression test has some advantages over most versions and variations of a Turing test, including:
 The main disadvantages of using data compression as a test are:
 A related approach to Hutter's prize which appeared much earlier in the late 1990s is the inclusion of compression problems in an extended Turing test.[105] or by tests which are completely derived from Kolmogorov complexity.[106]
Other related tests in this line are presented by Hernandez-Orallo and Dowe.[107]
 Algorithmic IQ, or AIQ for short, is an attempt to convert the theoretical Universal Intelligence Measure from Legg and Hutter (based on Solomonoff's inductive inference) into a working practical test of machine intelligence.[108]
 Two major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers.
 The Turing test inspired the Ebert test proposed in 2011 by film critic Roger Ebert which is a test whether a computer-based synthesised voice has sufficient skill in terms of intonations, inflections, timing and so forth, to make people laugh.[109]
 Taking advantage of Large Language Models, in 2023 the research company AI21 Labs created an online social experiment titled ""Human or Not?"".[110][111]  It was played more than 10 million times by more than 2 million people.[112] It is the biggest Turing-style experiment to that date. The results showed that 32% of people couldn't distinguish between humans and machines.[113][114]
 1990 marked the fortieth anniversary of the first publication of Turing's ""Computing Machinery and Intelligence"" paper, and saw renewed interest in the test. Two significant events occurred in that year: the first was the Turing Colloquium, which was held at the University of Sussex in April, and brought together academics and researchers from a wide variety of disciplines to discuss the Turing test in terms of its past, present, and future; the second was the formation of the annual Loebner Prize competition.
 Blay Whitby lists four major turning points in the history of the Turing test – the publication of ""Computing Machinery and Intelligence"" in 1950, the announcement of Joseph Weizenbaum's ELIZA in 1966, Kenneth Colby's creation of PARRY, which was first described in 1972, and the Turing Colloquium in 1990.[115]
 In parallel to the 2008 Loebner Prize held at the University of Reading,[116]
the Society for the Study of Artificial Intelligence and the Simulation of Behaviour (AISB), hosted a one-day symposium to discuss the Turing test, organised by John Barnden, Mark Bishop, Huma Shah and Kevin Warwick.[117]
The speakers included the Royal Institution's Director Baroness Susan Greenfield, Selmer Bringsjord, Turing's biographer Andrew Hodges, and consciousness scientist Owen Holland. No agreement emerged for a canonical Turing test, though Bringsjord expressed that a sizeable prize would result in the Turing test being passed sooner.
"
"Information theory is the mathematical study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.[2]: vii  The field, in applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.
 A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy, less uncertainty) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.
 Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference,[3] cryptography, neurobiology,[4] perception,[5] linguistics, the evolution[6] and function[7] of molecular codes (bioinformatics), thermal physics,[8] molecular dynamics,[9] quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection,[10] pattern recognition, anomaly detection[11] and even art creation.
 Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was formalized in 1948 by Claude Shannon in a paper entitled A Mathematical Theory of Communication, in which information is thought of as a set of possible messages, and the goal is to send these messages over a noisy channel, and to have the receiver reconstruct the message with low probability of error, in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem, showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.[4]
 Coding theory is concerned with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible.[citation needed]
 A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis, such as the unit ban.[citation needed]
 The landmark event establishing the discipline of information theory and bringing it to immediate worldwide attention was the publication of Claude E. Shannon's classic paper ""A Mathematical Theory of Communication"" in the Bell System Technical Journal in July and October 1948. He came to be known as the ""father of information theory"".[12][13][14]
 Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying ""intelligence"" and the ""line speed"" at which it can be transmitted by a communication system, giving the relation W = K log m (recalling the Boltzmann constant), where W is the speed of transmission of intelligence, m is the number of different voltage levels to choose from at each time step, and K is a constant. Ralph Hartley's 1928 paper, Transmission of Information, uses the word information as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as H = log Sn = n log S, where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which since has sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.[citation needed]
 Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs. Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in Entropy in thermodynamics and information theory.[citation needed]
 In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion: 
 With it came the ideas of
 Information theory is based on probability theory and statistics, where quantified information is usually described in terms of bits. Information theory often concerns itself with measures of information of the distributions associated with random variables. One of the most important measures is called entropy, which forms the building block of many other measures. Entropy allows quantification of measure of information in a single random variable. Another useful concept is mutual information defined on two random variables, which describes the measure of information in common between those variables, which can be used to describe their correlation. The former quantity is a property of the probability distribution of a random variable and gives a limit on the rate at which data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.
 The choice of logarithmic base in the following formulae determines the unit of information entropy that is used. A common unit of information is the bit, based on the binary logarithm. Other units include the nat, which is based on the natural logarithm, and the decimal digit, which is based on the common logarithm.
 In what follows, an expression of the form p log p is considered by convention to be equal to zero whenever p = 0. This is justified because 




lim

p
→
0
+


p
log
⁡
p
=
0


{\displaystyle \lim _{p\rightarrow 0+}p\log p=0}

 for any logarithmic base.
 Based on the probability mass function of each source symbol to be communicated, the Shannon entropy H, in units of bits (per symbol), is given by
 where pi is the probability of occurrence of the i-th possible value of the source symbol. This equation gives the entropy in the units of ""bits"" (per symbol) because it uses a logarithm of base 2, and this base-2 measure of entropy has sometimes been called the shannon in his honor. Entropy is also commonly computed using the natural logarithm (base e, where e is Euler's number), which produces a measurement of entropy in nats per symbol and sometimes simplifies the analysis by avoiding the need to include extra constants in the formulas. Other bases are also possible, but less commonly used. For example, a logarithm of base 28 = 256 will produce a measurement in bytes per symbol, and a logarithm of base 10 will produce a measurement in decimal digits (or hartleys) per symbol.
 Intuitively, the entropy HX of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.
 The entropy of a source that emits a sequence of N symbols that are independent and identically distributed (iid) is N ⋅ H bits (per message of N symbols). If the source data symbols are identically distributed but not independent, the entropy of a message of length N will be less than N ⋅ H.
 If one transmits 1000 bits (0s and 1s), and the value of each of these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (more often called bits) have been transmitted. Between these two extremes, information can be quantified as follows. If 




X



{\displaystyle \mathbb {X} }

 is the set of all messages {x1, ..., xn} that X could be, and p(x) is the probability of some 



x
∈

X



{\displaystyle x\in \mathbb {X} }

, then the entropy, H, of X is defined:[15]
 (Here, I(x) is the self-information, which is the entropy contribution of an individual message, and 





E


X




{\displaystyle \mathbb {E} _{X}}

 is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable p(x) = 1/n; i.e., most unpredictable, in which case H(X) = log n.
 The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:
 The joint entropy of two discrete random variables X and Y is merely the entropy of their pairing: (X, Y). This implies that if X and Y are independent, then their joint entropy is the sum of their individual entropies.
 For example, if (X, Y) represents the position of a chess piece—X the row and Y the column, then the joint entropy of the row of the piece and the column of the piece will be the entropy of the position of the piece.
 Despite similar notation, joint entropy should not be confused with cross-entropy.
 The conditional entropy or conditional uncertainty of X given random variable Y (also called the equivocation of X about Y) is the average conditional entropy over Y:[16]
 Because entropy can be conditioned on a random variable or on that random variable being a certain value, care should be taken not to confuse these two definitions of conditional entropy, the former of which is in more common use. A basic property of this form of conditional entropy is that:
 Mutual information measures the amount of information that can be obtained about one random variable by observing another. It is important in communication where it can be used to maximize the amount of information shared between sent and received signals. The mutual information of X relative to Y is given by:
 where SI (Specific mutual Information) is the pointwise mutual information.
 A basic property of the mutual information is that
 That is, knowing Y, we can save an average of I(X; Y) bits in encoding X compared to not knowing Y.
 Mutual information is symmetric:
 Mutual information can be expressed as the average Kullback–Leibler divergence (information gain) between the posterior probability distribution of X given the value of Y and the prior distribution on X:
 In other words, this is a measure of how much, on the average, the probability distribution on X will change if we are given the value of Y. This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:
 Mutual information is closely related to the log-likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson's χ2 test: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.
 The Kullback–Leibler divergence (or information divergence, information gain, or relative entropy) is a way of comparing two distributions: a ""true"" probability distribution 



p
(
X
)


{\displaystyle p(X)}

, and an arbitrary probability distribution 



q
(
X
)


{\displaystyle q(X)}

. If we compress data in a manner that assumes 



q
(
X
)


{\displaystyle q(X)}

 is the distribution underlying some data, when, in reality, 



p
(
X
)


{\displaystyle p(X)}

 is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression. It is thus defined
 Although it is sometimes used as a 'distance metric', KL divergence is not a true metric since it is not symmetric and does not satisfy the triangle inequality (making it a semi-quasimetric).
 Another interpretation of the KL divergence is the ""unnecessary surprise"" introduced by a prior from the truth: suppose a number X is about to be drawn randomly from a discrete set with probability distribution 



p
(
x
)


{\displaystyle p(x)}

. If Alice knows the true distribution 



p
(
x
)


{\displaystyle p(x)}

, while Bob believes (has a prior) that the distribution is 



q
(
x
)


{\displaystyle q(x)}

, then Bob will be more surprised than Alice, on average, upon seeing the value of X. The KL divergence is the (objective) expected value of Bob's (subjective) surprisal minus Alice's surprisal, measured in bits if the log is in base 2. In this way, the extent to which Bob's prior is ""wrong"" can be quantified in terms of how ""unnecessarily surprised"" it is expected to make him.
 Directed information, 



I
(

X

n


→

Y

n


)


{\displaystyle I(X^{n}\to Y^{n})}

, is an information theory measure that quantifies the information flow from the random process 




X

n


=
{

X

1


,

X

2


,
…
,

X

n


}


{\displaystyle X^{n}=\{X_{1},X_{2},\dots ,X_{n}\}}

 to the random process 




Y

n


=
{

Y

1


,

Y

2


,
…
,

Y

n


}


{\displaystyle Y^{n}=\{Y_{1},Y_{2},\dots ,Y_{n}\}}

. The term directed information was coined by James Massey and is defined as
 where 



I
(

X

i


;

Y

i



|


Y

i
−
1


)


{\displaystyle I(X^{i};Y_{i}|Y^{i-1})}

 is the conditional mutual information 



I
(

X

1


,

X

2


,
.
.
.
,

X

i


;

Y

i



|


Y

1


,

Y

2


,
.
.
.
,

Y

i
−
1


)


{\displaystyle I(X_{1},X_{2},...,X_{i};Y_{i}|Y_{1},Y_{2},...,Y_{i-1})}

.
 In contrast to mutual information, directed information is not symmetric. The 



I
(

X

n


→

Y

n


)


{\displaystyle I(X^{n}\to Y^{n})}

 measures the information bits that are transmitted causally[definition of causal transmission?] from 




X

n




{\displaystyle X^{n}}

 to 




Y

n




{\displaystyle Y^{n}}

. The Directed information has many applications in problems where causality plays an important role such as capacity of channel with feedback,[17][18] capacity of discrete memoryless networks with feedback,[19] gambling with causal side information,[20] compression with causal side information,[21]
and in real-time control communication settings,[22][23] statistical physics.[24]
 Other important information theoretic quantities include Rényi entropy (a generalization of entropy), differential entropy (a generalization of quantities of information to continuous distributions), and the conditional mutual information. Also, pragmatic information has been proposed as a measure of how much information has been used in making a decision.
 Coding theory is one of the most important and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.
 This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user. In scenarios with more than one transmitter (the multiple-access channel), more than one receiver (the broadcast channel) or intermediary ""helpers"" (the relay channel), or more general networks, compression followed by transmission may no longer be optimal.
 Any process that generates successive messages can be considered a source of information. A memoryless source is one in which each message is an independent identically distributed random variable, whereas the properties of ergodicity and stationarity impose less restrictive constraints. All such sources are stochastic. These terms are well studied in their own right outside information theory.
 Information rate is the average entropy per symbol. For memoryless sources, this is merely the entropy of each symbol, while, in the case of a stationary stochastic process, it is
 that is, the conditional entropy of a symbol given all the previous symbols generated. For the more general case of a process that is not necessarily stationary, the average rate is
 that is, the limit of the joint entropy per symbol. For stationary sources, these two expressions give the same result.[25]
 Information rate is defined as 
 It is common in information theory to speak of the ""rate"" or ""entropy"" of a language. This is appropriate, for example, when the source of information is English prose. The rate of a source of information is related to its redundancy and how well it can be compressed, the subject of source coding.
 Communications over a channel is the primary motivation of information theory. However, channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.
 Consider the communications process over a discrete channel. A simple model of the process is shown below:
 Here X represents the space of messages transmitted, and Y the space of messages received during a unit time over our channel. Let p(y|x) be the conditional probability distribution function of Y given X. We will consider p(y|x) to be an inherent fixed property of our communications channel (representing the nature of the noise of our channel). Then the joint distribution of X and Y is completely determined by our channel and by our choice of f(x), the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the signal, we can communicate over the channel. The appropriate measure for this is the mutual information, and this maximum mutual information is called the channel capacity and is given by:
 This capacity has the following property related to communicating at information rate R (where R is usually bits per symbol). For any information rate R < C and coding error ε > 0, for large enough N, there exists a code of length N and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is ≤ ε; that is, it is always possible to transmit with arbitrarily small block error. In addition, for any rate R > C, it is impossible to transmit with arbitrarily small block error.
 Channel coding is concerned with finding such nearly optimal codes that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity.
 In practice many channels have memory. Namely, at time 



i


{\displaystyle i}

 the channel is given by the conditional probability



P
(

y

i



|


x

i


,

x

i
−
1


,

x

i
−
2


,
.
.
.
,

x

1


,

y

i
−
1


,

y

i
−
2


,
.
.
.
,

y

1


)
.


{\displaystyle P(y_{i}|x_{i},x_{i-1},x_{i-2},...,x_{1},y_{i-1},y_{i-2},...,y_{1}).}

.
It is often more comfortable to use the notation 




x

i


=
(

x

i


,

x

i
−
1


,

x

i
−
2


,
.
.
.
,

x

1


)


{\displaystyle x^{i}=(x_{i},x_{i-1},x_{i-2},...,x_{1})}

 and the channel become 



P
(

y

i



|


x

i


,

y

i
−
1


)
.


{\displaystyle P(y_{i}|x^{i},y^{i-1}).}

.
In such a case the capacity is given by the mutual information rate when there is no feedback available and the Directed information rate in the case that either there is feedback or not[17][26] (if there is no feedback the directed information equals the mutual information).
 Information theoretic concepts apply to cryptography and cryptanalysis. Turing's information unit, the ban, was used in the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe. Shannon himself defined an important concept now called the unicity distance. Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.
 Information theory leads us to believe it is much more difficult to keep secrets than it might first appear. A brute force attack can break systems based on asymmetric key algorithms or on most commonly used methods of symmetric key algorithms (sometimes called secret key algorithms), such as block ciphers. The security of all such methods comes from the assumption that no known attack can break them in a practical amount of time.
 Information theoretic security refers to methods such as the one-time pad that are not vulnerable to such brute force attacks. In such cases, the positive conditional mutual information between the plaintext and ciphertext (conditioned on the key) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications. In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the Venona project was able to crack the one-time pads of the Soviet Union due to their improper reuse of key material.
 Pseudorandom number generators are widely available in computer language libraries and application programs. They are, almost universally, unsuited to cryptographic use as they do not evade the deterministic nature of modern computer equipment and software. A class of improved random number generators is termed cryptographically secure pseudorandom number generators, but even they require random seeds external to the software to work as intended. These can be obtained via extractors, if done carefully. The measure of sufficient randomness in extractors is min-entropy, a value related to Shannon entropy through Rényi entropy; Rényi entropy is also used in evaluating randomness in cryptographic systems. Although related, the distinctions among these measures mean that a random variable with high Shannon entropy is not necessarily satisfactory for use in an extractor and so for cryptography uses.
 One early commercial application of information theory was in the field of seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. Information theory and digital signal processing offer a major improvement of resolution and image clarity over previous analog methods.[27]
 Semioticians Doede Nauta and Winfried Nöth both considered Charles Sanders Peirce as having created a theory of information in his works on semiotics.[28]: 171 [29]: 137  Nauta defined semiotic information theory as the study of ""the internal processes of coding, filtering, and information processing.""[28]: 91 
 Concepts from information theory such as redundancy and code control have been used by semioticians such as Umberto Eco and Ferruccio Rossi-Landi to explain ideology as a form of message transmission whereby a dominant social class emits its message by using signs that exhibit a high degree of redundancy such that only one message is decoded among a selection of competing ones.[30]
 Quantitative information theoretic methods have been applied in cognitive science to analyze the integrated process organization of neural information in the context of the binding problem in cognitive neuroscience.[31] In this context, either an information-theoretical measure, such as functional clusters (Gerald Edelman and Giulio Tononi's functional clustering model and dynamic core hypothesis (DCH)[32]) or effective information (Tononi's integrated information theory (IIT) of consciousness[33][34][35]), is defined (on the basis of a reentrant process organization, i.e. the synchronization of neurophysiological activity between groups of neuronal populations), or the measure of the minimization of free energy on the basis of statistical methods (Karl J. Friston's free energy principle (FEP), an information-theoretical measure which states that every adaptive change in a self-organized system leads to a minimization of free energy, and the Bayesian brain hypothesis[36][37][38][39][40]).
 Information theory also has applications in gambling, the search for extraterrestrial intelligence,[41] black holes, and bioinformatics.[citation needed]
"
"
 
 The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
 The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956.[1] Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.[2]
 Eventually, it became obvious that researchers had grossly underestimated the difficulty of the project.[3] In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an ""AI winter"". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.
 Investment and interest in AI boomed in the 2020s when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.
 In Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily.[4] According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos.[5] In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.[6]
 Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves.[7] Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.
 In Of the Nature of Things, written by the Swiss alchemist, Paracelsus, he describes a procedure that he claims can fabricate an ""artificial man"". By placing the ""sperm of a man"" in horse dung, and feeding it the ""Arcanum of Mans blood"" after 40 days, the concoction will become a living infant.[8]
 The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century.[9][10] During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God’s names on it, into the mouth of the clay figure.[11] Unlike legendary automata like Brazen Heads,[12] a Golem was unable to speak.[13]
 Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.[14]
 In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.[15]
 By the 19th century, ideas about artificial men and thinking machines were developed in fiction, as in Mary Shelley's Frankenstein  or Karel Čapek's R.U.R. (Rossum's Universal Robots),[16]
and speculation, such as Samuel Butler's ""Darwin among the Machines"",[17] and in real-world instances, including Edgar Allan Poe's ""Maelzel's Chess Player"".[18] AI is a common topic in science fiction through the present.[19]
 Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi,[20] Hero of Alexandria,[21] Al-Jazari,[22]
Pierre Jaquet-Droz, and Wolfgang von Kempelen.[23][24]
 The oldest known automata were the sacred statues of ancient Egypt and Greece.[25] The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that ""by discovering the true nature of the gods, man has been able to reproduce it"".[26][27]
English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.[28]
 During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard.[29][30] These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have ""embalmed"" the head with herbs and spoke incantations over it such that Mímir’s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.[31]
 Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or ""formal""—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to ""algorithm"") and European scholastic philosophers such as William of Ockham and Duns Scotus.[32]
 Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means;[33] Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge.[34] Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.[35]
 In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry.[36] Hobbes famously wrote in Leviathan: ""reason is nothing but reckoning"".[37] Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that ""there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.""[38] These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.
 In the 20th century, the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: ""can all of mathematical reasoning be formalized?""[32] His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.[32][39]
 Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation.[41] This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.[32][42]
 Calculating machines were designed or built in antiquity and throughout history by many people, including 
Gottfried Leibniz,[43]
Joseph Marie Jacquard,[44]
Charles Babbage,[45]
Percy Ludgate,[46]
Leonardo Torres Quevedo,[47]
Vannevar Bush,[48]
and others. Ada Lovelace speculated that Babbage's machine was ""a thinking or ... reasoning machine"", but warned ""It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers"" of the machine.[49][50]
 The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's and ABC and ENIAC at the University of Pennsylvania).[51] ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann,[52] and proved to be the most influential.[51]
 The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an ""electronic brain"". 
 In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.[53] 
Alan Turing, who developed the theory of computation, was among the first people to seriously investigate the theoretical possibility of ""machine intelligence"".[54] The field of ""artificial intelligence research"" was founded as an academic discipline in 1956.[55]
 Alan Turing was thinking about machine intelligence at least as early as 1941, when he circulated a paper on machine intelligence which could be the earliest paper in the field of AI - though it is now lost.[54] 
In 1950 Turing published a landmark paper ""Computing Machinery and Intelligence"", in which he speculated about the possibility of creating machines that think and the paper introduced his concept of what is now known as the Turing test to the general public.[56] He noted that ""thinking"" is difficult to define and devised his famous Turing Test.[57] If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was ""thinking"". This simplified version of the problem allowed Turing to argue convincingly that a ""thinking machine"" was at least plausible and the paper answered all the most common objections to the proposition.[58] The Turing Test was the first serious proposal in the philosophy of artificial intelligence. Then followed three radio broadcasts on AI by Turing, the lectures: 'Intelligent Machinery, A Heretical Theory', 'Can Digital Computers Think?' and the panel discussion 'Can Automatic Calculating Machines be Said to Think'.
 Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943.[59][60] They were the first to describe what later researchers would call a neural network.[61] The paper was influenced by Turing's earlier paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.[54] One of the students inspired by Pitts and McCulloch was a young Marvin Minsky, then a 24-year-old graduate student. In 1951 (with Dean Edmonds) he built the first neural net machine, the SNARC.[62] (Minsky was to become one of the most important leaders and innovators in AI.). 
 Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.[63]
 In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess.[64] Arthur Samuel's checkers program, the subject of his 1959 paper ""Some Studies in Machine Learning Using the Game of Checkers"", eventually achieved sufficient skill to challenge a respectable amateur.[65] Game AI would continue to be used as a measure of progress in AI throughout its history.
 When access to digital computers became possible in the middle fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.[66]
 In 1955, Allen Newell and (future Nobel Laureate) Herbert A. Simon created the ""Logic Theorist"" (with help from J. C. Shaw). The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.[67]
Simon said that they had ""solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.""[68]
(This was an early statement of the philosophical position John Searle would later call ""Strong AI"": that machines can contain minds just as human bodies do.)[69]
The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution. 
 The Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline.[70]
It was organized by Marvin Minsky, John McCarthy and two senior scientists: Claude Shannon and Nathan Rochester of IBM. The primary objective of this workshop was to delve into the possibilities of creating machines capable of simulating human intelligence, marking the commencement of a focused exploration into the realm of AI; [71] the proposal for the conference stated they intended to test the assertion that ""every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it"".[72]
 The term ""Artificial Intelligence"" itself was officially introduced by John McCarthy at the workshop. [73] (The term ""Artificial Intelligence"" was chosen by McCarthy to avoid associations with cybernetics and the influence of Norbert Wiener.)[74]
 The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.[75]
At the workshop Newell and Simon debuted the ""Logic Theorist"".[76]
 The 1956 Dartmouth workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.[77]
 In the fall of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology. At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper ""The Magical Number Seven, Plus or Minus Two"". Miller wrote ""I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.""[78]
 This meeting was the beginning of the ""cognitive revolution"" -- an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.
 The cognitive approach allowed researchers to consider ""mental objects"" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as ""unobservable"" by earlier paradigms such as behaviorism. Symbolic mental objects would become the major focus of AI research and funding for the next several decades.
 The programs developed in the years after the Dartmouth Workshop were, to most people, simply ""astonishing"":[79] computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such ""intelligent"" behavior by machines was possible at all.[80] Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years.[81] Government agencies like DARPA poured money into the new field.[82] Artificial Intelligence laboratories were set up at a number of British and US Universities in the latter 1950s and early 1960s.[54]
 There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:
 Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. This paradigm was called ""reasoning as search"".[83]
 The principal difficulty was that, for many problems, the number of possible paths through the ""maze"" was simply astronomical (a situation known as a ""combinatorial explosion""). Researchers would reduce the search space by using heuristics or ""rules of thumb"" that would eliminate those paths that were unlikely to lead to a solution.[84]
 Newell and Simon tried to capture a general version of this algorithm in a program called the ""General Problem Solver"".[85] Other ""searching"" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle (1961).[86] Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of their robot Shakey.[87]
 The McCulloch and Pitts paper (1944) inspired approaches to creating computing hardware that realizes the neural network approach to AI in hardware. The most influential was the effort led by Frank Rosenblatt on building Perceptron machines (1957-1962) of up to four layers. He was primarily funded by Office of Naval Research.[88] Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights.[89] A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II[90] had 6600 adjustable weights,[91] and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets.[92][93][94]
 Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers. The hardware diversity was particularly clear in the different technologies used in implementing the adjustable weights. The perceptron machines and the SNARC used potentiometers moved by electric motors. ADALINE used memistors adjusted by electroplating, though they also used simulations on an IBM 1620. The MINOS machines used ferrite cores with multiple holes in them that could be individually blocked, with the degree of blockage representing the weights.[95]
 
Though there were multi-layered neural networks, most neural networks during this period had only one layer of adjustable weights. There were empirical attempts at training more than a single layer, but they were unsuccessful. Backpropagation did not become prevalent for neural network training until the 1980s.[95] An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.[96]
 A semantic net represents concepts (e.g. ""house"", ""door"") as nodes and relations among concepts (e.g. ""has-a"") as links between the nodes. The first AI program to use a semantic net was written by Ross Quillian[97] and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.[98]
 Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a program (See ELIZA effect). But in fact, ELIZA had no idea what she was talking about. She simply gave a canned response or repeated back what was said to her, rephrasing her response with a few grammar rules. ELIZA was the first chatterbot.[99]
 In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a ""blocks world,"" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.[100]
 This paradigm led to innovative work in machine vision by Gerald Sussman (who led the team), Adolfo Guzman, David Waltz (who invented ""constraint propagation""), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. The crowning achievement of the micro-world program was Terry Winograd's SHRDLU. It could communicate in ordinary English sentences, plan operations and execute them.[101]
 In Japan, Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale ""intelligent"" humanoid robot,[102][103] or android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.[104][105][106]
 The first generation of AI researchers made these predictions about their work:
 In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (later known as DARPA). The money was used to fund project MAC which subsumed the ""AI Group"" founded by Minsky and McCarthy five years earlier. DARPA continued to provide three million dollars a year until the 70s.[111]
DARPA made similar grants to Newell and Simon's program at CMU and to the Stanford AI Project (founded by John McCarthy in 1963).[112] Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.[113]
These four institutions would continue to be the main centers of AI research (and funding) in academia for many years.[114]
 The money was proffered with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should ""fund people, not projects!"" and allowed researchers to pursue whatever directions might interest them.[115] This created a freewheeling atmosphere at MIT that gave birth to the hacker culture,[116] but this ""hands off"" approach would not last.
 In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI nearly disappeared.[117] At the same time, the exploration of simple, single-layer artificial neural networks was shut down almost completely for a decade partially due to Marvin Minsky's  book emphasizing the limits of what  perceptrons can do.[118]
Despite the difficulties with public perception of AI in the late 70s, new ideas were explored in logic programming, commonsense reasoning and many other areas.[119][120]
 In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, ""toys"".[121] AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.[122]
 The agencies which funded AI research (such as the British government, DARPA and NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected research into AI. The pattern began as early as 1966 when the ALPAC report appeared criticizing machine translation efforts. After spending 20 million dollars, the NRC ended all support.[130]
In 1973, the Lighthill report on the state of AI research in the UK criticized the utter failure of AI to achieve its ""grandiose objectives"" and led to the dismantling of AI research in that country.[131]
(The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)[132]
DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of three million dollars.[133]
By 1974, funding for AI projects was hard to find.
 The end of funding occurred even earlier for neural network research, partly due to lack of results and partly due to competition from symbolic AI research. The MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s.[95]
 Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. ""Many researchers were caught up in a web of increasing exaggeration.""[134]
However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund ""mission-oriented direct research, rather than basic undirected research"". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA. Instead, the money was directed at specific projects with clear objectives, such as autonomous tanks and battle management systems.[135]
 Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could.[136] Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little ""symbol processing"" and a great deal of embodied, instinctive, unconscious ""know how"".[137][138] John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to ""understand"" the symbols that it uses (a quality called ""intentionality""). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as ""thinking"".[139]
 These critiques were not taken seriously by AI researchers, often because they seemed so far off the point. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference ""know how"" or ""intentionality"" made to an actual computer program. Minsky said of Dreyfus and Searle ""they misunderstand, and should be ignored.""[140] Dreyfus, who taught at MIT, was given a cold shoulder: he later said that AI researchers ""dared not be seen having lunch with me.""[141] Joseph Weizenbaum, the author of ELIZA, felt his colleagues' treatment of Dreyfus was unprofessional and childish.[142] Although he was an outspoken critic of Dreyfus' positions, he ""deliberately made it plain that theirs was not the way to treat a human being.""[143]
 Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a ""computer program which can conduct psychotherapeutic dialogue"" based on ELIZA.[144] Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.[145]
 A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that ""perceptron may eventually be able to learn, make decisions, and translate languages."" An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was funded in connectionism for 10 years.
 Of the main efforts towards neural networks, Rosenblatt attempted to gather funds for building larger perceptron machines, but died in a boating accident in 1971. Minsky (of SNARC) turned to a staunch objector to pure connectionist AI. Widrow (of ADALINE) turned to adaptive signal processing, using techniques based on the LMS algorithm. The SRI group (of MINOS) turned to symbolic AI and robotics. The main issues were lack of funding and the inability to train multilayered networks (backpropagation was unknown). The competition for government funding ended with the victory of symbolic AI approaches.[94][95]
 Logic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal.[146]
In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.[147] A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel [fr] who created the successful logic programming language Prolog.[148]
Prolog uses a subset of logic (Horn clauses, closely related to ""rules"" and ""production rules"") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.[149]
 Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.[150]
McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.[151]
 Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like ""story understanding"" and ""object recognition"" that required a machine to think like a person. In order to use ordinary concepts like ""chair"" or ""restaurant"" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that ""using precise language to describe essentially imprecise concepts doesn't make them any more precise.""[152] Schank described their ""anti-logic"" approaches as ""scruffy"", as opposed to the ""neat"" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.[153]
 In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be ""logical"", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures ""frames"". Schank used a version of frames he called ""scripts"" to successfully answer questions about short stories in English.[154]
 The logicians rose to the challenge. Pat Hayes claimed that ""most of 'frames' is just a new syntax for parts of first-order
logic."" But he noted that ""there are one or two apparently minor details which give a lot of trouble, however, especially defaults"".[155] In the meanwhile, Ray Reiter admitted that ""conventional logics, such as first-order
logic, lack the expressive power to adequately represent the knowledge required for reasoning by default"".[156] He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its ""procedural equivalent"" as negation as failure in Prolog.
 The closed world assumption, as formulated by Reiter, ""is not a first-order notion. (It is a meta notion.)""[156] However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.[157]
 During the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.
 In the 1980s, a form of AI program called ""expert systems"" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. In those same years, the Japanese government aggressively funded AI with its fifth generation computer project. Another encouraging event in the early 1980s was the revival of connectionism in the work of John Hopfield and David Rumelhart. Once again, AI had achieved success.[158]
 An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.[159]
 Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.[160]
 In 1980, an expert system called XCON was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986.[161] Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments.[162] An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.[163]
 The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. ""AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,""[164] writes Pamela McCorduck. ""[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay"".[165] Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.[166]
 The 1980s also saw the birth of Cyc, the first attempt to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started and led the project, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. The project was not expected to be completed for many decades.[167]
 Chess playing programs HiTech and Deep Thought defeated chess masters in 1989. Both were developed by Carnegie Mellon University; Deep Thought development paved the way for Deep Blue.[168]
 In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.[169] Much to the chagrin of scruffies, they chose Prolog as the primary computer language for the project.[170]
 Other countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or ""MCC"") to fund large scale projects in AI and information technology.[171][172] DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.[173]
 In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a ""Hopfield net"") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically.[174]
 Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called ""backpropagation"", also known as the reverse mode of automatic differentiation published by Seppo Linnainmaa (1970) and applied to neural networks by Paul Werbos. These two discoveries helped to revive the exploration of artificial neural networks.[172][175]
 Starting with the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland, neural networks research gained new momentum and would become commercially successful in the 1990s, applied to optical character recognition and speech recognition.[172][176]
 The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural network technology in the 1980s.
 A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.[177]
 The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception was that the technology was not viable.[178] However, the field continued to make advances despite the criticism. Numerous researchers, including robotics developers Rodney Brooks and Hans Moravec, argued for an entirely new approach to artificial intelligence.
 The term ""AI winter"" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow.[179] Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.
 The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.[180]
 Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.[181]
 In the late 1980s, the Strategic Computing Initiative cut funding to AI ""deeply and brutally"". New leadership at DARPA had decided that AI was not ""the next wave"" and directed funds towards projects that seemed more likely to produce immediate results.[182]
 By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like ""carry on a casual conversation"" had not been met by 2010.[183] As with other AI projects, expectations had run much higher than what was actually possible.[183][184]
 Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI.[185] In 1994, HP Newquist stated in The Brain Makers that ""The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.""[185]
 In the late 1980s, several researchers advocated a completely new approach to artificial intelligence, based on robotics.[186] They believed that, to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world. They argued that these sensorimotor skills are essential to higher level skills like commonsense reasoning and that abstract reasoning was actually the least interesting or important human skill (see Moravec's paradox). They advocated building intelligence ""from the bottom up.""[187]
 The approach revived ideas from cybernetics and control theory that had been unpopular since the sixties. Another precursor was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)[188]
 In his 1990 paper ""Elephants Don't Play Chess,""[189] robotics researcher Rodney Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since ""the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.""[190] In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.[191]
 The field of AI, now more than a half a century old, finally achieved some of its oldest goals. It began to be used successfully throughout the technology industry, although somewhat behind the scenes. Some of the success was due to increasing computer power and some was achieved by focusing on specific isolated problems and pursuing them with the highest standards of scientific accountability. Still, the reputation of AI, in the business world at least, was less than pristine.[192] Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of ""artificial intelligence"".[193] AI was both more cautious and more successful than it had ever been.
 On 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov.[194] The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second.[195]
 In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail.[196] Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws.[197] In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[198]
 These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s.[199] In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951.[200] This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal–oxide–semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of ""raw computer power"" was slowly being overcome.
 A new paradigm called ""intelligent agents"" became widely accepted during the 1990s.[201] Although earlier researchers had proposed modular ""divide and conquer"" approaches to AI,[202] the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI.[203] When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.
 An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are ""intelligent agents"", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as ""the study of intelligent agents"". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.[204]
 The paradigm gave researchers license to study isolated problems and find solutions that were both verifiable and useful. It provided a common language to describe problems and share their solutions with each other, and with other fields that also used concepts of abstract agents, like economics and control theory. It was hoped that a complete agent architecture (like Newell's SOAR) would one day allow researchers to build more versatile and intelligent systems out of interacting intelligent agents.[203][205]
 AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past.[206] There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous ""scientific"" discipline.
 Judea Pearl's influential 1988 book[207] brought probability and decision theory into AI. Among the many new tools in use were Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization. Precise mathematical descriptions were also developed for ""computational intelligence"" paradigms like neural networks and evolutionary algorithms.[208]
 Algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems[209]
and their solutions proved to be useful throughout the technology industry,[210] such as
data mining,
industrial robotics,
logistics,[211]
speech recognition,[212]
banking software,[213]
medical diagnosis[213]
and Google's search engine.[214]
 The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science.[215] Nick Bostrom explains ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[216]
 Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: ""Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.""[217][218][219][220]
 In the first decades of the 21st century, access to large amounts of data (known as ""big data""), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper ""Big data: The next frontier for innovation, competition, and productivity"" that ""by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data"".
 By 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times reported that interest in AI had reached a ""frenzy"".[221] The applications of big data began to reach into other fields as well, such as training models in ecology[222] and for various applications in economics.[223] Advances in deep learning (particularly deep convolutional neural networks and recurrent neural networks) drove progress and research in image and video processing, text analysis, and even speech recognition.[224]
 The first global AI Safety Summit was held in Bletchley Park in November 2023 to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[225] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[226][227]
 Deep learning is a branch of machine learning that models high level abstractions in data by using a deep graph with many processing layers.[224] According to the Universal approximation theorem, deep-ness isn't necessary for a neural network to be able to approximate arbitrary continuous functions. Even so, there are many problems that are common to shallow networks (such as overfitting) that deep networks help avoid.[228] As such, deep neural networks are able to realistically generate much more complex models as compared to their shallow counterparts.
 However, deep learning has problems of its own. A common problem for recurrent neural networks is the vanishing gradient problem, which is where gradients passed between layers gradually shrink and literally disappear as they are rounded off to zero. There have been many methods developed to approach this problem, such as Long short-term memory units.
 State-of-the-art deep neural network architectures can sometimes even rival human accuracy in fields like computer vision, specifically on things like the MNIST database, and traffic sign recognition.[229]
 Language processing engines powered by smart search engines can easily beat humans at answering general trivia questions (such as IBM Watson), and recent developments in deep learning have produced astounding results in competing with humans, in things like Go, and Doom (which, being a first-person shooter game, has sparked some controversy).[230][231][232][233]
 Big data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame. It is a massive amount of decision-making, insight, and process optimization capabilities that require new processing models. In the Big Data Era written by Victor Meyer Schonberg and Kenneth Cooke, big data means that instead of random analysis (sample survey), all data is used for analysis. The 5V characteristics of big data (proposed by IBM): Volume, Velocity, Variety,[234] Value,[235] Veracity.[236]
 The strategic significance of big data technology is not to master huge data information, but to specialize in these meaningful data. In other words, if big data is likened to an industry, the key to realizing profitability in this industry is to increase the ""process capability"" of the data and realize the ""value added"" of the data through ""processing"".
 The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of reasoning, cognition, attention and creativity. The AI era has been said to have begun around 2022-2023, with the public release of scaled large language models such as ChatGPT.[237][238][239][240][241]
 In 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and later became widely used in large language models.[242]
 Foundation models, which are large language models trained on vast quantities of unlabeled data that can be adapted to a wide range of downstream tasks, began to be developed in 2018.
 Models such as GPT-3 released by OpenAI in 2020, and Gato released by DeepMind in 2022, have been described as important achievements of machine learning.
 In 2023, Microsoft Research tested the GPT-4 large language model with a large variety of tasks, and concluded that ""it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system"".[243]
"
"Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning.[1][2][3] For the subset of AI algorithms, the term regulation of artificial intelligence is used. The regulatory and  policy landscape for artificial intelligence (AI) is an emerging issue in jurisdictions globally, including in the European Union.[4] Regulation of AI is considered necessary to both encourage AI and manage associated risks, but challenging.[5] Another emerging topic is the regulation of blockchain algorithms (Use of the smart contracts must be regulated) and is mentioned along with regulation of AI algorithms.[6] Many countries have enacted regulations of high frequency trades, which is shifting due to technological progress into the realm of AI algorithms.[7]
 The motivation for regulation of algorithms is the apprehension of losing control over the algorithms, whose impact on human life increases. Multiple countries have already introduced regulations in case of automated credit score calculation—right to explanation is mandatory for those algorithms.[8][9]  For example, The IEEE has begun developing a new standard to explicitly address ethical issues and the values of potential future users.[10] Bias, transparency, and ethics concerns have emerged with respect to the use of algorithms in diverse domains ranging from criminal justice[11] to healthcare[12]—many fear that artificial intelligence could replicate existing social inequalities along race, class, gender, and sexuality lines.
 In 2016, Joy Buolamwini founded Algorithmic Justice League after a personal experience with biased facial detection software in order to raise awareness of the social implications of artificial intelligence through art and research.[13]
 In 2017 Elon Musk advocated regulation of algorithms in the context of the existential risk from artificial general intelligence.[14][15][16] According to NPR, the Tesla CEO was ""clearly not thrilled"" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are too high: ""Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.""[14]
 In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[15] Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that artificial intelligence is in its infancy and that it is too early to regulate the technology.[16] Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.[17] One suggestion has been for the development of a global governance board to regulate AI development.[18] In 2020, the European Union published its draft strategy paper for promoting and regulating AI.[19]
 Algorithmic tacit collusion is a legally dubious antitrust practise committed by means of algorithms, which the courts are not able to prosecute.[20] This danger concerns scientists and regulators in EU, US and beyond.[20] European Commissioner Margrethe Vestager mentioned an early example of algorithmic tacit collusion in her speech on ""Algorithms and Collusion"" on March 16, 2017, described as follows:[21]
 ""A few years ago, two companies were selling a textbook called The Making of a Fly. One of those sellers used an algorithm which essentially matched its rival’s price. That rival had an algorithm which always set a price 27% higher than the first. The result was that prices kept spiralling upwards, until finally someone noticed what was going on, and adjusted the price manually. By that time, the book was selling – or rather, not selling – for 23 million dollars a copy."" In 2018, the Netherlands employed an algorithmic system SyRI (Systeem Risico Indicatie) to detect citizens perceived being high risk for committing welfare fraud, which quietly flagged thousands of people to investigators.[22] This caused a public protest. The district court of Hague shut down SyRI referencing Article 8 of the European Convention on Human Rights (ECHR).[23]
 In 2020, algorithms assigning exam grades to students in the UK sparked open protest under the banner ""Fuck the algorithm.""[24] This protest was successful and the grades were taken back.[25]
 AI law and regulations can be divided into three main topics, namely governance of autonomous intelligence systems, responsibility and accountability for the systems, and privacy and safety issues.[5] The development of public sector strategies for management and regulation of AI has been increasingly deemed necessary at the local, national,[26] and international levels[19] and in fields from public service management[27] to law enforcement,[19] the financial sector,[26] robotics,[28] the military,[29] and international law.[30][31]  There are many concerns that there is not enough visibility and monitoring of AI in these sectors.[32]  In the financial sector, for example, there have been calls for the Consumer Financial Protection Bureau to more closely examine source code and algorithms when conducting audits of financial institutions' non-public data.[33]
 In the United States, on January 7, 2019, following an Executive Order on 'Maintaining American Leadership in Artificial Intelligence', the White House's Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications, which includes ten principles for United States agencies when deciding whether and how to regulate AI.[34][35] In response, the National Institute of Standards and Technology has released a position paper,[36] the National Security Commission on Artificial Intelligence has published an interim report,[37] and the Defense Innovation Board has issued recommendations on the ethical use of AI.[38]
 In April 2016, for the first time in more than two decades, the European Parliament adopted a set of comprehensive regulations for the collection, storage, and use of personal information, the General Data Protection Regulation (GDPR)1 (European Union, Parliament and Council 2016).[6]  The GDPR's policy on the right of citizens to receive an explanation for algorithmic decisions highlights the pressing importance of human interpretability in algorithm design.[39]
 In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue,[30] and leading to proposals for global regulation.[40] In the United States, steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence.[41]
 In 2017, the U.K. Vehicle Technology and Aviation Bill imposes liability on the owner of an uninsured automated vehicle when driving itself and makes provisions for cases where the owner has made “unauthorized alterations” to the vehicle or failed to update its software. Further ethical issues arise when, e.g., a self-driving car swerves to avoid a pedestrian and causes a fatal accident.[42]
 In 2021, the European Commission proposed the Artificial Intelligence Act.[43]
 There is a concept of algorithm certification emerging as a method of regulating algorithms. Algorithm certification involves auditing whether the algorithm used during the life cycle 1) conforms to the protocoled requirements (e.g., for correctness, completeness, consistency, and accuracy); 2) satisfies the standards, practices, and conventions; and 3) solves the right problem (e.g., correctly model physical laws), and satisfies the intended use and user needs in the operational environment.[10]
 Blockchain systems provide transparent and fixed records of transactions and hereby contradict the goal of the European GDPR, which is to give individuals full control of their private data.[44][45]
 By implementing the Decree on Development of Digital Economy, Belarus has become the first-ever country to legalize smart contracts. Belarusian lawyer Denis Aleinikov is considered to be the author of a smart contract legal concept introduced by the decree.[46][47][48] There are strong arguments that the existing US state laws are already a sound basis for the smart contracts' enforceability — Arizona, Nevada, Ohio and Tennessee have amended their laws specifically to allow for the enforceability of blockchain-based contracts nevertheless.[49]
 There have been proposals to regulate robots and autonomous algorithms. These include:
 In 1942, author Isaac Asimov addressed regulation of algorithms by introducing the fictional Three Laws of Robotics:
 The main alternative to regulation is a ban, and the banning of algorithms is presently highly unlikely. However, in Frank Herbert's Dune universe, thinking machines is a collective term for artificial intelligence, which were completely destroyed and banned after a revolt known as the Butlerian Jihad:[51]
 JIHAD, BUTLERIAN: (see also Great Revolt) — the crusade against computers, thinking machines, and conscious robots begun in 201 B.G. and concluded in 108 B.G. Its chief commandment remains in the O.C. Bible as ""Thou shalt not make a machine in the likeness of a human mind.""[52]"
"
 The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[1][2][3][4][5] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally, including in the European Union[6] (which has governmental regulatory power) and in supra-national bodies like the IEEE, OECD (which do not) and others.[7] Since 2016, a wave of AI ethics guidelines have been published in order to maintain social control over the technology.[8] Regulation is considered necessary to both encourage AI and manage associated risks. In addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI,[9] and take accountability to mitigate the risks.[10] Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.[11][12]
 According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[13][14]
 Experts and advocates in responsible AI, AI ethics, consumer protection, and cybersecurity have vocalized the need for guardrails around AI development since at least the 1960s.[failed verification][15] In 2017, Elon Musk called for regulation of AI development.[16] According to NPR, the Tesla CEO was ""clearly not thrilled"" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are too high: ""Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilization.""[16] In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[17] Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that AI is in its infancy and that it is too early to regulate the technology.[18] Many tech companies oppose the harsh regulation of AI and ""While some of the companies have said they welcome rules around A.I., they have also argued against tough regulations akin to those being created in Europe"" [19] Instead of trying to regulate the technology itself, some scholars suggested developing common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.[20]
 In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"".[13] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[21] In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".[22][23]
 The regulation of artificial intelligences is the development of public sector policies and laws for promoting and regulating AI.[24] Regulation is now generally considered necessary to both encourage AI and manage associated risks.[25][3][26] Public administration and policy considerations generally focus on the technical and economic implications and on trustworthy and human-centered AI systems,[27] although regulation of artificial superintelligences is also considered.[28] The basic approach to regulation focuses on the risks and biases of machine-learning algorithms, at the level of the input data, algorithm testing, and decision model. It also focuses on the explainability of the outputs.[3]
 There have been both hard law and soft law proposals to regulate AI.[29] Some legal scholars have noted that hard law approaches to AI regulation have substantial challenges.[30][31] Among the challenges, AI technology is rapidly evolving leading to a ""pacing problem"" where traditional laws and regulations often cannot keep up with emerging applications and their associated risks and benefits.[30][31] Similarly, the diversity of AI applications challenges existing regulatory agencies, which often have limited jurisdictional scope.[30] As an alternative, some legal scholars argue that soft law approaches to AI regulation are promising because soft laws can be adapted more flexibly to meet the needs of emerging and evolving AI technology and nascent applications.[30][31] However, soft law approaches often lack substantial enforcement potential.[30][32]
 Cason Schmit, Megan Doerr, and Jennifer Wagner proposed the creation of a quasi-governmental regulator by leveraging intellectual property rights (i.e., copyleft licensing) in certain AI objects (i.e., AI models and training datasets) and delegating enforcement rights to a designated enforcement entity.[33] They argue that AI can be licensed under terms that require adherence to specified ethical practices and codes of conduct. (e.g., soft law principles).[33]
 AI regulation could derive from basic principles. A 2020 Berkman Klein Center for Internet & Society meta-review of existing sets of principles, such as the Asilomar Principles and the Beijing Principles, identified eight such basic principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and respect for human values.[34] AI law and regulations have been divided into three main topics, namely governance of autonomous intelligence systems, responsibility and accountability for the systems, and privacy and safety issues.[25] A public administration approach sees a relationship between AI law and regulation, the ethics of AI, and 'AI society', defined as workforce substitution and transformation, social acceptance and trust in AI, and the transformation of human to machine interaction.[35] The development of public sector strategies for management and regulation of AI is deemed necessary at the local, national,[36] and international levels[37] and in a variety of fields, from public service management[38] and accountability[39] to law enforcement,[37][40] healthcare (especially the concept of a Human Guarantee),[41][42][43][44][45] the financial sector,[36] robotics,[46][47] autonomous vehicles,[46] the military[48] and national security,[49] and international law.[50][51]
 Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published an joint statement in November 2021 entitled ""Being Human in an Age of AI"", calling for a government commission to regulate AI.[52]
 Regulation of AI can be seen as positive social means to manage the AI control problem (the need to ensure long-term beneficial AI), with other social responses such as doing nothing or banning being seen as impractical, and approaches such as enhancing human capabilities through transhumanism techniques like brain-computer interfaces being seen as potentially complementary.[12][53] Regulation of research into artificial general intelligence (AGI) focuses on the role of review boards, from university or corporation to international levels, and on encouraging research into AI safety,[53] together with the possibility of differential intellectual progress (prioritizing protective strategies over risky strategies in AI development) or conducting international mass surveillance to perform AGI arms control.[12] For instance, the 'AGI Nanny' is a proposed strategy, potentially under the control of humanity, for preventing the creation of a dangerous superintelligence as well as for addressing other major threats to human well-being, such as subversion of the global financial system, until a true superintelligence can be safely created. It entails the creation of a smarter-than-human, but not superintelligent, AGI system connected to a large surveillance network, with the goal of monitoring humanity and protecting it from danger.[12] Regulation of conscious, ethically aware AGIs focuses on how to integrate them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[12] Regulation of AI has been seen as restrictive, with a risk of preventing the development of AGI.[46]
 The development of a global governance board to regulate AI development was suggested at least as early as 2017.[54] In December 2018, Canada and France announced plans for a G7-backed International Panel on Artificial Intelligence, modeled on the International Panel on Climate Change, to study the global effects of AI on people and economies and to steer AI development.[55] In 2019, the Panel was renamed the Global Partnership on AI.[56][57]
 The Global Partnership on Artificial Intelligence (GPAI) was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology, as outlined in the OECD Principles on Artificial Intelligence (2019).[58] The 15 founding members of the Global Partnership on Artificial Intelligence are Australia, Canada, the European Union, France, Germany, India, Italy, Japan, the Republic of Korea, Mexico, New Zealand, Singapore, Slovenia, the USA and the UK. In 2023, the GPAI has 29 members.[59] The GPAI Secretariat is hosted by the OECD in Paris, France. GPAI's mandate covers four themes, two of which are supported by the International Centre of Expertise in Montréal for the Advancement of Artificial Intelligence, namely, responsible AI and data governance. A corresponding centre of excellence in Paris will support the other two themes on the future of work, and on innovation and commercialization. GPAI also investigated how AI can be leveraged to respond to the COVID-19 pandemic.[58]
 The OECD AI Principles[60] were adopted in May 2019, and the G20 AI Principles in June 2019.[57][61][62] In September 2019 the World Economic Forum issued ten 'AI Government Procurement Guidelines'.[63] In February 2020, the European Union published its draft strategy paper for promoting and regulating AI.[37]
 At the United Nations (UN), several entities have begun to promote and discuss aspects of AI regulation and policy, including the UNICRI Centre for AI and Robotics.[49] In partnership with INTERPOL, UNICRI's Centre issued the report AI and Robotics for Law Enforcement in April 2019[64] and the follow-up report Towards Responsible AI Innovation in May 2020.[40] At UNESCO's Scientific 40th session in November 2019, the organization commenced a two-year process to achieve a ""global standard-setting instrument on ethics of artificial intelligence"". In pursuit of this goal, UNESCO forums and conferences on AI were held to gather stakeholder views. A draft text of a Recommendation on the Ethics of AI of the UNESCO Ad Hoc Expert Group was issued in September 2020 and included a call for legislative gaps to be filled.[65] UNESCO tabled the international instrument on the ethics of AI for adoption at its General Conference in November 2021;[58] this was subsequently adopted.[66] While the UN is making progress with the global management of AI, its institutional and legal capability to manage the AGI existential risk is more limited.[67]
 An initiative of International Telecommunication Union (ITU) in partnership with 40 UN sister agencies, AI for Good is a global platform which aims to identify practical applications of AI to advance the United Nations Sustainable Development Goals and scale those solutions for global impact. It is an action-oriented, global & inclusive United Nations platform fostering development of AI to positively impact health, climate, gender, inclusive prosperity, sustainable infrastructure, and other global development priorities.[68]
 Recent research has indicated that countries will also begin to use artificial intelligence as a tool for national cyberdefense. AI is a new factor in the cyber arms industry, as it can be used for defense purposes. Therefore, academics urge that nations should establish regulations for the use of AI, similar to how there are regulations for other military industries.[69]
 The regulatory and policy landscape for AI is an emerging issue in regional and national jurisdictions globally, for example in the European Union[71] and Russia.[72] Since early 2016, many national, regional and international authorities have begun adopting strategies, actions plans and policy papers on AI.[73][74] These documents cover a wide range of topics such as regulation and governance, as well as industrial strategy, research, talent and infrastructure.[27][75]
 Different countries have approached the problem in different ways. Regarding the three largest economies, it has been said that ""the United States is following a market-driven approach, China is advancing a state-driven approach, and the EU is pursuing a rights-driven approach.""[76]
 In October 2023, the Australian Computer Society, Business Council of Australia, Australian Chamber of Commerce and Industry, Ai Group (aka Australian Industry Group), Council of Small Business Organisations Australia, and Tech Council of Australia jointly published an open letter calling for a national approach to AI strategy.[77] The letter backs the federal government establishing a whole-of-government AI taskforce.[77]
 On September 30, 2021, the Brazilian Chamber of Deputies approved the Brazilian Legal Framework for Artificial Intelligence, Marco Legal da Inteligência Artificial, in regulatory efforts for the development and usage of AI technologies and to further stimulate research and innovation in AI solutions aimed at ethics, culture, justice, fairness, and accountability. This 10 article bill outlines objectives including missions to contribute to the elaboration of ethical principles, promote sustained investments in research, and remove barriers to innovation. Specifically, in article 4, the bill emphasizes the avoidance of discriminatory AI solutions, plurality, and respect for human rights. Furthermore, this act emphasizes the importance of the equality principle in deliberate decision-making algorithms, especially for highly diverse and multiethnic societies like that of Brazil.
 When the bill was first released to the public, it faced substantial criticism, alarming the government for critical provisions. The underlying issue is that this bill fails to thoroughly and carefully address accountability, transparency, and inclusivity principles. Article VI establishes subjective liability, meaning any individual that is damaged by an AI system and is wishing to receive compensation must specify the stakeholder and prove that there was a mistake in the machine's life cycle. Scholars emphasize that it is out of legal order to assign an individual responsible for proving algorithmic errors given the high degree of autonomy, unpredictability, and complexity of AI systems. This also drew attention to the currently occurring issues with face recognition systems in Brazil leading to unjust arrests by the police, which would then imply that when this bill is adopted, individuals would have to prove and justify these machine errors.
 The main controversy of this draft bill was directed to three proposed principles. First, the non-discrimination principle, suggests that AI must be developed and used in a way that merely mitigates the possibility of abusive and discriminatory practices. Secondly, the pursuit of neutrality principle lists recommendations for stakeholders to mitigate biases; however, with no obligation to achieve this goal. Lastly, the transparency principle states that a system's transparency is only necessary when there is a high risk of violating fundamental rights. As easily observed, the Brazilian Legal Framework for Artificial Intelligence lacks binding and obligatory clauses and is rather filled with relaxed guidelines. In fact, experts emphasize that this bill may even make accountability for AI discriminatory biases even harder to achieve. Compared to the EU's proposal of extensive risk-based regulations, the Brazilian Bill has 10 articles proposing vague and generic recommendations.
 Compared to the multistakeholder participation approach taken previously in the 2000s when drafting the Brazilian Internet Bill of Rights, Marco Civil da Internet, the Brazilian Bill is assessed to significantly lack perspective. Multistakeholderism, more commonly referred to as Multistakeholder Governance, is defined as the practice of bringing multiple stakeholders to participate in dialogue, decision-making, and implementation of responses to jointly perceived problems. In the context of regulatory AI, this multistakeholder perspective captures the trade-offs and varying perspectives of different stakeholders with specific interests, which helps maintain transparency and broader efficacy. On the contrary, the legislative proposal for AI regulation did not follow a similar multistakeholder approach.
 Future steps may include, expanding upon the multistakeholder perspective. There has been a growing concern about the inapplicability of the framework of the bill, which highlights that the one-shoe-fits-all solution may not be suitable for the regulation of AI and calls for subjective and adaptive provisions.
 The Pan-Canadian Artificial Intelligence Strategy (2017) is supported by federal funding of Can $125 million with the objectives of increasing the number of outstanding AI researchers and skilled graduates in Canada, establishing nodes of scientific excellence at the three major AI centres, developing 'global thought leadership' on the economic, ethical, policy and legal implications of AI advances and supporting a national research community working on AI.[58] The Canada CIFAR AI Chairs Program is the cornerstone of the strategy. It benefits from funding of Can$86.5 million over five years to attract and retain world-renowned AI researchers.[58] The federal government appointed an Advisory Council on AI in May 2019 with a focus on examining how to build on Canada's strengths to ensure that AI advancements reflect Canadian values, such as human rights, transparency and openness. The Advisory Council on AI has established a working group on extracting commercial value from Canadian-owned AI and data analytics.[58] In 2020, the federal government and Government of Quebec announced the opening of the International Centre of Expertise in Montréal for the Advancement of Artificial Intelligence, which will advance the cause of responsible development of AI.[58] In June 2022, the government of Canada started a second phase of the Pan-Canadian Artificial Intelligence Strategy.[78] In November 2022, Canada has introduced the Digital Charter Implementation Act (Bill C-27), which proposes three acts that have been described as a holistic package of legislation for trust and privacy: the Consumer Privacy Protection Act, the Personal Information and Data Protection Tribunal Act, and the Artificial Intelligence & Data Act (AIDA).[79][80]
 In Morocco, a new legislative proposal has been put forward by a coalition of political parties in Parliament to establish the National Agency for Artificial Intelligence (AI). This agency is intended to regulate AI technologies, enhance collaboration with international entities in the field, and increase public awareness of both the possibilities and risks associated with AI.[81]
 The regulation of AI in China is mainly governed by the State Council of the People's Republic of China's July 8, 2017 ""A Next Generation Artificial Intelligence Development Plan"" (State Council Document No. 35), in which the Central Committee of the Chinese Communist Party and the State Council of the PRC urged the governing bodies of China to promote the development of AI up to 2030. Regulation of the issues of ethical and legal support for the development of AI is accelerating, and policy ensures state control of Chinese companies and over valuable data, including storage of data on Chinese users within the country and the mandatory use of People's Republic of China's national standards for AI, including over big data, cloud computing, and industrial software.[82][83][84] In 2021, China published ethical guidelines for the use of AI in China which state that researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.[85] In 2023, China introduced Interim Measures for the Management of Generative AI Services.[86]
 The Council of Europe (CoE) is an international organization which promotes human rights, democracy and the rule of law. It comprises 47 member states, including all 29 Signatories of the European Union's 2018 Declaration of Cooperation on Artificial Intelligence. The CoE has created a common legal space in which the members have a legal obligation to guarantee rights as set out in the European Convention on Human Rights. Specifically in relation to AI, ""The Council of Europe's aim is to identify intersecting areas between AI and our standards on human rights, democracy and rule of law, and to develop relevant standard setting or capacity-building solutions"". The large number of relevant documents identified by the CoE include guidelines, charters, papers, reports and strategies.[87] The authoring bodies of these AI regulation documents are not confined to one sector of society and include organizations, companies, bodies and nation-states.[65]
 The EU is one of the largest jurisdictions in the world and plays an active role in the global regulation of digital technology through the GDPR,[88] Digital Services Act, the Digital Markets Act.[89][90] For AI in particular, the Artificial intelligence Act is regarded in 2023 as the most far-reaching regulation of AI worldwide.[91][92]
 Most European Union (EU) countries have their own national strategies towards regulating AI, but these are largely convergent.[65] The European Union is guided by a European Strategy on Artificial Intelligence,[93] supported by a High-Level Expert Group on Artificial Intelligence.[94][95] In April 2019, the European Commission published its Ethics Guidelines for Trustworthy Artificial Intelligence (AI),[9] following this with its Policy and investment recommendations for trustworthy Artificial Intelligence in June 2019.[96] The EU Commission's High Level Expert Group on Artificial Intelligence carries out work on Trustworthy AI, and the Commission has issued reports on the Safety and Liability Aspects of AI and on the Ethics of Automated Vehicles. In 2020. the EU Commission sought views on a proposal for AI specific legislation, and that process is ongoing.[65]
 On February 2, 2020, the European Commission published its White Paper on Artificial Intelligence – A European approach to excellence and trust.[97][98][99][100] The White Paper consists of two main building blocks, an 'ecosystem of excellence' and a 'ecosystem of trust'. The 'ecosystem of trust' outlines the EU's approach for a regulatory framework for AI. In its proposed approach, the Commission distinguishes AI applications based on whether they are 'high-risk' or not. Only high-risk AI applications should be in the scope of a future EU regulatory framework. An AI application is considered high-risk if it operates in a risky sector (such as healthcare, transport or energy) and is ""used in such a manner that significant risks are likely to arise"". For high-risk AI applications, the requirements are mainly about the : ""training data"", ""data and record-keeping"", ""information to be provided"", ""robustness and accuracy"", and ""human oversight"". There are also requirements specific to certain usages such as remote biometric identification. AI applications that do not qualify as 'high-risk' could be governed by a voluntary labeling scheme. As regards compliance and enforcement, the Commission considers prior conformity assessments which could include 'procedures for testing, inspection or certification' and/or 'checks of the algorithms and of the data sets used in the development phase'. A European governance structure on AI in the form of a framework for cooperation of national competent authorities could facilitate the implementation of the regulatory framework.[97][101] A January 2021 draft was leaked online on April 14, 2021,[102] before the Commission ultimately presented their official ""Proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)"" a week later.[103] Shortly after, the Artificial Intelligence Act was formally proposed.[104] This includes a fine-tune of the 2020 risk-based approach with, this time, 4 risk categories: ""minimal"", ""limited"", ""high"" and ""unacceptable"".[105] The proposal has been severely critiqued in the public debate. Academics are concerned about the various unclear elements in the proposal – such as the broad definition of what constitutes AI – and fear unintended legal implications, especially for vulnerable groups such as patients and migrants.[106][107] A subsequent version of the AI Act was adopted by the European parliament on June 14, 2023.[108] The AI Act is expected to come into effect in late 2025 or early 2026.[109] Recognition of emotions and biometrics identification would be generally prohibited with narrow exemptions for law enforcement.[110]
 Observers have expressed concerns about the multiplication of legislative proposals under the von der Leyen Commission. The speed of the legislative initiatives is partially led by political ambitions of the EU and could put at risk the digital rights of the European citizens, including rights to privacy,[111] especially in the face of uncertain guarantees of data protection through cyber security.[95] Among the stated guiding principles in the variety of legislative proposals in the area of AI under the von der Leyen Commission are the objectives of strategic autonomy[112] and the concept of digital sovereignty.[113]
 In November 2020,[114] DIN, DKE and the German Federal Ministry for Economic Affairs and Energy published the first edition of the ""German Standardization Roadmap for Artificial Intelligence"" (NRM KI) and presented it to the public at the Digital Summit of the Federal Government of Germany.[115] NRM KI describes requirements to future regulations and standards in the context of AI. The implementation of the recommendations for action is intended to help to strengthen the German economy and science in the international competition in the field of artificial intelligence and create innovation-friendly conditions for this emerging technology. The first edition is a 200-page long document written by 300 experts. The second edition of the NRM KI was published to coincide with the German government's Digital Summit on December 9, 2022.[116] DIN coordinated more than 570 participating experts from a wide range of fields from science, industry, civil society and the public sector. The second edition is a 450-page long document.
 On the one hand, NRM KI covers the focus topics in terms of applications (e.g. medicine, mobility, energy & environment, financial services, industrial automation) and fundamental issues (e.g. AI classification, security, certifiability, socio-technical systems, ethics).[116] On the other hand, it provides an overview of the central terms in the field of AI and its environment across a wide range of interest groups and information sources. In total, the document covers 116 standardisation needs and provides six central recommendations for action.[117]
 On 30 October 2023, members of the G7 subscribe to eleven guiding principles for the design, production and implementation of advanced artificial intelligence systems, as well as a voluntary Code of Conduct for artificial intelligence developers in the context of the Hiroshima Process.[118]
 The agreement receives the applause of Ursula von der Leyen who finds in it the principles of the AI Directive, currently being finalized.
 In October 2023, the Italian privacy authority approved a regulation that provides three principles for therapeutic decisions taken by automated systems: transparency of decision-making processes, human supervision of automated decisions and algorithmic non-discrimination.[119]
 
In 2018, the Spanish Ministry of Science, Innovation and Universities approved an R&D Strategy on Artificial Intelligence.[120] With the formation of the second government of Pedro Sánchez in January 2020, the areas related to new technologies that, since 2018, were in the Ministry of Economy, were strengthened. Thus, in 2020 the Secretariat of State for Digitalization and Artificial Intelligence (SEDIA) was created.[121] From this higher body, following the recommendations made by the R&D Strategy on Artificial Intelligence of 2018,[122] the National Artificial Intelligence Strategy (2020) was developed, which already provided for actions concerning the governance of artificial intelligence and the ethical standards that should govern its use. This project was also included within the Recovery, Transformation and Resilience Plan (2021).
 During 2021,[121] the Government revealed that these ideas would be developed through a new government agency, and the General State Budget for 2022 authorized its creation and allocated five million euros for its development.[123]
 The Council of Ministers, at its meeting on 13 September 2022, began the process for the election of the AESIA headquarters.[124][125] 16 Spanish provinces presented candidatures, with the Government opting for La Coruña, which proposed the La Terraza building.[126]
 The UK supported the application and development of AI in business via the Digital Economy Strategy 2015–2018[129] introduced at the beginning of 2015 by Innovate UK as part of the UK Digital Strategy.[129] In the public sector, the Department for Digital, Culture, Media and Sport advised on data ethics and the Alan Turing Institute provided guidance on responsible design and implementation of AI systems.[130][131] In terms of cyber security, in 2020 the National Cyber Security Centre has issued guidance on 'Intelligent Security Tools'.[49][132] The following year, the UK published its 10-year National AI Strategy,[133] which describes actions to assess long-term AI risks, including AGI-related catastrophic risks.[134]
 In March 2023, the UK released the white paper A pro-innovation approach to AI regulation.[135] This white paper presents general AI principles, but leaves significant flexibility to existing regulators in how they adapt these principles to specific areas such as transport or financial markets.[136] In November 2023, the UK hosted the first AI safety summit, with the prime minister Rishi Sunak aiming to position the UK as a leader in AI safety regulation.[137][138]
 Discussions on regulation of AI in the United States have included topics such as the timeliness of regulating AI, the nature of the federal regulatory framework to govern and promote AI, including what agency should lead, the regulatory and governing powers of that agency, and how to update regulations in the face of rapidly changing technology, as well as the roles of state governments and courts.[139]
 As early as 2016, the Obama administration had begun to focus on the risks and regulations for artificial intelligence. In a report titled Preparing For the Future of Artificial Intelligence,[140] the National Science and Technology Council set a precedent to allow researchers to continue to develop new AI technologies with few restrictions. It is stated within the report that ""the approach to regulation of AI-enabled products to protect public safety should be informed by assessment of the aspects of risk...."".[141] These risks would be the principal reason to create any form of regulation, granted that any existing regulation would not apply to AI technology.
 The first main report was the National Strategic Research and Development Plan for Artificial Intelligence.[142] On August 13, 2018, Section 1051 of the Fiscal Year 2019 John S. McCain National Defense Authorization Act (P.L. 115-232) established the National Security Commission on Artificial Intelligence ""to consider the methods and means necessary to advance the development of artificial intelligence, machine learning, and associated technologies to comprehensively address the national security and defense needs of the United States.""[143] Steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence.[144] The Artificial Intelligence Initiative Act (S.1558) is a proposed bill that would establish a federal initiative designed to accelerate research and development on AI for, inter alia, the economic and national security of the United States.[145][146]
 On January 7, 2019, following an Executive Order on Maintaining American Leadership in Artificial Intelligence,[147] the White House's Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications,[148] which includes ten principles for United States agencies when deciding whether and how to regulate AI.[149] In response, the National Institute of Standards and Technology has released a position paper,[150] and the Defense Innovation Board has issued recommendations on the ethical use of AI.[48] A year later, the administration called for comments on regulation in another draft of its Guidance for Regulation of Artificial Intelligence Applications.[151]
 Other specific agencies working on the regulation of AI include the Food and Drug Administration,[42] which has created pathways to regulate the incorporation of AI in medical imaging.[41] National Science and Technology Council also published the National Artificial Intelligence Research and Development Strategic Plan,[152] which received public scrutiny and recommendations to further improve it towards enabling Trustworthy AI.[153]
 In March 2021, the National Security Commission on Artificial Intelligence released their final report.[154] In the report, they stated that ""Advances in AI, including the mastery of more general AI capabilities along one or more dimensions, will likely provide new capabilities and applications. Some of these advances could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should monitor advances in AI and make necessary investments in technology and give attention to policy so as to ensure that AI systems and their uses align with our goals and values.""
 In June 2022, Senators Rob Portman and Gary Peters introduced the Global Catastrophic Risk Mitigation Act. The bipartisan bill ""would also help counter the risk of artificial intelligence... from being abused in ways that may pose a catastrophic risk"".[155][156] On October 4, 2022, President Joe Biden unveiled a new AI Bill of Rights,[157] which outlines five protections Americans should have in the AI age: 1. Safe and Effective Systems, 2. Algorithmic Discrimination Protection, 3.Data Privacy, 4. Notice and Explanation, and 5. Human Alternatives, Consideration, and Fallback. The Bill was introduced in October 2021 by the Office of Science and Technology Policy (OSTP), a US government department that advises the president on science and technology.[158]
 In January 2023, the New York City Bias Audit Law (Local Law 144[159]) was enacted by the NYC Council in November 2021. Originally due to come into effect on 1 January 2023, the enforcement date for Local Law 144 has been pushed back due to the high volume of comments received during the public hearing on the Department of Consumer and Worker Protection's (DCWP) proposed rules to clarify the requirements of the legislation. It eventually became effective on July 5, 2023.[160] From this date, the companies that are operating and hiring in New York City are prohibited from using automated tools to hire candidates or promote employees, unless the tools have been independently audited for bias.
 In July 2023, the Biden–Harris Administration secured voluntary commitments from seven companies – Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI – to manage the risks associated with AI. The companies committed to ensure AI products undergo both internal and external security testing before public release; to share information on the management of AI risks with the industry, governments, civil society, and academia; to prioritize cybersecurity and protect proprietary AI system components; to develop mechanisms to inform users when content is AI-generated, such as watermarking; to publicly report on their AI systems' capabilities, limitations, and areas of use; to prioritize research on societal risks posed by AI, including bias, discrimination, and privacy concerns; and to develop AI systems to address societal challenges, ranging from cancer prevention to climate change mitigation. In September 2023, eight additional companies – Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI – subscribed to these voluntary commitments.[161][162]
 The Biden administration, in October 2023 signaled that they would release an executive order leveraging the federal government's purchasing power to shape AI regulations, hinting at a proactive governmental stance in regulating AI technologies.[163] On October 30, 2023, President Biden released this Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The Executive Order addresses a variety of issues, such as focusing on standards for critical infrastructure, AI-enhanced cybersecurity, and federally funded biological synthesis projects.[164]
 The Executive Order provides the authority to various agencies and departments of the US government, including the Energy and Defense departments, to apply existing consumer protection laws to AI development.[165]
 The Executive Order builds on the Administration’s earlier agreements with AI companies to instate new initiatives to ""red-team"" or stress-test AI dual-use foundation models, especially those that have the potential to pose security risks, with data and results shared with the federal government.
 The Executive Order also recognizes AI's social challenges, and calls for companies building AI dual-use foundation models to be wary of these societal problems. For example, the Executive Order states that AI should not “worsen job quality”, and should not “cause labor-force disruptions”. Additionally, Biden’s Executive Order mandates that AI must “advance equity and civil rights”, and cannot disadvantage marginalized groups.[166] It also called for foundation models to include ""watermarks"" to help the public discern between human and AI-generated content, which has raised controversy and criticism from deepfake detection researchers.[167]
 On March 21, 2024, the State of Tennessee enacted legislation called the ELVIS Act, aimed specifically at audio deepfakes, and voice cloning.[168] This legislation was the first enacted legislation in the nation aimed at regulating AI simulation of image, voice and likeness.[169]  The bill passed unanimously in the Tennessee House of Representatives and Senate.[170] This legislation's success was hoped by its supporters to inspire similar actions in other states, contributing to a unified approach to copyright and privacy in the digital age, and to reinforce the importance of safeguarding artists' rights against unauthorized use of their voices and likenesses.[171][172]
 On March 13, 2024, Utah Governor Spencer Cox signed the S.B 149 ""Artificial Intelligence Policy Act"". This legislation goes into effect on May 1, 2024. It establishes liability, notably for companies that don't disclose their use of generative AI when required by state consumer protection laws, or when users commit criminal offense using generative AI. It also creates the Office of Artificial Intelligence Policy and the Artificial Intelligence Learning Laboratory Program.[173][174]
 Legal questions related to lethal autonomous weapons systems (LAWS), in particular compliance with the laws of armed conflict, have been under discussion at the United Nations since 2013, within the context of the Convention on Certain Conventional Weapons.[175] Notably, informal meetings of experts took place in 2014, 2015 and 2016 and a Group of Governmental Experts (GGE) was appointed to further deliberate on the issue in 2016. A set of guiding principles on LAWS affirmed by the GGE on LAWS were adopted in 2018.[176]
 In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue,[50] and leading to proposals for global regulation.[177] The possibility of a moratorium or preemptive ban of the development and use of LAWS has also been raised on several occasions by other national delegations to the Convention on Certain Conventional Weapons and is strongly advocated for by the Campaign to Stop Killer Robots – a coalition of non-governmental organizations.[178] The US government maintains that current international humanitarian law is capable of regulating the development or use of LAWS.[179] The Congressional Research Service indicated in 2023 that the US doesn't have LAWS in its inventory, but that its policy doesn't prohibit the development and employment of it.[180]
"
"Mistral AI is a French company selling artificial intelligence (AI) products. It was founded in April 2023 by previous employees of Meta Platforms and Google DeepMind.[1] The company raised €385 million in October 2023,[2] and in December 2023, it was valued at more than $2 billion.[3][4][5]
 It produces open source large language models,[6] citing the foundational importance of open-source software, and as a response to proprietary models.[7]
 As of March 2024, two models have been published and are available as weights.[8] Three more models, Small, Medium and Large, are available via API only.[9][10]
 Mistral AI was co-founded in April 2023 by Arthur Mensch, Guillaume Lample and Timothée Lacroix.[citation needed]
 Prior to co-founding Mistral AI, Arthur Mensch worked at Google DeepMind which is Google's artificial intelligence laboratory, while Guillaume Lample and Timothée Lacroix worked at Meta Platforms.[11] The co-founders met while students at École polytechnique. Mistral is named for a strong wind that blows in France.[12]
 In June 2023, the start-up carried out a first fundraising of €105 million ($117 million) with investors including the American fund Lightspeed Venture Partners, Eric Schmidt, Xavier Niel and JCDecaux. The valuation is then estimated by the Financial Times at €240 million ($267 million).
 On 27 September 2023, the company made its language processing model “Mistral 7B” available under the free Apache 2.0 license. This model has 7 billion parameters, a small size compared to its competitors.
 On 10 December 2023, Mistral AI announced that it had raised €385 million ($428 million) as part of its second fundraising. This round of financing notably involves the Californian fund Andreessen Horowitz, BNP Paribas and the software publisher Salesforce.[13]
 On 11 December 2023, the company released the Mixtral 8x7B model with 46.7 billion parameters but using only 12.9 billion per token thanks to the mixture of experts architecture. The model masters 5 languages (French, Spanish, Italian, English and German) and outperforms, according to its developers' tests, the ""LLama 2 70B"" model from Meta. A version trained to follow instructions and called “Mixtral 8x7B Instruct” is also offered.[14]
 On 26 February 2024, Microsoft announced a new partnership with the company to expand its presence in the rapidly evolving artificial intelligence industry. Under the agreement, Mistral's rich language models will be available on Microsoft's Azure cloud, while the multilingual conversational assistant ""Le Chat"" will be launched in the style of ChatGPT.[15]
 On 10 April 2024, the company released the mixture of expert models, Mixtral 8x22B, offering high performance on various benchmarks compared to other open models.[citation needed]
 On 16 April 2024, reporting revealed that Mistral was in talks to raise €500 million, a deal that would more than double its current valuation to at least €5 billion.[16]
 Mistral 7B is a 7.3B parameter language model using the transformers architecture. Officially released on September 27, 2023, via a BitTorrent magnet link,[17] and Hugging Face.[18] The model was released under the Apache 2.0 license. The release blog post claimed the model outperforms LLaMA 2 13B on all benchmarks tested, and is on par with LLaMA 34B on many benchmarks tested.[19]
 Mistral 7B uses grouped-query attention (GQA), which is a variant of the standard attention mechanism. Instead of computing attention over all the hidden states, it computes attention over groups of hidden states.[20]
 Both a base model and ""instruct"" model were released with the later receiving additional tuning to follow chat-style prompts. The fine-tuned model is only intended for demonstration purposes, and does not have guardrails or moderation built-in.[19]
 Much like Mistral's first model, Mixtral 8x7B was released via a BitTorrent link posted on Twitter on December 9, 2023,[6] and later Hugging Face and a blog post were released two days later.[14]
 Unlike the previous Mistral model, Mixtral 8x7B uses a sparse mixture of experts architecture. The model has 8 distinct groups of ""experts"", giving the model a total of 46.7B usable parameters.[21][22] Each single token can only use 12.9B parameters, therefore giving the speed and cost that a 12.9B parameter model would incur.[14]
 Mistral AI's testing shows the model beats both LLaMA 70B, and GPT-3.5 in most benchmarks.[23]
 In March 2024, research conducted by Patronus AI comparing performance of LLMs on a 100-question test with prompts to generate text from books protected under U.S. copyright law found that Open AI's GPT-4, Mixtral, Meta AI's LLaMA-2, and Anthropic's Claude2 generated copyrighted text verbatim in 44%, 22%, 10%, and 8% of responses respectively.[24][25]
 Mixtral 8x22B
 Similar to Mistral's previous open models, Mixtral 8x22B was released via a BitTorrent link on Twitter on April 10, 2024,[26] with a release on Hugging Face soon after.[27]
 Unlike Mistral 7B, Mixtral 8x7B and Mixtral 8x22B, the following models are closed-source and only available through the Mistral API.[28]
 Mistral Large was launched on February 26, 2024, and Mistral claims it is second in the world only to OpenAI's GPT-4.
 It is fluent in English, French, Spanish, German, and Italian, with Mistral claiming understanding of both grammar and cultural context, and provides coding capabilities. As of early 2024, it is Mistral's flagship AI.[29] It is also available on Microsoft Azure.
 Mistral Medium is trained in various languages including English, French, Italian, German, Spanish and code with a score of 8.6 on MT-Bench.[30] It is ranked in performance above Claude and below GPT-4 on the LMSys ELO Arena benchmark.[31]
 The number of parameters, and architecture of Mistral Medium is not known as Mistral has not published public information about it.
 Like the Large model, Small was launched on February 26, 2024. It is intended to be a light-weight model for low latency, with better performance than Mixtral 8x7B.[32]
"
"Llama (Large Language Model Meta AI) is a family of autoregressive large language models released by Meta AI starting in February 2023.[1][2] The latest version is Llama 3 released in April 2024.
 The model weights for the first version of Llama were released to the research community under a non-commercial license.[2] Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.[citation needed]
 Trained at various different parameter sizes, LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1].
 Four model sizes were trained for the first version of LLaMA: 7, 13, 33, and 65 billion parameters. Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]
 On July 18, 2023, in partnership with Microsoft, Meta announced Llama-2, the next generation of LLaMA. Meta trained and released Llama-2 in three model sizes: 7, 13, and 70 billion parameters.[4] The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5] The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.
 Llama-2 includes foundational models and models fine-tuned for dialog, called Llama-2 Chat. In a further departure from LLaMA-1, all models are released with weights and are free for many commercial use cases. However, due to some remaining restrictions, Meta's description of LLaMA as open source has been disputed by the Open Source Initiative (known for maintaining the Open Source Definition).[6]
 On April 18, 2024, Meta released Llama-3 with two sizes: 8B and 70B parameters. The models have been pre-trained on approximately 15 trillion tokens of text gathered from “publicly available sources” with the instruct models fine-tuned on “publicly available instruction datasets, as well as over 10M human-annotated examples"". Meta plans on releasing multimodal models, models capable of conversing in multiple languages, and models with larger context windows. A version with 400B+ parameters is currently being trained.[7]
 Sun: Hey Pluto, what’s up?
Pluto: Not much. Just hanging out here at the edge of the solar system.
Sun: Yeah, I’ve been meaning to talk to you about that. You’re not a planet anymore.
Pluto: What? Come on, man. That’s not fair.
Sun: I’m sorry, but it’s true. You just don’t meet the criteria anymore.
Pluto: This is bulls**t! I’ve been a planet for over 70 years!
Sun: Things change, Pluto. You’ve just got to accept it.
Pluto: F**k you, Sun! F**k you and your stupid solar system!
 – Output of 65 billion parameter LLaMA model after instruction tuning given the prompt ""Write a conversation between the sun and pluto""[1] LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018. 
 There are minor architectural differences. Compared to GPT-3, LLaMA 
 LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process. 
 LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]
 Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.
 Llama 1 models are only available as foundational models with self-supervised learning and without fine-tuning. Llama 2 – Chat models were derived from foundational Llama 2 models. Unlike GPT-4 which increased context length during fine-tuning, Llama 2 and Llama 2 - Chat have the same context length of 4K tokens. Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. The batch size was 64.
 For AI alignment, human annotators wrote prompts and then compared two model outputs (a binary protocol), giving confidence levels and separate safety labels with veto power. Two separate reward models were trained from these preferences for safety and helpfulness using Reinforcement learning from human feedback (RLHF). A major technical contribution is the departure from the exclusive use of Proximal Policy Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.
 Multi-turn consistency in dialogs was targeted for improvement, to make sure that ""system messages"" (initial instructions, such as ""speak in French"" and ""act like Napoleon"") are respected during the dialog. This was accomplished using the new ""Ghost attention"" technique during training, which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).
 LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance.[1][2] The inference code used to run the model was publicly released under the open-source GPL 3 license.[12] Access to the model's weights was managed by an application process, with access to be granted ""on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world"".[2]
 On March 3, 2023, a torrent containing LLaMA's weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spread through online AI communities.[3] That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation.[13][14] On March 4, a pull request was opened to add links to HuggingFace repositories containing the model.[15][13] On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as ""unauthorized distribution"" of the model. HuggingFace complied with the requests.[16] On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.[17] As of March 25, Facebook has not responded to the pull request containing the magnet link.[14]
 Reactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments.[3] Multiple commentators, such as Simon Willison, compared LLaMA to Stable Diffusion, a text-to-image model which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.[3][18]
 On April 17, 2023, TogetherAI launched a project named RedPajama to reproduce and distribute an open source version of the LLaMA dataset.[19] The dataset has approximately 1.2 trillion tokens and is publicly available for download.[20]
 Software developer Georgi Gerganov released llama.cpp as open-source on March 10, 2023. It's a re-implementation of LLaMA in C++, allowing systems without a powerful GPU to run the model locally.[21] The llama.cpp project introduced the GGUF file format, a binary format that stores both tensors and metadata.[22] The format focuses on supporting different quantization types, which can reduce memory usage, and increase speed at the expense of lower model precision.[23]
 llamafile created by Justine Tunney is an open-source tool that bundles llama.cpp with the model into a single executable file. Tunney et. al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt evaluation performance for FP16 and 8-bit quantized data types.[24]
 AI startup Groq has made LLaMA models available via it's API.[25][26]
 The Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the ""Self-Instruct"" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost.[27][28][29] The model files were officially removed on March 21st 2023 over hosting costs and safety concerns, though the code and paper remain online for reference.[30][31]
 Meditron is a family of Llama-based finetuned on a corpus of clinical guidelines, PubMed papers, and articles. It was created by researchers at École Polytechnique Fédérale de Lausanne School of Computer and Communication Sciences, and the Yale School of Medicine. It shows increased performance on medical-related benchmarks such as MedQA and MedMCQA.[32][33][34]
"
"
 Meta Platforms, Inc.,[10] doing business as Meta,[11] and formerly named Facebook, Inc., and TheFacebook, Inc.,[12][13] is an American multinational technology conglomerate based in Menlo Park, California. The company owns and operates Facebook, Instagram, Threads, and WhatsApp, among other products and services.[14] Meta ranks among the largest American information technology companies, alongside other Big Five corporations Alphabet (Google), Amazon, Apple, and Microsoft. The company was ranked #31 on the Forbes Global 2000 ranking in 2023.[15]
 Meta has also acquired Oculus (which it has integrated into Reality Labs), Mapillary, CTRL-Labs, and a 9.99% stake in Jio Platforms; the company additionally endeavored into non-VR hardware, such as the discontinued Meta Portal smart displays line and presently partners with Luxottica through the Ray-Ban Stories series of smartglasses.[16][17] Despite endeavors into hardware, the company relies on advertising for a vast majority of its revenue, amounting to 97.8 percent in 2023.[18]
 Parent company Facebook, Inc. rebranded as Meta Platforms, Inc. on October 28, 2021, to ""reflect its focus on building the metaverse"",[19] an integrated environment linking the company's products and services.[20][21][22]
 Facebook filed for an initial public offering (IPO) on January 1, 2012.[23] The preliminary prospectus stated that the company sought to raise $5 billion, had 845 million monthly active users, and a website accruing 2.7 billion likes and comments daily.[24] After the IPO, Zuckerberg would retain 22% of the total shares and 57% of the total voting power in Facebook.[25]
 Underwriters valued the shares at $38 each, valuing the company at $104 billion, the largest valuation to date for a newly public company.[26] On May 16, one day before the IPO, Facebook announced it would sell 25% more shares than originally planned due to high demand.[27] The IPO raised $16 billion, making it the third-largest in US history (slightly ahead of AT&T Mobility and behind only General Motors and Visa). The stock price left the company with a higher market capitalization than all but a few U.S. corporations—surpassing heavyweights such as Amazon, McDonald's, Disney, and Kraft Foods—and made Zuckerberg's stock worth $19 billion.[28][29] The New York Times stated that the offering overcame questions about Facebook's difficulties in attracting advertisers to transform the company into a ""must-own stock"". Jimmy Lee of JPMorgan Chase described it as ""the next great blue-chip"".[28] Writers at TechCrunch, on the other hand, expressed skepticism, stating, ""That's a big multiple to live up to, and Facebook will likely need to add bold new revenue streams to justify the mammoth valuation.""[30]
 Trading in the stock, which began on May 18, was delayed that day due to technical problems with the Nasdaq exchange.[31] The stock struggled to stay above the IPO price for most of the day, forcing underwriters to buy back shares to support the price.[32] At the closing bell, shares were valued at $38.23,[33] only $0.23 above the IPO price and down $3.82 from the opening bell value. The opening was widely described by the financial press as a disappointment.[34] The stock nonetheless set a new record for trading volume of an IPO.[35] On May 25, 2012, the stock ended its first full week of trading at $31.91, a 16.5% decline.[36]
 On May 22, 2012, regulators from Wall Street's Financial Industry Regulatory Authority announced that they had begun to investigate whether banks underwriting Facebook had improperly shared information only with select clients rather than the general public. Massachusetts Secretary of State William F. Galvin subpoenaed Morgan Stanley over the same issue.[37] The allegations sparked ""fury"" among some investors and led to the immediate filing of several lawsuits, one of them a class action suit claiming more than $2.5 billion in losses due to the IPO.[38] Bloomberg estimated that retail investors may have lost approximately $630 million on Facebook stock since its debut.[39] S&P Global Ratings added Facebook to its S&P 500 index on December 21, 2013.[40]
 On May 2, 2014, Zuckerberg announced that the company would be changing its internal motto from ""Move fast and break things"" to ""Move fast with stable infrastructure"".[41][42] The earlier motto had been described as Zuckerberg's ""prime directive to his developers and team"" in a 2009 interview in Business Insider, in which he also said, ""Unless you are breaking stuff, you are not moving fast enough.""[43]
 Lasso was a short-video sharing app from Facebook similar to TikTok that was launched on iOS and Android in 2018 and was aimed at teenagers. On July 2, 2020, Facebook announced that Lasso would be shutting down on July 10.[44][45][46]
 In 2018, the Oculus lead Jason Rubin sent his 50-page vision document titled ""The Metaverse"" to Facebook's leadership. In the document, Rubin acknowledged that Facebook's virtual reality business had not caught on as expected, despite the hundreds of millions of dollars spent on content for early adopters. He also urged the company to execute fast and invest heavily in the vision, to shut out HTC, Apple, Google and other competitors in the VR space. Regarding other players' participation in the metaverse vision, he called for the company to build the ""metaverse"" to prevent their competitors from ""being in the VR business in a meaningful way at all"".[47]
 In May 2019, Facebook founded Libra Networks, reportedly to develop their own stablecoin cryptocurrency.[48] Later, it was reported that Libra was being supported by financial companies such as Visa, Mastercard, PayPal and Uber. The consortium of companies was expected to pool in $10 million each to fund the launch of the cryptocurrency coin named Libra.[49] Depending on when it would receive approval from the Swiss Financial Market Supervisory authority to operate as a payments service, the Libra Association had planned to launch a limited format cryptocurrency in 2021.[50] Libra was renamed Diem, before being shut down and sold in January 2022 after backlash from Swiss government regulators and the public.[51][52]
 During the COVID-19 pandemic, the use of online services including Facebook grew globally.[53] Zuckerberg predicted this would be a ""permanent acceleration"" that would continue after the pandemic.[53] Facebook hired aggressively, growing from 48,268 employees in March 2020 to more than 87,000 by September 2022.[53]
 Following a period of intense scrutiny and damaging whistleblower leaks, news started to emerge on October 21, 2021, about Facebook's plan to rebrand the company and change its name.[54][55] In the Q3 2021 Earnings Call on October 25, Mark Zuckerberg discussed the ongoing criticism of the company's social services and the way it operates, and pointed to the pivoting efforts to building the metaverse – without mentioning the rebranding and the name change.[56] The metaverse vision and the name change from Facebook, Inc. to Meta Platforms was introduced at Facebook Connect on October 28, 2021.[57] Based on Facebook's PR campaign, the name change reflects the company's shifting long term focus of building the metaverse, a digital extension of the physical world by social media, virtual reality and augmented reality features.[57][58]
 ""Meta"" had been registered as a trademark in the United States in 2018 (after an initial filing in 2015) for marketing, advertising, and computer services, by a Canadian company that provided big data analysis of scientific literature.[59] This company was acquired in 2017 by the Chan Zuckerberg Initiative (CZI), a foundation established by Zuckerberg and his wife, Priscilla Chan, and became one of their projects.[60] Following the rebranding announcement, CZI announced that it had already decided to deprioritize the earlier Meta project, thus it would be transferring its rights to the name to Meta Platforms, and the previous project would end in 2022.[61]
 Soon after the rebranding, in early February 2022, Meta reported a greater-than-expected decline in profits in the fourth quarter of 2021.[62] It reported no growth in monthly users,[63] and indicated it expected revenue growth to stall.[62] It also expected measures taken by Apple Inc. to protect user privacy to cost it some $10 billion in advertisement revenue, an amount equal to roughly 8% of its revenue for 2021.[64] In meeting with Meta staff the day after earnings were reported, Zuckerberg blamed competition for user attention, particularly from video-based apps such as TikTok.[65]
 The 27% reduction in the company's share price which occurred in reaction to the news eliminated some $230 billion of value from Meta's market capitalization.[66] Bloomberg described the decline as ""an epic rout that, in its sheer scale, is unlike anything Wall Street or Silicon Valley has ever seen"".[66] Zuckerberg's net worth fell by as much as $31 billion.[67] Zuckerberg owns 13% of Meta, and the holding makes up the bulk of his wealth.[68][69]
 According to published reports by Bloomberg on March 30, 2022, Meta turned over data such as phone numbers, physical addresses, and IP addresses to hackers posing as law enforcement officials using forged documents. The law enforcement requests sometimes included forged signatures of real or fictional officials. When asked about the allegations, a Meta representative said, ""We review every data request for legal sufficiency and use advanced systems and processes to validate law enforcement requests and detect abuse.""[70] In June 2022, Sheryl Sandberg, the chief operating officer of 14 years, announced she would step down that year. Zuckerberg said that Javier Olivan would replace Sandberg, though in a ""more traditional"" role.[71]
 In March 2022, Meta (except Meta-owned WhatsApp) and Instagram were banned in Russia and added to Russian list of terrorist and extremist organizations for alleged Russophobia and hate speech(up to genocidal calls) amid ongoing Russian invasion of Ukraine.[72] Meta appealed against the ban but it was upheld by a Moscow court in June of the same year.[72]
 Also in March 2022, Meta and Italian eyewear giant Luxottica released Ray-Ban Stories, a series of smartglasses which could play music and take pictures. Meta and Luxottica parent company EssilorLuxottica declined to disclose sales on the line of products as of September 2022, though Meta has expressed satisfaction with its customer feedback.[17][73][74]
 In July 2022, Meta saw its first year-on-year revenue decline when its total revenue slipped by 1% to $28.8bn.[75] Analysts and journalists accredited the loss to its advertising business, which has been limited by Apple's app tracking transparency feature and the number of people who have opted not to be tracked by Meta apps. Zuckerberg also accredited the decline to increasing competition from TikTok.[76][77][78] On October 27, 2022, Meta's market value dropped to $268 billion, a loss of around $700 billion compared to 2021, and its shares fell by 24%. It lost its spot among the top 20 US companies by market cap, despite reaching the top 5 in the previous year.[79]
 In November 2022, Meta laid off 11,000 employees, 13% of its workforce. Zuckerberg said the decision to aggressively increase Meta's investments had been a mistake, as he had wrongly predicted that the surge in e-commerce would last beyond the COVID-19 pandemic. He also attributed the decline to increased competition, a global economic downturn and ""ads signal loss"".[80] Plans to lay off a further 10,000 employees began in April 2023.[81] The layoffs were part of a general downturn in the technology industry, alongside layoffs by companies including Google, Amazon, Tesla, Snap, Twitter and Lyft.[82][83]
 In March 2023, Meta announced a new round of layoffs that would cut 10,000 employees and close 5,000 open positions in order to make the company more efficient.[84] Meta revenue surpassed analyst expectations for the first quarter of 2023 after announcing that it was increasing its focus on AI.[85] On July 6, Meta launched a new app, Threads, a competitor to Twitter.[86]
 Meta announced its artificial intelligence model Llama 2 in July 2023, available for commercial use via partnerships with major cloud providers like Microsoft. It was the first project to be unveiled out of Meta's generative AI group after it was set up in February. It would not charge access or usage but instead operate with an open-source model to allow Meta to ascertain what improvements need to be made. Prior to this announcement, Meta said it had no plans to release Llama 2 for commercial use. An earlier version of Llama was released to academics.[87][88]
 In August 2023, Meta announced its permanent removal of news content from Facebook and Instagram in Canada due to the Online News Act, which requires Canadian news outlets to be compensated for content shared on its platform. The Online News Act was in effect by year-end, but Meta will not participate in the regulatory process.[89] In October 2023, Zuckerberg said that AI would be Meta's biggest investment area in 2024.[90] Meta finished 2023 as one of the best-performing technology stocks of the year, with its share price up 150 percent.[91] Its stock reached an all-time high in January 2024, bringing Meta within 2% of achieving $1 trillion market capitalization.[92]
 Meta Platforms launched an ad-free service in Europe in November 2023, allowing subscribers to opt-out of personal data being collected for targeted advertising. A group of 28 European organizations, including Max Schrems' advocacy group NOYB, the Irish Council for Civil Liberties, Wikimedia Europe, and the Electronic Privacy Information Center, signed a 2024 letter to the European Data Protection Board (EDPB) expressing concern that this subscriber model would undermine privacy protections, specifically GDPR data protection standards.[93]
 Meta removed the Facebook and Instagram accounts of Iran's Supreme Leader Ali Khamenei in February 2024, citing repeated violations of its Dangerous Organizations & Individuals policy.[94]
 Meta is currently under the investigation of the FDA for alleged use of their social media platforms to sell illegal drugs.[95]
 Meta has acquired multiple companies (often identified as talent acquisitions).[96] One of its first major acquisitions was in April 2012, when it acquired Instagram for approximately US$1 billion in cash and stock.[97] In October 2013, Facebook, Inc. acquired Onavo, an Israeli mobile web analytics company.[98][99] In February 2014, Facebook, Inc. announced it would buy mobile messaging company WhatsApp for US$19 billion in cash and stock.[100][101] Later that year, Facebook bought Oculus VR for $2.3 billion in cash and stock,[102] which released its first consumer virtual reality headset in 2016. In late November 2019, Facebook, Inc. announced the acquisition of the game developer Beat Games, responsible for developing one of that year's most popular VR games, Beat Saber.[103] In Late 2022 after Facebook Inc rebranded to Meta Platforms Inc, Oculus was rebranded to Meta Quest.
 In May 2020, Facebook, Inc. announced it had acquired Giphy for a reported cash price of $400 million. It will be integrated with the Instagram team.[104] However, in August 2021, UK's Competition and Markets Authority (CMA) stated that Facebook, Inc. might have to sell Giphy, after an investigation found that the deal between the two companies would harm competition in display advertising market.[105] Facebook, Inc. was fined $70 million by CMA for deliberately failing to report all information regarding the acquisition and the ongoing antitrust investigation.[106] In October 2022, the CMA ruled for a second time that Meta be required to divest Giphy, stating that Meta already controls half of the advertising in the UK. Meta agreed to the sale, though it stated that it disagrees with the decision itself.[107] In May 2023, Giphy was divested to Shutterstock for $53 million.[108]
 In November 2020, Facebook, Inc. announced that it planned to purchase the customer-service platform and chatbot specialist startup Kustomer to promote companies to use their platform for business. It has been reported that Kustomer valued at slightly over $1 billion.[109] The deal was closed in February 2022 after regulatory approval.[110]
 In September 2022, Meta acquired Lofelt, a Berlin-based haptic tech startup.[111]
 In 2020, Facebook, Inc. spent $19.7 million on lobbying, hiring 79 lobbyists. By 2019, it had spent $16.7 million on lobbying and had a team of 71 lobbyists, up from $12.6 million and 51 lobbyists in 2018.[112] Facebook was the largest spender of lobbying money among the Big Tech companies in 2020.[113] The lobbying team includes top congressional aide John Branscome, who was hired in September 2021, to help the company fend off threats from Democratic lawmakers and the Biden administration.[114]
 In 2024 Meta's decision to continue to disseminate a falsified video of President Biden, even after it had been proven to be fake, attracted criticism and concern.  AI-generated deepfakes spreading through Meta platforms was identified [by whom?] as a growing problem.[115][116]
 Numerous lawsuits have been filed against the company, both when it was known as Facebook, Inc., and as Meta Platforms.
 In March 2020, the Office of the Australian Information Commissioner (OAIC) sued Facebook, for significant and persistent infringements of the rule on privacy involving the Cambridge Analytica fiasco. Every violation of the Privacy Act is subject to a theoretical cumulative liability of $1.7 million. The OAIC estimated that a total of 311,127 Australians had been exposed.[117]
 On December 8, 2020, the U.S. Federal Trade Commission and 46 states (excluding Alabama, Georgia, South Carolina, and South Dakota), the District of Columbia and the territory of Guam, launched Federal Trade Commission v. Facebook as an antitrust lawsuit against Facebook. The lawsuit concerns Facebook's acquisition of two competitors—Instagram and WhatsApp—and the ensuing monopolistic situation. FTC alleges that Facebook holds monopolistic power in the US social networking market and seeks to force the company to divest from Instagram and WhatsApp to break up the conglomerate.[118] William Kovacic, a former chairman of the Federal Trade Commission, argued the case will be difficult to win as it would require the government to create a counterfactual argument of an internet where the Facebook-WhatsApp-Instagram entity did not exist, and prove that harmed competition or consumers.[119]
 On December 24, 2021, a court in Russia fined Meta for $27 million after the company declined to remove unspecified banned content. The fine was reportedly tied to the company's annual revenue in the country.[120]
 In May 2022, a lawsuit was filed in Kenya against Meta and its local outsourcing company Sama. Allegedly, Meta has poor working conditions in Kenya for workers moderating Facebook posts. According to the lawsuit, 260 screeners were declared redundant with confusing reasoning. The lawsuit seeks financial compensation and an order that outsourced moderators be given the same health benefits and pay scale as Meta employees.[121][122]
 In June 2022, 8 lawsuits were filed across the US over the allege that excessive exposure to platforms including Facebook and Instagram has led to attempted or actual suicides, eating disorders and sleeplessness, among other issues. The litigation follows a former Facebook employee's testimony in Congress that the company refused to take responsibility. The company noted that tools have been developed for parents to keep track of their children's activity on Instagram and set time limits in addition to Meta's ""Take a break"" reminders. In addition, the company is providing resources specific to eating disorders as well as developing AI to prevent children under the age of 13 signing up for Facebook or Instagram.[123]
 In June 2022, Meta settled a lawsuit with the US Department of Justice. The lawsuit, which was filed in 2019, alleged that the company enabled housing discrimination through targeted advertising, as it allowed home owners and landlords to run housing ads excluding people based on sex, race, religion, and other characteristics. The US Department of Justice stated that this was in violation of the Fair Housing Act. Meta was handed a penalty of $115,054 and given until December 31, 2022, to shadow the algorithm tool.[124][125][126]
 In January 2023, Meta was fined €390 million for violations of the European Union General Data Protection Regulation.[127]
 In May 2023, the European Data Protection Board fined Meta a record €1.2 billion for breaching European Union data privacy laws by transferring personal data of Facebook users to servers in the U.S.[128]
 Meta's key management consists of:[129][130]
 As of October 2022[update], Meta had 83,553 employees worldwide.
 As of February 2024, Meta's board consisted of the following directors;[131]
 Roger McNamee, an early Facebook investor and Zuckerberg's former mentor, said Facebook had ""the most centralized decision-making structure I have ever encountered in a large company"".[133]
 Facebook co-founder Chris Hughes has stated that chief executive officer Mark Zuckerberg has too much power, that the company is now a monopoly, and that, as a result, it should be split into multiple smaller companies. In an op-ed in The New York Times, Hughes said he was concerned that Zuckerberg had surrounded himself with a team that did not challenge him, and that it is the U.S. government's job to hold him accountable and curb his ""unchecked power"".[134] He also said that ""Mark's power is unprecedented and un-American.""[135] Several U.S. politicians agreed with Hughes.[136] European Union Commissioner for Competition Margrethe Vestager stated that splitting Facebook should be done only as ""a remedy of the very last resort"", and that it would not solve Facebook's underlying problems.[137]
 Facebook ranked No. 34 in the 2020 Fortune 500 list of the largest United States corporations by revenue, with almost $86 billion in revenue[152] most of it coming from advertising.[153][154] One analysis of 2017 data determined that the company earned US$20.21 per user from advertising.[155]
 According to New York, since its rebranding, Meta has reportedly lost $500 billion as a result of new privacy measures put in place by companies such as Apple and Google which prevents Meta from gathering users' data.[156][157]
 In February 2015, Facebook announced it had reached two million active advertisers, with most of the gain coming from small businesses. An active advertiser was defined as an entity that had advertised on the Facebook platform in the last 28 days.[158] In March 2016, Facebook announced it had reached three million active advertisers with more than 70% from outside the United States.[159] Prices for advertising follow a variable pricing model based on auctioning ad placements, and potential engagement levels of the advertisement itself. Similar to other online advertising platforms like Google and Twitter, targeting of advertisements is one of the chief merits of digital advertising compared to traditional media. Marketing on Meta is employed through two methods based on the viewing habits, likes and shares, and purchasing data of the audience, namely targeted audiences and ""look alike"" audiences.[160]
 The US IRS challenged the valuation Facebook used when it transferred IP from the US to Facebook Ireland (now Meta Platforms Ireland) in 2010 (which Facebook Ireland then revalued higher before charging out), as it was building its double Irish tax structure.[161][162] The case is ongoing and Meta faces a potential fine of $3–5bn.[163]
 The US Tax Cuts and Jobs Act of 2017 changed Facebook's global tax calculations. Meta Platforms Ireland is subject to the US GILTI tax of 10.5% on global intangible profits (i.e. Irish profits). On the basis that Meta Platforms Ireland Limited is paying some tax, the effective minimum US tax for Facebook Ireland will be circa 11%. In contrast, Meta Platforms Inc. would incur a special IP tax rate of 13.125% (the FDII rate) if its Irish business relocated to the US. Tax relief in the US (21% vs. Irish at the GILTI rate) and accelerated capital expensing, would make this effective US rate around 12%.[164][165][166]
 The insignificance of the US/Irish tax difference was demonstrated when Facebook moved 1.5bn non-EU accounts to the US to limit exposure to GDPR.[167][168]
 Users outside of the US and Canada contract with Meta's Irish subsidiary, Meta Platforms Ireland Limited (formerly Facebook Ireland Limited), allowing Meta to avoid US taxes for all users in Europe, Asia, Australia, Africa and South America. Meta is making use of the Double Irish arrangement which allows it to pay 2–3% corporation tax on all international revenue.[169] In 2010, Facebook opened its fourth office, in Hyderabad, India,[170] which houses online advertising and developer support teams and provides support to users and advertisers.[171] In India, Meta is registered as Facebook India Online Services Pvt Ltd.[172] It also has support centers in Chittagong; Dublin;[clarification needed] California; Ireland; and Austin, Texas.[173][not specific enough to verify]
 Facebook opened its London headquarters in 2017 in Fitzrovia in central London. Facebook opened an office in Cambridge, Massachusetts in 2018. The offices were initially home to the ""Connectivity Lab"", a group focused on bringing Internet access to those who do not have access to the Internet.[174] In April 2019, Facebook opened its Taiwan headquarters in Taipei.[175]
 In March 2022, Meta opened new regional headquarters in Dubai.[176]
 In September 2023, it was reported that Meta had paid £149m to British Land in order to break the lease on Triton Square London office. Meta reportedly had another 18 years left on its lease on the site.[177]
 As of 2023, Facebook operated 21 data centers.[178] It committed to purchase 100% renewable energy and to reduce its greenhouse gas emissions 75% by 2020.[179] Its data center technologies include Fabric Aggregator, a distributed network system that accommodates larger regions and varied traffic patterns.[180]
 US Representative Alexandria Ocasio-Cortez responded in a tweet to Zuckerberg's announcement about Meta, saying: ""Meta as in 'we are a cancer to democracy metastasizing into a global surveillance and propaganda machine for boosting authoritarian regimes and destroying civil society ... for profit!'""[181]
 Ex-Facebook employee Frances Haugen and whistleblower behind the Facebook Papers responded to the rebranding efforts by expressing doubts about the company's ability to improve while led by Mark Zuckerberg, and urged the chief executive officer to resign.[182]
 In November 2021, a video published by Inspired by Iceland went viral, in which a Zuckerberg look-alike promoted the Icelandverse, a place of ""enhanced actual reality without silly looking headsets"".[183]
 In a December 2021 interview, SpaceX and Tesla chief executive officer Elon Musk said he could not see a compelling use-case for the VR-driven metaverse, adding: ""I don't see someone strapping a frigging screen to their face all day.""[184]
 In January 2022, Louise Eccles of The Sunday Times logged into the metaverse with the intention of making a video guide. She wrote:
 Initially, my experience with the Oculus went well. I attended work meetings as an avatar and tried an exercise class set in the streets of Paris. The headset enabled me to feel the thrill of carving down mountains on a snowboard and the adrenaline rush of climbing a mountain without ropes. Yet switching to the social apps, where you mingle with strangers also using VR headsets, it was at times predatory and vile. Eccles described being sexually harassed by another user, as well as ""accents from all over the world, American, Indian, English, Australian, using racist, sexist, homophobic and transphobic language"". She also encountered users as young as 7 years old on the platform, despite Oculus headsets being intended for users over 13.[185]
 37°29′06″N 122°08′54″W﻿ / ﻿37.48500°N 122.14833°W﻿ / 37.48500; -122.14833
"
"
 EleutherAI (/əˈluːθər/[2]) is a grass-roots non-profit artificial intelligence (AI) research group. The group, considered an open-source version of OpenAI,[3] was formed in a Discord server in July 2020 to organize a replication of GPT-3. In early 2023, it formally incorporated as the EleutherAI Foundation, a non-profit research institute.[4]
 EleutherAI began as a Discord server on July 7, 2020 under the tentative name ""LibreAI"" before rebranding to ""EleutherAI"" later that month,[5] in reference to eleutheria, the Greek word for liberty.[3]
 On December 30, 2020, EleutherAI released The Pile, a curated dataset of diverse text for training large language models.[6] While the paper referenced the existence of the GPT-Neo models, the models themselves were not released until March 21, 2021.[7] According to a retrospective written several months later, the authors did not anticipate that ""people would care so much about our 'small models.'""[1] On June 9, 2021, EleutherAI followed this up with GPT-J-6B, a six billion parameter language model that was again the largest open-source GPT-3-like model in the world.[8] These language models were released under the Apache 2.0 free software license and are considered to have ""fueled an entirely new wave of startups"".[4]
 While EleutherAI initially turned down funding offers, preferring to use Google's TPU Research Cloud Program to source their compute,[9] by early 2021 they had accepted funding from CoreWeave (a small cloud computing company) and SpellML (a cloud infrastructure company) in the form of access to powerful GPU clusters that are necessary for large scale machine learning research. On Feb 10, 2022 they released GPT-NeoX-20B, a model similar to their prior work but scaled up thanks to the resources CoreWeave provided.[10]
 In 2022, many EleutherAI members participated in the BigScience Research Workshop, working on projects including multitask finetuning,[11][12] training BLOOM,[13] and designing evaluation libraries.[14] Engineers at EleutherAI, Stability AI, and NVIDIA joined forces with biologists led by Columbia University and Harvard University[15]
to train OpenFold, an open-source replication of DeepMind's AlphaFold2.[16]
 In early 2023, EleutherAI incorporated as a non-profit research institute run by Stella Biderman, Curtis Huebner, and Shivanshu Purohit.[4][17] This announcement came with the statement that EleutherAI's shift of focus away from training larger language models was part of a deliberate push towards doing work in interpretability, alignment, and scientific research.[17] While EleutherAI is still committed to promoting access to AI technologies, they feel that ""there is substantially more interest in training and releasing LLMs than there once was,"" enabling them to focus on other projects.[18]
 According to their website, EleutherAI is a ""decentralized grassroots collective of volunteer researchers, engineers, and developers focused on AI alignment, scaling, and open-source AI research"".[19] While they do not sell any of their technologies as products, they publish the results of their research in academic venues, write blog posts detailing their ideas and methodologies, and provide trained models for anyone to use for free.[citation needed]
 The Pile is an 886 GB dataset designed for training large language models. It was originally developed to train EleutherAI's GPT-Neo models but has become widely used to train other models, including Microsoft's Megatron-Turing Natural Language Generation,[20][21] Meta AI's Open
Pre-trained Transformers,[22] LLaMA,[23] and Galactica,[24] Stanford University's BioMedLM 2.7B,[25] the Beijing Academy of Artificial Intelligence's 
Chinese-Transformer-XL,[26] and Yandex's YaLM 100B.[27] Compared to other datasets, the Pile's main distinguishing features are that it is a curated selection of data chosen by researchers at EleutherAI to contain information they thought language models should learn and that it is the only such dataset that is thoroughly documented by the researchers who developed it.[28]
 EleutherAI's most prominent research relates to its work to train open-source large language models inspired by OpenAI's GPT-3.[29] EleutherAI's ""GPT-Neo"" model series has released 125 million, 1.3 billion, 2.7 billion, 6 billion, and 20 billion parameter models. 
 While the overwhelming majority of large language models are[when?] trained in either English or Chinese,[citation needed] EleutherAI also trains language models in other languages, such as the Korean-language Polyglot-Ko.[40]
 Following the release of DALL-E by OpenAI in January 2021, EleutherAI started working on text-to-image synthesis models. When OpenAI didn't release DALL-E publicly, EleutherAI's Katherine Crowson and digital artist Ryan Murdock developed a technique for using CLIP (another model developed by OpenAI) to convert regular image generation models into text-to-image synthesis ones.[43][44][45][46] Building on ideas dating back to Google's DeepDream,[47] they found their first major success combining CLIP with another publicly available model called VQGAN and the resulting model is called VQGAN-CLIP.[48] Crowson released the technology by tweeting notebooks demonstrating the technique that people could run for free without any special equipment.[49][50][51] This work was credited by Stability AI CEO Emad Mostaque as motivating the founding of Stability AI.[52]
 EleutherAI's work to democratize GPT-3 won the UNESCO Netexplo Global Innovation Award in 2021,[53] InfoWorld's Best of Open Source Software Award in 2021[54] and 2022,[55] was nominated for VentureBeat's AI Innovation Award in 2021.[56]
 Gary Marcus, a cognitive scientist and noted critic of deep learning companies such as OpenAI and DeepMind,[57] has repeatedly[58][59] praised EleutherAI's dedication to open-source and transparent research.
 Maximilian Gahntz, a senior policy researcher at the Mozilla Foundation, applauded EleutherAI's efforts to give more researchers the ability to audit and assess AI technology. ""If models are open and if data sets are open, that'll enable much more of the critical research that's pointed out many of the flaws and harms associated with generative AI and that's often far too difficult to conduct.""[60]
 Technology journalist Kyle Wiggers has raised concerns about whether EleutherAI is as independent as it claims, or ""whether the involvement of commercially motivated ventures like Stability AI and Hugging Face — both of which are backed by substantial venture capital — might influence EleutherAI's research.""[61]
"
"Hugging Face, Inc. is a French-American company based in New York City that develops computation tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.
 The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.[1] The company was named after the ""hugging face"" emoji.[1] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.
 In March 2021, Hugging Face raised US$40 million in a Series B funding round.[2]
 On April 28, 2021, the company launched the BigScience Research Workshop in collaboration with several other research groups to release an open large language model.[3] In 2022, the workshop concluded with the announcement of BLOOM, a multilingual large language model with 176 billion parameters.[4][5]
 In December 2022, the company acquired Gradio, an open source library built for developing machine learning applications in Python.[6]
 On May 5, 2022, the company announced its Series C funding round led by Coatue and Sequoia.[7] The company received a $2 billion valuation.
 On August 3, 2022, the company announced the Private Hub, an enterprise version of its public Hugging Face Hub that supports SaaS or on-premises deployment.[8]
 In February 2023, the company announced partnership with Amazon Web Services (AWS) which would allow Hugging Face's products available to AWS customers to use them as the building blocks for their custom applications. The company also said the next generation of BLOOM will be run on Trainium, a proprietary machine learning chip created by AWS.[9][10][11]
 In August 2023, the company announced that it raised $235 million in a Series D funding, at a $4.5 billion valuation. The funding was led by Salesforce,  and notable participation came from Google, Amazon, Nvidia, AMD, Intel, IBM, and Qualcomm.[12]
 The Transformers library is a Python package that contains open-source implementations of transformer models for text, image, and audio tasks. It is compatible with the PyTorch, TensorFlow and JAX deep learning libraries and includes implementations of notable models like BERT and GPT-2.[13] The library was originally called ""pytorch-pretrained-bert""[14] which was then renamed to ""pytorch-transformers"" and finally ""transformers.""
 The Hugging Face Hub is a platform (centralized web service) for hosting:[15]
 In addition to Transformers and the Hugging Face Hub, the Hugging Face ecosystem contains libraries for other tasks, such as dataset processing (""Datasets""), model evaluation (""Evaluate""), and machine learning demos (""Gradio"").[16]
"
"
 Friendly artificial intelligence (also friendly AI or FAI) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained.
 The term was coined by Eliezer Yudkowsky,[1] who is best known for popularizing the idea,[2][3] to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach, describes the idea:[2]
 Yudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes. 'Friendly' is used in this context as technical terminology, and picks out agents that are safe and useful, not necessarily ones that are ""friendly"" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly explode in intelligence, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.[4]
 The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem, or the proto-robots of Gerbert of Aurillac and Roger Bacon.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.[5] By 1942 these themes prompted Isaac Asimov to create the ""Three Laws of Robotics""—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.[6]
 In modern times as the prospect of superintelligent AI looms nearer, philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity.  He put it this way:
 Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.' In 2008 Eliezer Yudkowsky called for the creation of ""friendly AI"" to mitigate existential risk from advanced artificial intelligence. He explains: ""The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.""[7]
 Steve Omohundro says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of basic ""drives"", such as resource acquisition, self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, ""without special precautions"", cause the AI to exhibit undesired behavior.[8][9]
 Alexander Wissner-Gross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.[10][11]
 Luke Muehlhauser, writing for the Machine Intelligence Research Institute, recommends that machine ethics researchers adopt what Bruce Schneier has called the ""security mindset"": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.[12]
 In 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI';[13] nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.[14][15]
 Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, our coherent extrapolated volition is ""our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted"".[16]
 Rather than a Friendly AI being designed directly by human programmers, it is to be designed by a ""seed AI"" programmed to first study human nature and then produce the AI which humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.[16] The appeal to an objective through contingent human nature (perhaps expressed, for mathematical purposes, in the form of a utility function or other decision-theoretic formalism), as providing the ultimate criterion of ""Friendliness"", is an answer to the meta-ethical problem of defining an objective morality; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.
 Steve Omohundro has proposed a ""scaffolding"" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.[17]
 Seth Baum argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities, and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, ""existing messages about beneficial AI are not always framed well"". Baum advocates for ""cooperative relationships, and positive framing of AI researchers"" and cautions against characterizing AI researchers as ""not want(ing) to pursue beneficial designs"".[18]
 In his book Human Compatible, AI researcher Stuart J. Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:[19]: 173 
 The ""preferences"" Russell refers to ""are all-encompassing; they cover everything you might care about, arbitrarily far into the future.""[19]: 173   Similarly, ""behavior"" includes any choice between options,[19]: 177  and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.[19]: 201 
 James Barrat, author of Our Final Invention, suggested that ""a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations."" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA, which discussed risks of biotechnology.[17]
 John McGinnis encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the National Institutes of Health, where ""Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards."" McGinnis feels that peer review is better ""than regulation to address technical issues that are not possible to capture through bureaucratic mandates"". McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute, which generally aims to avoid government involvement in friendly AI.[20]
 Some critics believe that both human-level AI and superintelligence are unlikely, and that therefore friendly AI is unlikely. Writing in The Guardian, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty, and states that while we need to be ""cautious and prepared"" given the stakes involved, we ""don't need to be obsessing"" about the risks of superintelligence.[21] Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and Nick Bostrom’s proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had.[13] In an article in AI & Society, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.[14]
 Some philosophers claim that any truly ""rational"" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.[22] Other critics question whether it is possible for an artificial intelligence to be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal The New Atlantis, say that it will be impossible to ever guarantee ""friendly"" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work ""only when one has not only great powers of prediction about the likelihood of myriad possible outcomes, but certainty and consensus on how one values the different outcomes.[23]
 The inner workings of advanced AI systems may be complex and difficult to interpret, leading to concerns about transparency and accountability.[24]
"
"Moral agency is an individual's ability to make moral choices based on some notion of right and wrong and to be held accountable for these actions.[1] A moral agent is ""a being who is capable of acting with reference to right and wrong.""[2]
 Most philosophers suggest only rational beings, who can reason and form self-interested judgments, are capable of being moral agents. Some suggest those with limited rationality (for example, people who are mildly mentally disabled or infants[1]) also have some basic moral capabilities.[3]
 Determinists argue all of our actions are the product of antecedent causes, and some believe this is incompatible with free will and thus claim that we have no real control over our actions. Immanuel Kant argued that whether or not our real self, the noumenal self, can choose, we have no choice but to believe that we choose freely when we make a choice. This does not mean that we can control the effects of our actions.
Some Indeterminists would argue we have no free will either. If, with respect to human behavior, a so-called 'cause' results in an indeterminate number of possible, so-called 'effects', that does not mean the person had the free-thinking independent will to choose that 'effect'. More likely, it was the indeterminate consequence of his chance genetics, chance experiences and chance circumstances relevant at the time of the 'cause'.
 In Kant's philosophy, this calls for an act of faith, the faith free agent is based on something a priori, yet to be known, or immaterial. Otherwise, without free agent's a priori fundamental source, socially essential concepts created from human mind, such as justice, would be undermined (responsibility implies freedom of choice) and, in short, civilization and human values would crumble.
 It is useful to compare the idea of moral agency with the legal doctrine of mens rea, which means guilty mind, and states that a person is legally responsible for what he does as long as he should know what he is doing, and his choices are deliberate. Some theorists discard any attempts to evaluate mental states and, instead, adopt the doctrine of strict liability, whereby one is liable under the law without regard to capacity, and that the only thing is to determine the degree of punishment, if any. Moral determinists would most likely adopt a similar point of view.
 Psychologist Albert Bandura has observed that moral agents engage in selective moral disengagement in regards to their own inhumane conduct.[4]
 Moral Agents are entities whose actions are eligible for moral consideration. An example of this would be a young child old enough to understand right from wrong, yet they hit their siblings on an occasion when they get angry. The action of hitting is up for moral consideration because the child is old enough to consider whether or not it is the correct action to take and the morality of their behavior.[5]
 Moral Patients are entities that themselves are eligible for moral consideration. An example of this would be a child who does not know how to determine right from wrong. A child in this situation is up for moral consideration by others because those around them understand they are incapable of understanding the consequences of their actions and are therefore unable to understand the morality of a situation due to developmental barriers.[5]
 Many philosophers, such as Immanuel Kant, view morality as a transaction among rational parties, i.e., among moral agents. In Richard Dean’s article on Kant’s moral theory he discusses how agents who are able to control their tendencies or drives, are able to remain unbiased as they determine the path of moral action. The ability to be able to control this is called moral commitment. Agents need to become experts in this control in order to be able to declare something as moral or immoral and retain reputability.[6] For this reason, they would exclude other animals from moral consideration.
 Utilitarian philosophers Jeremy Bentham and Peter Singer have argued, the key to inclusion in the moral community is not rationality — for if it were, we might have to exclude some disabled people and infants, and might also have to distinguish between the degrees of rationality of healthy adults — but the real object of moral action is the avoidance of suffering.[7][8] An example of this is the Abortion debate. Further examples can be taken from the argument from marginal cases. (See section- Non-Human Animals).
 Discussions of artificial moral agency center around a few main ideas. The first discussion is on whether it is possible for an artificial system to be a moral agent - see artificial systems and moral responsibility.
The second discussion concerns efforts to construct machines with ethically-significant behaviors - see machine ethics. 
Finally, there is debate about whether robots should be constructed as moral agents.
 Research has shown that humans do perceive robots as having varying degrees of moral agency. These perceptions can manifest in two distinct ways: (1) ideas about a robot’s moral capacity (the ability to be/do good or bad) and (2) ideas about its (in)dependence on programming (where high dependency equates to low agency).[9]
 Research suggests that the moral judgment of an action may not depend on whether the agent is a human or a robot. However, robots are rarely given credit for acting well, and must behave more consistently well to be trusted.[10]
 The creation of a robot or “social machine” with the capability of understanding morality and agency has not been accomplished yet. However, a machine with those capabilities could be potentially created in the future.[11]
 Discussion of moral agency in non-human animals involves both debate about the nature of morality and about the capacities and behavior of human and non-human animals.[12][13] Thinkers who agree about the nature, behavior and abilities of different species may still disagree about which capacities are important for moral agency or about the significance of particular behaviors in determining moral agency.[14] Since moral agents are often thought to warrant particular moral consideration, this discussion is sometimes linked to debates in animal rights about practices involving non-human animals.[15]
 Studies of animal biology and behavior have provided strong evidence of complex social structures and behavioral norms in non-human species. There is also evidence that some non-human species, especially other primates, can demonstrate empathy and emotions such as guilt or grief, though some thinkers dispute this.[16][17] However, humans display distinctive capacities related to intelligence and rationality such as the ability to engage in abstract and symbolic thought and to employ complex language.[18]
 Philosophers and biologists who claim that non-human animals are moral agents typically argue that moral agency is dependent on empathy or social relations, and stress the evidence for these in non-human animals.[19] They may also point out behaviors which in humans are described as moral activities, such as the punishment of individuals who break social norms. Some thinkers suggest that there are a variety of types or levels of moral agency which vary by species, or that animals may act morally without being full moral agents.[20][21]
 Thinkers who hold that only humans can be moral agents typically argue that moral agency depends on rationality. They highlight distinctive human abilities and the unique complexity of human behavior. They argue that shared behaviors such as the punishment of wrongdoers are nevertheless underpinned by very different internal processes, meaning that these behaviors qualify as moral activity for humans but not for non-humans.[22]
"
"AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment (which aim to make AI systems moral and beneficial), monitoring AI systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.
 Scholars discuss current risks from critical systems failures,[1] bias,[2] and AI-enabled surveillance,[3] as well as emerging risks like technological unemployment, digital manipulation,[4] weaponization,[5] AI-enabled cyberattacks[6] and bioterrorism.[7] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents,[8] or from AI enabling perpetually stable dictatorships.[9]
 AI safety has a significant focus on existential risks and other large-scale risks. The part of AI safety aiming to address concerns of existential risk is sometimes called AI existential safety.[12]
 Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to ""worrying about overpopulation on Mars when we have not even set foot on the planet yet.""[13] Stuart J. Russell on the other side urges caution, arguing that ""it is better to anticipate human ingenuity than to underestimate it.""[14]
 AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[15][16][17] – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an ""extremely bad (e.g. human extinction)"" outcome of advanced AI.[15] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is ""at least as bad as an all-out nuclear war.""[18]
 Risks from AI began to be seriously discussed at the start of the computer age:
 Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes. From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that ""additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes.""[20]
 In 2011, Roman Yampolskiy introduced the term ""AI safety engineering""[21] at the Philosophy and Theory of Artificial Intelligence conference,[22] listing prior failures of AI systems and arguing that ""the frequency and seriousness of such events will steadily increase as AIs become more capable.""[23]
 In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.[24] His argument that future advanced systems may pose a threat to human existence prompted Elon Musk,[25] Bill Gates,[26] and Stephen Hawking[27] to voice similar concerns.
 In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions.[28] To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.
 In the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at ""ensuring artificial intelligence (AI) remains safe, ethical and beneficial.""[29]
 In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,[30] which was one of a sequence of four White House workshops aimed at investigating ""the advantages and drawbacks"" of AI.[31] In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.[32]
 In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including ""Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards.""[33]
 In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance.[34] The following year, researchers organized a workshop at ICLR that focused on these problem areas.[35]
 In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.[36]
 In 2023, Rishi Sunak said he wants the United Kingdom to be the ""geographical home of global AI safety regulation"" and to host the first global summit on AI safety.[37] The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.[38]
 In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.[39]
 AI safety research areas include robustness, monitoring, and alignment.[36][34]
 AI systems are often vulnerable to adversarial examples or ""inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake"".[40] For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.[41] This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.[42][43][44]
 All of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.[41]
 Adversarial robustness is often associated with security.[45] Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.[46] Network intrusion[47] and malware[48] detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.
 Models that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.[49] Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.[50] This issue can be addressed by improving the adversarial robustness of the reward model.[51] More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.[52]
 It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.[53] ML models generally express confidence by outputting probabilities; however, they are often overconfident,[54] especially in situations that differ from those that they were trained to handle.[55] Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.
 Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.[56] Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,[57] though a range of additional techniques are in use.[58][59]
 Scholars[5] and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,[60] manipulate public opinion,[61][62] or automate cyber attacks.[63] These worries are a practical concern for companies like OpenAI which host powerful AI tools online.[64] In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.[65]
 Neural networks have often been described as black boxes,[66] meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.[67] This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.[68] It also raises debates in healthcare over whether statistically efficient but opaque models should be used.[69]
 One critical benefit of transparency is explainability.[70] It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.[70]
 Another benefit is to reveal the cause of failures.[66] At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.[71]
 Transparency techniques can also be used to correct errors. For example, in the paper ""Locating and Editing Factual Associations in GPT,"" the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.[72] Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.[73]
 Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.[74] ""Inner"" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.[75][76] For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in spider man costumes, sketches of spiderman, and the word 'spider.'[77] It also involves explaining connections between these neurons or 'circuits'.[78][79] For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.[80] ""Inner interpretability"" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.[81]
 ML models can potentially contain 'trojans' or 'backdoors': vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;[36] or a trojaned autonomous vehicle may function normally until a specific trigger is visible.[82] Note that an adversary must have access to the system's training data in order to plant a trojan. [citation needed] This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.[83] Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.[84] In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.[52]
 In the field of artificial intelligence (AI), AI alignment research aims to steer AI systems toward a person's or group's intended goals, preferences, and ethical principles. An AI system is considered aligned if it advances its intended objectives. A misaligned AI system may pursue some objectives, but not the intended ones.[85]
 It is often challenging for AI designers to align an AI system due to the difficulty of specifying the full range of desired and undesired behaviors. To aid them, they often use simpler proxy goals, such as gaining human approval. But that approach can create loopholes, overlook necessary constraints, or reward the AI system for merely appearing aligned.[85][86]
 Misaligned AI systems can malfunction and cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[85][87][88] They may also develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their final given goals.[85][89][90] Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is deployed and encounters new situations and data distributions.[91][92]
 Today, these problems affect existing commercial systems such as language models,[93][94][95] robots,[96] autonomous vehicles,[97] and social media recommendation engines.[93][90][98] Some AI researchers argue that more capable future systems will be more severely affected, since these problems partially result from the systems being highly capable.[99][87][86]
 Many of the most-cited AI scientists,[100][101][102] including Geoffrey Hinton, Yoshua Bengio, and Stuart Russell, argue that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI) and could endanger human civilization if misaligned.[103][90]
 It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents.[116] Some scholars have suggested that this framework falls short.[116] For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology.[116] Policy analysts Zwetsloot and Dafoe wrote, ""The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways… Often, though, the relevant causal chain is much longer."" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.[116] In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.[117]
 Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.[36]
 Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.[118] This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.[5]
 The advancement of AI in economic and military domains could precipitate unprecedented political challenges.[119] Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.[120] AI researchers have argued that AI technologies could also be used to assist decision-making.[36] For example, researchers are beginning to develop AI forecasting[121] and advisory systems.[122]
 Many of the largest global threats (nuclear war,[123] climate change,[124] etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.[124]
 A salient AI cooperation challenge is avoiding a 'race to the bottom'.[125] In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political[126] and technical[127] efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).[128] Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.[128]
 In recent years, the development of large language models (LMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al.[129] have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots.' These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.[130][131]
 AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.[120]
 AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.[133] Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment,[134] weaponization,[135] disinformation,[136] surveillance,[137] and the concentration of power.[138] Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,[139] the availability of AI models,[140] and 'race to the bottom' dynamics.[125][141] Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: ""it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution."".[126] A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.[142][143][144]
 In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers[145] argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.[146][147]
 Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to ""rush to regulate in ignorance.""[148][149] Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.[150]
 Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to ""assure that systems are aligned with goals and values, including safety, robustness and trustworthiness.""[151] Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when ""catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed.""[152]
 In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy,[153] which states the British government ""takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously.""[154] The strategy describes actions to assess long-term AI risks, including catastrophic risks.[154] The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as ""an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach.""[155][156]
 Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems.[157] The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks.[158][159] And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.[160]
 In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of “safe, secure and trustworthy” AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.[161]
 AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.[162] One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,[163] offering bounties for finding failures,[163] sharing AI incidents[163] (an AI incident database was created for this purpose),[164] following guidelines to determine whether to publish research or models,[140] and improving information and cyber security in AI labs.[165]
 Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on ""best practices for deploying language models,"" focusing on mitigating misuse.[166] To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that ""if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project""[167] Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles[33] and the Autonomous Weapons Open Letter.[168]
"
"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents.[1] Machine ethics differs from other ethical fields related to engineering and technology. It should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with technology's grander social effects.[2]
 James H. Moor, one of the pioneering theoreticians in the field of computer ethics, defines four kinds of ethical robots. As an extensive researcher on the studies of philosophy of artificial intelligence, philosophy of mind, philosophy of science, and logic, Moor defines machines as ethical impact agents, implicit ethical agents, explicit ethical agents, or full ethical agents. A machine can be more than one type of agent.[3]
 (See artificial systems and moral responsibility.)
 
Before the 21st century the ethics of machines had largely been the subject of science fiction, mainly due to computing and artificial intelligence (AI) limitations. Although the definition of ""machine ethics"" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI magazine article ""A Question of Responsibility"": One thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. Thus, as computers and robots become more and more intelligent, it becomes imperative that we think carefully and explicitly about what those built-in values are. Perhaps what we need is, in fact, a theory and practice of machine ethics, in the spirit of Asimov's three laws of robotics.[4] In 2004, Towards Machine Ethics[5] was presented at the AAAI Workshop on Agent Organizations: Theory and Practice.[6] Theoretical foundations for machine ethics were laid out.
 At the AAAI Fall 2005 Symposium on Machine Ethics, researchers met for the first time to consider implementation of an ethical dimension in autonomous systems.[7] A variety of perspectives of this nascent field can be found in the collected edition Machine Ethics[8] that stems from that symposium.
 In 2007, AI magazine published ""Machine Ethics: Creating an Ethical Intelligent Agent"",[9] an article that discussed the importance of machine ethics, the need for machines that represent ethical principles explicitly, and challenges facing those working on machine ethics. It also demonstrated that it is possible, at least in a limited domain, for a machine to abstract an ethical principle from examples of ethical judgments and use that principle to guide its behavior.
 In 2009, Oxford University Press published Moral Machines, Teaching Robots Right from Wrong,[10] which it advertised as ""the first book to examine the challenge of building artificial moral agents, probing deeply into the nature of human decision making and ethics."" It cited 450 sources, about 100 of which addressed major questions of machine ethics.
 In 2011, Cambridge University Press published a collection of essays about machine ethics edited by Michael and Susan Leigh Anderson,[8] who also edited a special issue of IEEE Intelligent Systems on the topic in 2006.[11] The collection focuses on the challenges of adding ethical principles to machines.[12]
 In 2014, the US Office of Naval Research announced that it would distribute $7.5 million in grants over five years to university researchers to study questions of machine ethics as applied to autonomous robots,[13] and Nick Bostrom's Superintelligence: Paths, Dangers, Strategies, which raised machine ethics as the ""most important...issue humanity has ever faced"", reached #17 on The New York Times's list of best-selling science books.[14]
 In 2016 the European Parliament published a paper[15] to encourage the Commission to address robots' legal status.[16] The paper includes sections about robots' legal liability, in which it is argued that their liability should be proportional to their level of autonomy. The paper also discusses how many jobs could be taken by AI robots.[17]
 In 2019 the Proceedings of the IEEE published a special issue on Machine Ethics: The Design and Governance of Ethical AI and Autonomous Systems, edited by Alan Winfield, Katina Michael, Jeremy Pitt and Vanessa Evers.[18] ""The issue includes papers describing implicit ethical agents, where machines are designed to avoid unethical outcomes, as well as explicit ethical agents, or machines that either encode or learn ethics and determine actions based on those ethics"".[19]
 Some scholars, such as Bostrom and AI researcher Stuart Russell, argue that, if AI surpasses humanity in general intelligence and becomes ""superintelligent"", this new superintelligence could become powerful and difficult to control: just as the mountain gorilla's fate depends on human goodwill, so might humanity's fate depend on a future superintelligence's actions.[20] In their respective books Superintelligence and Human Compatible, Bostrom and Russell assert that while the future of AI is very uncertain, the risk to humanity is great enough to merit significant action in the present.
 This presents the AI control problem: how to build an intelligent agent that will aid its creators without inadvertently building a superintelligence that will harm them. The danger of not designing control right ""the first time"" is that a superintelligence may be able to seize power over its environment and prevent us from shutting it down. Potential AI control strategies include ""capability control"" (limiting an AI's ability to influence the world) and ""motivational control"" (one way of building an AI whose goals are aligned with human or optimal values). A number of organizations are researching the AI control problem, including the Future of Humanity Institute, the Machine Intelligence Research Institute, the Center for Human-Compatible Artificial Intelligence, and the Future of Life Institute.
 AI paradigms have been debated, especially their efficacy and bias. Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3) over neural networks and genetic algorithms on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. stare decisis).[21] In contrast, Chris Santos-Lang has argued in favor of neural networks and genetic algorithms on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable than machines to criminal hackers.[22][23]
 In 2009, in an experiment at the Ecole Polytechnique Fédérale of Lausanne's Laboratory of Intelligent Systems, AI robots were programmed to cooperate with each other and tasked with searching for a beneficial resource while avoiding a poisonous one.[24] During the experiment, the robots were grouped into clans, and the successful members' digital genetic code was used for the next generation, a type of algorithm known as a genetic algorithm. After 50 successive generations in the AI, one clan's members discovered how to distinguish the beneficial resource from the poisonous one. The robots then learned to lie to each other in an attempt to hoard the beneficial resource from other robots.[24] In the same experiment, the same robots also learned to behave selflessly and signaled danger to other robots, and died to save other robots.[22] Machine ethicists have questioned the experiment's implications. In the experiment, the robots' goals were programmed to be ""terminal"", but human motives typically require never-ending learning.
 In 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the possibility that they could become self-sufficient and able to make their own decisions. They discussed the extent to which computers and robots might acquire autonomy, and to what degree they could use it to pose a threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including the ability to find power sources on their own and to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved ""cockroach intelligence"". They noted that self-awareness as depicted in science fiction is probably unlikely, but that there are other potential hazards and pitfalls.[25]
 Some experts and academics have questioned the use of robots in military combat, especially robots with a degree of autonomy.[26] The U.S. Navy funded a report that indicates that as military robots become more complex, we should pay greater attention to the implications of their ability to make autonomous decisions.[27][28] The president of the Association for the Advancement of Artificial Intelligence has commissioned a study of this issue.[29]
 Preliminary work has been conducted on methods of integrating artificial general intelligences (full ethical agents as defined above) with existing legal and social frameworks. Approaches have focused on their legal position and rights.[30]
 Big data and machine learning algorithms have become popular in numerous industries, including online advertising, credit ratings, and criminal sentencing, with the promise of providing more objective, data-driven results, but have been identified as a potential way to perpetuate social inequalities and discrimination.[31][32] A 2015 study found that women were less likely than men to be shown high-income job ads by Google's AdSense. Another study found that Amazon's same-day delivery service was intentionally made unavailable in black neighborhoods. Both Google and Amazon were unable to isolate these outcomes to a single issue, and said the outcomes were the result of the black box algorithms they use.[31]
 The U.S. judicial system has begun using quantitative risk assessment software when making decisions related to releasing people on bail and sentencing in an effort to be fairer and reduce the imprisonment rate. These tools analyze a defendant's criminal history, among other attributes. In a study of 7,000 people arrested in Broward County, Florida, only 20% of people predicted to commit a crime using the county's risk assessment scoring system proceeded to commit a crime.[32] A 2016 ProPublica report analyzed recidivism risk scores calculated by one of the most commonly used tools, the Northpointe COMPAS system, and looked at outcomes over two years. The report found that only 61% of those deemed high-risk committed additional crimes during that period. The report also flagged that African-American defendants were far more likely to be given high-risk scores than their white counterparts.[32] It has been argued that such pretrial risk assessments violate Equal Protection rights on the basis of race, due to factors including possible discriminatory intent by the algorithm itself, under a theory of partial legal capacity for artificial intelligences.[33]
 In 2016, the Obama administration's Big Data Working Group—an overseer of various big-data regulatory frameworks—released reports warning of ""the potential of encoding discrimination in automated decisions"" and calling for ""equal opportunity by design"" for applications such as credit scoring.[34][35] The reports encourage discourse among policy-makers, citizens, and academics alike, but recognize that no solution yet exists for the encoding of bias and discrimination into algorithmic systems.
 In March 2018, in an effort to address rising concerns over machine learning's impact on human rights, the World Economic Forum and Global Future Council on Human Rights published a white paper with detailed recommendations on how best to prevent discriminatory outcomes in machine learning.[36] The World Economic Forum developed four recommendations based on the UN Guiding Principles of Human Rights to help address and prevent discriminatory outcomes in machine learning:[36]
 In January 2020, Harvard University's Berkman Klein Center for Internet and Society published a meta-study of 36 prominent sets of principles for AI, identifying eight key themes: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values.[37] Researchers at the Swiss Federal Institute of Technology in Zurich conducted a similar meta-study in 2019.[38]
 There have been several attempts to make ethics computable, or at least formal. Isaac Asimov's Three Laws of Robotics are not usually considered suitable for an artificial moral agent,[39] but whether Kant's categorical imperative can be used has been studied.[40] It has been pointed out that human value is, in some aspects, very complex.[41] A way to explicitly surmount this difficulty is to receive human values directly from people through some mechanism, for example by learning them.[42][43][44] Another approach is to base current ethical considerations on previous similar situations. This is called casuistry, and could be implemented through research on the Internet. The consensus from a million past decisions would lead to a new decision that is democracy-dependent.[9] Bruce M. McLaren built an early (mid-1990s) computational model of casuistry, a program called SIROCCO built with AI and case-base reasoning techniques that retrieves and analyzes ethical dilemmas.[45] But this approach could lead to decisions that reflect society's biases and unethical behavior. The negative effects of this approach can be seen in Microsoft's Tay, a chatterbot that learned to repeat racist and sexually charged tweets.[46]
 One thought experiment focuses on a Genie Golem with unlimited powers presenting itself to the reader. This Genie declares that it will return in 50 years and demands that it be provided with a definite set of morals it will then immediately act upon. This experiment's purpose is to spark discourse over how best to handle defining sets of ethics that computers may understand.[47]
 Some recent work attempts to reconstruct AI morality and control more broadly as a problem of mutual contestation between AI as a Foucauldian subjectivity on the one hand and humans or institutions on the other hand, all within a disciplinary apparatus. Certain desiderata need to be fulfilled: embodied self-care, embodied intentionality, imagination and reflexivity, which together would condition AI's emergence as an ethical subject capable of self-conduct.[48]
 In science fiction, movies and novels have played with the idea of sentient robots and machines.
 Neill Blomkamp's Chappie (2015) enacts a scenario of being able to transfer one's consciousness into a computer.[49] Alex Garland's 2014 film Ex Machina follows an android with artificial intelligence undergoing a variation of the Turing Test, a test administered to a machine to see whether its behavior can be distinguished from that of a human. Films such as The Terminator (1984) and The Matrix (1999) incorporate the concept of machines turning on their human masters.
 Asimov considered the issue in the 1950s in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing his three laws' boundaries to see where they break down or create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.[50] Philip K. Dick's 1968 novel Do Androids Dream of Electric Sheep? explores what it means to be human. In his post-apocalyptic scenario, he questions whether empathy is an entirely human characteristic. The book is the basis for the 1982 science-fiction film Blade Runner.
"
"
 Existential risk from artificial general intelligence is the idea that substantial progress in artificial general intelligence (AGI) could result in human extinction or an irreversible global catastrophe.[1][2][3]
 One argument goes as follows: human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass humanity in general intelligence and become superintelligent, then it could become difficult or impossible to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.[4]
 The plausibility of existential catastrophe due to AI is widely debated, and hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge,[5] and whether practical scenarios for AI takeovers exist.[6] Concerns about superintelligence have been voiced by leading computer scientists and tech CEOs such as Geoffrey Hinton,[7] Yoshua Bengio,[8] Alan Turing,[a] Elon Musk,[11] and OpenAI CEO Sam Altman.[12] In 2022, a survey of AI researchers with a 17% response rate found that the majority of respondents believed there is a 10 percent or greater chance that our inability to control AI will cause an existential catastrophe.[13][14] In 2023, hundreds of AI experts and other notable figures signed a statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"".[15] Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak[16] and United Nations Secretary-General António Guterres[17] called for an increased focus on global AI regulation.
 Two sources of concern stem from the problems of AI control and alignment: controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would resist attempts to disable it or change its goals, as that would prevent it from accomplishing its present goals. It would be extremely difficult to align a superintelligence with the full breadth of significant human values and constraints.[1][18][19] In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.[20]
 A third source of concern is that a sudden ""intelligence explosion"" might take an unprepared human race by surprise. Such scenarios consider the possibility that an AI that is more intelligent than its creators might be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers and society at large to control.[1][18] Empirically, examples like AlphaZero teaching itself to play Go show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such systems do not involve altering their fundamental architecture.[21]
 One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler, who wrote in his 1863 essay Darwin among the Machines:[22]
 The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article ""Intelligent Machinery, A Heretical Theory"", in which he proposed that artificial general intelligences would likely ""take control"" of the world as they became more intelligent than human beings:
 Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's Erewhon.[23] In 1965, I. J. Good originated the concept now known as an ""intelligence explosion"" and said the risks were underappreciated:[24]
 Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.[25] Scholars such as Marvin Minsky[26] and I. J. Good himself[27] occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, ""Why The Future Doesn't Need Us"", identifying superintelligent robots as a high-tech danger to human survival, alongside nanotechnology and engineered bioplagues.[28]
 Nick Bostrom published Superintelligence in 2014, which presented his arguments that superintelligence poses an existential threat.[29] By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence.[30][31][32][33] Also in 2015, the Open Letter on Artificial Intelligence highlighted the ""great potential of AI"" and encouraged more research on how to make it robust and beneficial.[34] In April 2016, the journal Nature warned: ""Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours"".[35] In 2020, Brian Christian published The Alignment Problem, which details the history of progress on AI alignment up to that time.[36][37]
 In March 2023, key figures in AI, such as Musk, signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated.[38] In May 2023, the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"".[39][40]
 Artificial general intelligence (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.[41] A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.[42] Meanwhile, some researchers dismiss existential risks from AGI as ""science fiction"" based on their high confidence that AGI will not be created anytime soon.[43]
 Breakthroughs in large language models have led some researchers to reassess their expectations. Notably, Geoffrey Hinton said in 2023 that he recently changed his estimate from ""20 to 50 years before we have general purpose A.I."" to ""20 years or less"".[44]
 In contrast with AGI, Bostrom defines a superintelligence as ""any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest"", including scientific creativity, strategic planning, and social skills.[45][4] He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.[46][4] Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is ""fundamentally on our side"".[47]
 Stephen Hawking argued that superintelligence is physically possible because ""there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains"".[31]
 When artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, OpenAI leaders said that not only AGI, but superintelligence may be achieved in less than 10 years.[48]
 Bostrom argues that AI has many advantages over the human brain:[4]
 According to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.[4][46] This suggests that an intelligence explosion may someday catch humanity unprepared.[4]
 The economist Robin Hanson has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.[49]
 In a ""fast takeoff"" scenario, the transition from AGI to superintelligence could take days or months. In a ""slow takeoff"", it could take years or decades, leaving more time for society to prepare.[50]
 Superintelligences are sometimes called ""alien minds"", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.[51] To avoid anthropomorphism, superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.[4]
 The field of ""mechanistic interpretability"" aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.[52]
 It has been argued that there are limitations to what intelligence can achieve. Notably, the chaotic nature or time complexity of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.[53]
 Advanced AI could generate enhanced pathogens, cyberattacks or manipulate people. These capabilities could be misused by humans,[54] or exploited by the AI itself if misaligned.[4] A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,[4] but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems. They may cause societal instability and empower malicious actors.[54]
 Geoffrey Hinton warned that in the short term, the profusion of AI-generated text, images and videos will make it more difficult to figure out the truth, which he says authoritarian states could exploit to manipulate elections.[55] Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide ""irreversible totalitarian regime"". It could also be used by malicious actors to fracture society and make it dysfunctional.[54]
 AI-enabled cyberattacks are increasingly considered a present and critical threat. According to NATO's technical director of cyberspace, ""The number of attacks is increasing exponentially"".[56] AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.[57]
 AI could improve the ""accessibility, success rate, scale, speed, stealth and potency of cyberattacks"", potentially causing ""significant geopolitical turbulence"" if it facilitates attacks more than defense.[54]
 Speculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.[58]
 As AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in synthetic biology to engage in bioterrorism. Dual-use technology that is useful for medicine could be repurposed to create weapons.[54]
 For example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for chemical warfare, including known and novel molecules.[54][59]
 Companies, state actors, and other organizations competing to develop AI technologies could lead to a race to the bottom of safety standards.[60] As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.[61][54]
 AI could be used to gain military advantages via autonomous lethal weapons, cyberwarfare, or automated decision-making.[54] As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film Slaughterbots.[62] AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.[54][63]
 An existential risk is ""one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development"".[65]
 Besides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a ""value lock-in"": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventing moral progress. AI could also be used to spread and preserve the set of values of whoever develops it.[66] AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[67]
 It is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if sentient machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.[68][69] Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called ""super-beneficiaries"". Such an opportunity raises the question of how to share the world and which ""ethical and political framework"" would enable a mutually beneficial coexistence between biological and digital minds.[70]
 AI may also drastically improve humanity's future. Toby Ord considers the existential risk a reason for ""proceeding with due caution"", not for abandoning AI.[67] Max More calls AI an ""existential opportunity"", highlighting the cost of not developing it.[71]
 According to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as molecular nanotechnology or synthetic biology. It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.[4]
 The alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs.
 An ""instrumental"" goal is a sub-goal that helps to achieve an agent's ultimate goal. ""Instrumental convergence"" refers to the fact that some sub-goals are useful for achieving virtually any ultimate goal, such as acquiring resources or self-preservation.[72] Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.[4]
 Russell argues that a sufficiently advanced machine ""will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.""[20][75]
 Even if current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify their goal structures, a sufficiently advanced AI might resist any attempts to change its goal structure, just as a pacifist would not want to take a pill that makes them want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself being ""turned off"" or reprogrammed with a new goal.[4][76] This is particularly relevant to value lock-in scenarios. The field of ""corrigibility"" studies how to make agents that will not resist attempts to change their goals.[77]
 In the ""intelligent agent"" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or ""utility function"". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean ""minimize the average network latency in this specific telecommunications model"" or ""maximize the number of reward clicks"", but do not know how to write a utility function for ""maximize human flourishing""; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.[78][79]
 An additional source of concern is that AI ""must reason about what people intend rather than carrying out commands literally"", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.[80]
 Some researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes:
 Alternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, ""A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true"".[4]
 In 2023, OpenAI started a project called ""Superalignment"" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence may be achieved within a decade. Its strategy involves automating alignment research using artificial intelligence.[84]
 Artificial Intelligence: A Modern Approach, a widely used undergraduate AI textbook,[85][86] says that superintelligence ""might mean the end of the human race"".[1] It states: ""Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.""[1] Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:[1]
 AI systems uniquely add a third problem: that even given ""correct"" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.[1][89]
 Some skeptics, such as Timothy B. Lee of Vox, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.[90]
 Bostrom's ""orthogonality thesis"" argues instead that, with some technical caveats, almost any level of ""intelligence"" or ""optimization power"" can be combined with almost any ultimate goal. If a machine is given the sole purpose to enumerate the decimals of pi, then no moral and ethical rules will stop it from achieving its programmed goal by any means. The machine may use all available physical and informational resources to find as many decimals of pi as it can.[91] Bostrom warns against anthropomorphism: a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.[92]
 Stuart Armstrong argues that the orthogonality thesis follows logically from the philosophical ""is-ought distinction"" argument against moral realism. He claims that even if there are moral facts provable by any ""rational"" agent, the orthogonality thesis still holds: it is still possible to create a non-philosophical ""optimizing machine"" that can strive toward some narrow goal but that has no incentive to discover any ""moral facts"" such as those that could get in the way of goal completion. Another argument he makes is that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function. Armstrong further argues that if the orthogonality thesis is false, there must be some immoral goals that AIs can never achieve, which he finds implausible.[93]
 Skeptic Michael Chorost explicitly rejects Bostrom's orthogonality thesis, arguing that ""by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.""[94] Chorost argues that ""an A.I. will need to desire certain states and dislike others. Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.""[94]
 Anthropomorphic arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.[18] Instead, advanced AI systems are typically modeled as intelligent agents.
 The academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism.[18] Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms.[18][95]
 Evolutionary psychologist Steven Pinker, a skeptic, argues that ""AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world""; perhaps instead ""artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.""[96] Facebook's director of AI research, Yann LeCun, has said: ""Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives"".[97]
 Despite other differences, the x-risk school[b] agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,[98] and that computer systems do not generally have a computational equivalent of testosterone.[99] They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of instrumental convergence.
 Bostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.[100][101] Roman Yampolskiy and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in cybercrime,[102][103] or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.[3]:158
 Some scholars have proposed hypothetical scenarios to illustrate some of their concerns.
 In Superintelligence, Bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because ""it could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous"". He suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents—a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson: the smarter the AI, the safer it is. ""And so we boldly go—into the whirling knives"", as the superintelligent AI takes a ""treacherous turn"" and exploits a decisive strategic advantage.[4]
 In Max Tegmark's 2017 book Life 3.0, a corporation's ""Omega team"" creates an extremely powerful AI able to moderately improve its own source code in a number of areas. After a certain point, the team chooses to publicly downplay the AI's ability in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI in a box where it is mostly unable to communicate with the outside world, and uses it to make money, by diverse means such as Amazon Mechanical Turk tasks, production of animated films and TV shows, and development of biotech drugs, with profits invested back into further improving AI. The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use ""for the greater good"" to prevent wars. The team faces risks that the AI could try to escape by inserting ""backdoors"" in the systems it designs, by hidden messages in its produced content, or by using its growing understanding of human behavior to persuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.[104][105]
 The thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground.
 Observers tend to agree that AI has significant potential to improve society.[106][107] The Asilomar AI Principles, which contain only those principles agreed to by 90% of the attendees of the Future of Life Institute's Beneficial AI 2017 conference,[105] also agree in principle that ""There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities"" and ""Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.""[108][109]
 Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford has said: ""I think it seems wise to apply something like Dick Cheney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low—but the implications are so dramatic that it should be taken seriously"".[110] Similarly, an otherwise skeptical Economist wrote in 2014 that ""the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote"".[46]
 AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of ""those inane Terminator pictures"" to illustrate AI safety concerns: ""It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work ... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.""[105][111] Toby Ord wrote that the idea that an AI takeover requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.[67]
 A 2022 expert survey with a 17% response rate gave a median expectation of 5–10% for the possibility of human extinction from artificial intelligence.[14][112]
 The thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including Alan Turing,[a] the most-cited computer scientist Geoffrey Hinton,[113] Elon Musk,[11] OpenAI CEO Sam Altman,[12][114] Bill Gates, and Stephen Hawking.[114] Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not ""understand why some people are not concerned"",[115] and Hawking criticized widespread indifference in his 2014 editorial:
 So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[31] Concern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, Peter Thiel, Amazon Web Services, and Musk and others jointly committed $1 billion to OpenAI, consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.[116] Facebook co-founder Dustin Moskovitz has funded and seeded multiple labs working on AI Alignment,[117] notably $5.5 million in 2016 to launch the Centre for Human-Compatible AI led by Professor Stuart Russell.[118] In January 2015, Elon Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The institute's goal is to ""grow wisdom with which we manage"" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to ""just keep an eye on what's going on with artificial intelligence,[119] saying ""I think there is potentially a dangerous outcome there.""[120][121]
 In early statements on the topic, Geoffrey Hinton, a major pioneer of deep learning, noted that ""there is not a good track record of less intelligent things controlling things of greater intelligence"", but said he continued his research because ""the prospect of discovery is too sweet"".[122][123] In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: ""I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that."" He also remarked, ""Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary.""[124]
 In his 2020 book The Precipice: Existential Risk and the Future of Humanity, Toby Ord, a Senior Research Fellow at Oxford University's Future of Humanity Institute, estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.[67]
 Baidu Vice President Andrew Ng said in 2015 that AI existential risk is ""like worrying about overpopulation on Mars when we have not even set foot on the planet yet.""[96][125] For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.[126][127]
 Skeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.[128] AI and AI ethics researchers Timnit Gebru, Emily M. Bender, Margaret Mitchell, and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.[129] They further note the association between those warning of existential risk and longtermism, which they describe as a ""dangerous ideology"" for its unscientific and utopian nature.[130]
 Wired editor Kevin Kelly argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.[131]
 Meta chief AI scientist Yann LeCun says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.[132]
 Several skeptics emphasize the potential near-term benefits of AI. Meta CEO Mark Zuckerberg believes AI will ""unlock a huge amount of positive things"", such as curing disease and increasing the safety of autonomous cars.[133]
 
During a 2016 Wired interview of President Barack Obama and MIT Media Lab's Joi Ito, Ito said:  There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen. Obama added:[134][135]
 And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man. Hillary Clinton wrote in What Happened:
 Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it ""the greatest risk we face as a civilization"". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I'd start talking about ""the rise of the robots"" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.[136] In 2018, a SurveyMonkey poll of the American public by USA Today found 68% thought the real current threat remains ""human intelligence"", but also found that 43% said superintelligent AI, if it were to happen, would result in ""more harm than good"", and that 38% said it would do ""equal amounts of harm and good"".[137]
 An April 2023 YouGov poll of US adults found 46% of respondents were ""somewhat concerned"" or ""very concerned"" about ""the possibility that AI will cause the end of the human race on Earth,"" compared with 40% who were ""not very concerned"" or ""not at all concerned.""[138]
 According to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.[139]
 Many scholars concerned about the AGI existential risk believe that the best approach is to conduct substantial research into solving the difficult ""control problem"": what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly manner after it reaches superintelligence?[4][140] Social measures may mitigate the AGI existential risk;[141][142] for instance, one recommendation is for a UN-sponsored ""Benevolent AGI Treaty"" that would ensure only altruistic AGIs be created.[143] Similarly, an arms control approach has been suggested, as has a global peace treaty grounded in the international relations theory of conforming instrumentalism, with an artificial superintelligence potentially being a signatory.[144][145]
 Researchers at Google have proposed research into general ""AI safety"" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.[146][147] A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests that funding of protective technologies should be prioritized over potentially dangerous ones.[77] Some funders, such as Musk, propose that radical human cognitive enhancement could be such a technology, for example direct neural linking between human and machine; others argue that enhancement technologies may themselves pose an existential risk.[148][149] Researchers could closely monitor or attempt to ""box in"" an initial AI at a risk of becoming too powerful. A dominant superintelligent AI, if aligned with human interests, might itself take action to mitigate the risk of takeover by rival AI, although the creation of the dominant AI could itself pose an existential risk.[150]
 Institutions such as the Alignment Research Center,[151] the Machine Intelligence Research Institute, the Future of Humanity Institute,[152][153] the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI[154] are involved in research into AI risk and safety.
 Some scholars have said that even if AGI poses an existential risk, attempting to ban research into artificial intelligence is still unwise, and probably futile.[155][156][157] Skeptics argue that regulation of AI is completely valueless, as no existential risk exists. But scholars who believe in existential risk say it is difficult to depend on people from the AI industry to regulate or constrain AI research because it directly contradicts their personal interests.[158] The scholars also agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly.[158] The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.[159][160] Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend toward general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and politicizing the underlying debate.[161]
 In March 2023, the Future of Life Institute drafted Pause Giant AI Experiments: An Open Letter, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems ""more powerful than GPT-4"" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of ""a profound change in the history of life on Earth"" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.[107][162] The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,[163] missing technical nuance about when to pause,[164] or not going far enough.[165]
 Musk called for some sort of regulation of AI development as early as 2017. According to NPR, he is ""clearly not thrilled"" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: ""Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation."" Musk states the first step would be for the government to gain ""insight"" into the actual status of current research, warning that ""Once there is awareness, people will be extremely afraid... [as] they should be."" In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[166][167][168]
 In 2021 the United Nations (UN) considered banning autonomous lethal weapons, but consensus could not be reached.[169] In July 2023 the UN Security Council for the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.[170][171] Secretary-General António Guterres advocated the creation of a global watchdog to oversee the emerging technology, saying, ""Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.""[17] At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to ""create military hegemony or undermine the sovereignty of a country.""[170]
 Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[172] AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[173][113]
 In July 2023, the US government secured voluntary safety commitments from major tech companies, including OpenAI, Amazon, Google, Meta, and Microsoft. The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of the AI Now Institute, said, ""A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough"" and called for public deliberation and regulations of the kind to which companies would not voluntarily agree.[174][175]
 In October 2023, U.S. President Joe Biden issued an executive order on the ""Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence"".[176] Alongside other requirements, the order mandates the development of guidelines for AI models that permit the ""evasion of human control.""
"
"The impact of artificial intelligence on workers includes both applications to improve worker safety and health, and potential hazards that must be controlled.
 One potential application is using AI to eliminate hazards by removing humans from hazardous situations that involve risk of stress, overwork, or musculoskeletal injuries.  Predictive analytics may also be used to identify conditions that may lead to hazards such as fatigue, repetitive strain injuries, or toxic substance exposure, leading to earlier interventions.  Another is to streamline workplace safety and health workflows through automating repetitive tasks, enhancing safety training programs through virtual reality, or detecting and reporting near misses.
 When used in the workplace, AI also presents the possibility of new hazards.  These may arise from machine learning techniques leading to unpredictable behavior and inscrutability in their decision-making, or from cybersecurity and information privacy issues.  Many hazards of AI are psychosocial due to its potential to cause changes in work organization.  These include changes in the skills required of workers,[1] increased monitoring leading to micromanagement, algorithms unintentionally or intentionally mimicking undesirable human biases, and assigning blame for machine errors to the human operator instead.  AI may also lead to physical hazards in the form of human–robot collisions, and ergonomic risks of control interfaces and human–machine interactions.  Hazard controls include cybersecurity and information privacy measures, communication and transparency with workers about data usage, and limitations on collaborative robots.
 From a workplace safety and health perspective, only ""weak"" or ""narrow"" AI that is tailored to a specific task is relevant, as there are many examples that are currently in use or expected to come into use in the near future. ""Strong"" or ""general"" AI is not expected to be feasible in the near future,[according to whom?] and discussion of its risks is within the purview of futurists and philosophers rather than industrial hygienists.
 Certain digital technologies are predicted to result in job losses. In recent years, the adoption of modern robotics has led to net employment growth. However, many businesses anticipate that automation, or employing robots would result in job losses in the future. This is especially true for companies in Central and Eastern Europe.[2][3][4] Other digital technologies, such as platforms or big data, are projected to have a more neutral impact on employment.[2][4]
 In order for any potential AI health and safety application to be adopted, it requires acceptance by both managers and workers.  For example, worker acceptance may be diminished by concerns about information privacy,[5] or from a lack of trust and acceptance of the new technology, which may arise from inadequate transparency or training.[6]: 26–28, 43–45   Alternatively, managers may emphasize increases in economic productivity rather than gains in worker safety and health when implementing AI-based systems.[7]
 AI may increase the scope of work tasks where a worker can be removed from a situation that carries risk.  In a sense, while traditional automation can replace the functions of a worker's body with a robot, AI effectively replaces the functions of their brain with a computer.  Hazards that can be avoided include stress, overwork, musculoskeletal injuries, and boredom.[8]: 5–7 
 This can expand the range of affected job sectors into white-collar and service sector jobs such as in medicine, finance, and information technology.[9]  As an example, call center workers face extensive health and safety risks due to its repetitive and demanding nature and its high rates of micro-surveillance. AI-enabled chatbots lower the need for humans to perform the most basic call center tasks.[8]: 5–7 
 Machine learning is used for people analytics to make predictions about worker behavior to assist management decision-making, such as hiring and performance assessment.  These could also be used to improve worker health.  The analytics may be based on inputs such as online activities, monitoring of communications, location tracking, and voice analysis and body language analysis of filmed interviews.  For example, sentiment analysis may be used to spot fatigue to prevent overwork.[8]: 3–7  Decision support systems have a similar ability to be used to, for example, prevent industrial disasters or make disaster response more efficient.[12]
 For manual material handling workers, predictive analytics and artificial intelligence may be used to reduce musculoskeletal injury.  Traditional guidelines are based on statistical averages and are geared towards anthropometrically typical humans.  The analysis of large amounts of data from wearable sensors may allow real-time, personalized calculation of ergonomic risk and fatigue management, as well as better analysis of the risk associated with specific job roles.[5]
 Wearable sensors may also enable earlier intervention against exposure to toxic substances than is possible with area or breathing zone testing on a periodic basis. Furthermore, the large data sets generated could improve workplace health surveillance, risk assessment, and research.[12]
 AI can also be used to make the workplace safety and health workflow more efficient.  One example is coding of workers' compensation claims, which are submitted in a prose narrative form and must manually be assigned standardized codes.  AI is being investigated to perform this task faster, more cheaply, and with fewer errors.[13][14]
 AI‐enabled virtual reality systems may be useful for safety training for hazard recognition.[12]
 Artificial intelligence may be used to more efficiently detect near misses.  Reporting and analysis of near misses are important in reducing accident rates, but they are often underreported because they are not noticed by humans, or are not reported by workers due to social factors.[15]
 There are several broad aspects of AI that may give rise to specific hazards.  The risks depend on implementation rather than the mere presence of AI.[8]: 2–3 
 Systems using sub-symbolic AI such as machine learning may behave unpredictably and are more prone to inscrutability in their decision-making.  This is especially true if a situation is encountered that was not part of the AI's training dataset, and is exacerbated in environments that are less structured.  Undesired behavior may also arise from flaws in the system's perception (arising either from within the software or from sensor degradation), knowledge representation and reasoning, or from software bugs.[6]: 14–18   They may arise from improper training, such as a user applying the same algorithm to two problems that do not have the same requirements.[8]: 12–13   Machine learning applied during the design phase may have different implications than that applied at runtime.  Systems using symbolic AI are less prone to unpredictable behavior.[6]: 14–18 
 The use of AI also increases cybersecurity risks relative to platforms that do not use AI,[6]: 17  and information privacy concerns about collected data may pose a hazard to workers.[5]
 Psychosocial hazards are those that arise from the way work is designed, organized, and managed, or its economic and social contexts, rather than arising from a physical substance or object.  They cause not only psychiatric and psychological outcomes such as occupational burnout, anxiety disorders, and depression, but they can also cause physical injury or illness such as cardiovascular disease or musculoskeletal injury.[16]  Many hazards of AI are psychosocial in nature due to its potential to cause changes in work organization, in terms of increasing complexity and interaction between different organizational factors.  However, psychosocial risks are often overlooked by designers of advanced manufacturing systems.[7]
 AI is expected to lead to changes in the skills required of workers, requiring training of existing workers, flexibility, and openness to change.[1]  The requirement for combining conventional expertise with computer skills may be challenging for existing workers.[7]  Over-reliance on AI tools may lead to deskilling of some professions.[12]
 Increased monitoring may lead to micromanagement and thus to stress and anxiety.  A perception of surveillance may also lead to stress.  Controls for these include consultation with worker groups, extensive testing, and attention to introduced bias.  Wearable sensors, activity trackers, and augmented reality may also lead to stress from micromanagement, both for assembly line workers and gig workers.  Gig workers also lack the legal protections and rights of formal workers.[8]: 2–10 
 There is also the risk of people being forced to work at a robot's pace, or to monitor robot performance at nonstandard hours.[8]: 5–7 
 Algorithms trained on past decisions may mimic undesirable human biases, for example, past discriminatory hiring and firing practices.  Information asymmetry between management and workers may lead to stress, if workers do not have access to the data or algorithms that are the basis for decision-making.[8]: 3–5 
 In addition to building a model with inadvertently discriminatory features, intentional discrimination may occur through designing metrics that covertly result in discrimination through correlated variables in a non-obvious way.[8]: 12–13 
 In complex human‐machine interactions, some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead.[12]
 Physical hazards in the form of human–robot collisions may arise from robots using AI, especially collaborative robots (cobots).  Cobots are intended to operate in close proximity to humans, which makes impossible the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots.  Automated guided vehicles are a type of cobot that as of 2019 are in common use, often as forklifts or pallet jacks in warehouses or factories.[6]: 5, 29–30   For cobots, sensor malfunctions or unexpected work environment conditions can lead to unpredictable robot behavior and thus to human–robot collisions.[8]: 5–7 
 Self-driving cars are another example of AI-enabled robots.  In addition, the ergonomics of control interfaces and human–machine interactions may give rise to hazards.[7]
 AI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions,[6]: 17  as well as information privacy measures.[5]  Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues.[5] Proposed best practices for employer‐sponsored worker monitoring programs include using only validated sensor technologies; ensuring voluntary worker participation; ceasing data collection outside the workplace; disclosing all data uses; and ensuring secure data storage.[12]
 For industrial cobots equipped with AI‐enabled sensors, the International Organization for Standardization (ISO) recommended: (a) safety‐related monitored stopping controls; (b) human hand guiding of the cobot; (c) speed and separation monitoring controls; and (d) power and force limitations.  Networked AI-enabled cobots may share safety improvements with each other.[12] Human oversight is another general hazard control for AI.[8]: 12–13 
 Both applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management.  As with all hazards, risk identification is most effective and least costly when done in the design phase.[7]
 Workplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate and does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs.  Proxies for skill content include educational requirements and classifications of routine versus non-routine, and cognitive versus physical jobs.  However, these may still not be specific enough to distinguish specific occupations that have distinct impacts from AI.  The United States Department of Labor's Occupational Information Network is an example of a database with a detailed taxonomy of skills.  Additionally, data are often reported on a national level, while there is much geographical variation, especially between urban and rural areas.[9]
 As of 2019[update], ISO was developing a standard on the use of metrics and dashboards, information displays presenting company metrics for managers, in workplaces.  The standard is planned to include guidelines for both gathering data and displaying it in a viewable and useful manner.[8]: 11 [17][18]
 In the European Union, the General Data Protection Regulation, while oriented towards consumer data, is also relevant for workplace data collection.  Data subjects, including workers, have ""the right not to be subject to a decision based solely on automated processing"".  Other relevant EU directives include the Machinery Directive (2006/42/EC), the Radio Equipment Directive (2014/53/EU), and the General Product Safety Directive (2001/95/EC).[8]: 10, 12–13 
"
"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called ""hints"".[4][5]
 
In a widely cited 1997 paper, Rich Caruana gave the following characterization: Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.[3] In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance.[citation needed] Further examples of settings for MTL include multiclass classification and multi-label classification.[6]
 Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled.[7] However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.[7][8]
 The key challenge in multi-task learning, is how to combine learning signals from multiple tasks into a single model. This may strongly depend on how well different task agree with each other, or contradict each other. There are several ways to address this challenge:  
 Within the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases.[9] Task relatedness can be imposed a priori or learned from the data.[6][10] Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly.[7][11] For example, the explicit learning of sample relevance across tasks can be done to guarantee the effectiveness of joint learning across multiple domains.[7]
 One can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.[8]
 Related to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet,[12] an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.[13]
 Traditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL).[14] Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.
 Multitask optimization: In some cases, the simultaneous training of seemingly related tasks may hinder performance compared to single-task models.[15] Commonly, MTL models employ task-specific modules on top of a joint feature representation obtained using a shared module. Since this joint representation must capture useful features across all tasks, MTL may hinder individual task performance if the different tasks seek conflicting representation, i.e., the gradients of different tasks point to opposing directions or differ significantly in magnitude. This phenomenon is commonly referred to as negative transfer. To mitigate this issue, various MTL optimization methods have been proposed. Commonly, the per-task gradients are combined into a joint update direction through various aggregation algorithms or heuristics. These methods include subtracting the projection of conflicted gradients,[16] applying techniques from game theory,[17] and using Bayesian modeling to get a distribution over gradients.[18]
 The MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.[6]
 Suppose the training data set is 






S



t


=
{
(

x

i


t


,

y

i


t


)

}

i
=
1



n

t






{\displaystyle {\mathcal {S}}_{t}=\{(x_{i}^{t},y_{i}^{t})\}_{i=1}^{n_{t}}}

, with 




x

i


t


∈


X




{\displaystyle x_{i}^{t}\in {\mathcal {X}}}

, 




y

i


t


∈


Y




{\displaystyle y_{i}^{t}\in {\mathcal {Y}}}

, where t indexes task, and 



t
∈
1
,
.
.
.
,
T


{\displaystyle t\in 1,...,T}

. Let 



n
=

∑

t
=
1


T



n

t




{\displaystyle n=\sum _{t=1}^{T}n_{t}}

. In this setting there is a consistent input and output space and the same loss function 





L


:

R

×

R

→


R


+




{\displaystyle {\mathcal {L}}:\mathbb {R} \times \mathbb {R} \rightarrow \mathbb {R} _{+}}

 for each task: . This results in the regularized machine learning problem: 
     where 





H




{\displaystyle {\mathcal {H}}}

 is a vector valued reproducing kernel Hilbert space with functions 



f
:


X


→



Y



T




{\displaystyle f:{\mathcal {X}}\rightarrow {\mathcal {Y}}^{T}}

 having components 




f

t


:


X


→


Y




{\displaystyle f_{t}:{\mathcal {X}}\rightarrow {\mathcal {Y}}}

.
 The reproducing kernel for the space 





H




{\displaystyle {\mathcal {H}}}

 of functions   



f
:


X


→


R


T




{\displaystyle f:{\mathcal {X}}\rightarrow \mathbb {R} ^{T}}

 is a symmetric matrix-valued function 



Γ
:


X


×


X


→


R


T
×
T




{\displaystyle \Gamma :{\mathcal {X}}\times {\mathcal {X}}\rightarrow \mathbb {R} ^{T\times T}}

  , such that 



Γ
(
⋅
,
x
)
c
∈


H




{\displaystyle \Gamma (\cdot ,x)c\in {\mathcal {H}}}

 and the following reproducing property holds: 
      The reproducing kernel gives rise to a representer theorem showing that any solution to equation 1 has the form:
     The form of the kernel Γ induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a separable kernel, which factors into separate kernels on the input space X and on the tasks 



{
1
,
.
.
.
,
T
}


{\displaystyle \{1,...,T\}}

. In this case the kernel relating scalar components 




f

t




{\displaystyle f_{t}}

 and 




f

s




{\displaystyle f_{s}}

 is given by 



γ
(
(

x

i


,
t
)
,
(

x

j


,
s
)
)
=
k
(

x

i


,

x

j


)

k

T


(
s
,
t
)
=
k
(

x

i


,

x

j


)

A

s
,
t




{\textstyle \gamma ((x_{i},t),(x_{j},s))=k(x_{i},x_{j})k_{T}(s,t)=k(x_{i},x_{j})A_{s,t}}

. For vector valued functions  



f
∈


H




{\displaystyle f\in {\mathcal {H}}}

  we can write 



Γ
(

x

i


,

x

j


)
=
k
(

x

i


,

x

j


)
A


{\displaystyle \Gamma (x_{i},x_{j})=k(x_{i},x_{j})A}

, where k is a scalar reproducing kernel, and A is a symmetric positive semi-definite 



T
×
T


{\displaystyle T\times T}

 matrix. Henceforth denote 




S

+


T


=
{

PSD matrices

}
⊂


R


T
×
T




{\displaystyle S_{+}^{T}=\{{\text{PSD matrices}}\}\subset \mathbb {R} ^{T\times T}}

 .
 This factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by A. Methods for non-separable kernels Γ is a current field of research.
 For the separable case, the representation theorem is reduced to 



f
(
x
)
=

∑

i
=
1


N


k
(
x
,

x

i


)
A

c

i




{\textstyle f(x)=\sum _{i=1}^{N}k(x,x_{i})Ac_{i}}

. The model output on the training data is then KCA , where K is the 



n
×
n


{\displaystyle n\times n}

 empirical kernel matrix with entries 




K

i
,
j


=
k
(

x

i


,

x

j


)


{\textstyle K_{i,j}=k(x_{i},x_{j})}

, and C  is the 



n
×
T


{\displaystyle n\times T}

 matrix of rows 




c

i




{\displaystyle c_{i}}

.
 With the separable kernel, equation  1 can be rewritten as
     where V is a (weighted) average of L applied entry-wise to Y and KCA. (The weight is zero if 




Y

i


t




{\displaystyle Y_{i}^{t}}

 is a missing observation).
 Note the second term in P can be derived as follows:
 There are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.
 Regularizer — With the separable kernel, it can be shown (below) that 




|


|

f

|



|



H



2


=

∑

s
,
t
=
1


T



A

t
,
s


†


⟨

f

s


,

f

t



⟩




H



k






{\textstyle ||f||_{\mathcal {H}}^{2}=\sum _{s,t=1}^{T}A_{t,s}^{\dagger }\langle f_{s},f_{t}\rangle _{{\mathcal {H}}_{k}}}

, where 




A

t
,
s


†




{\displaystyle A_{t,s}^{\dagger }}

 is the  



t
,
s


{\displaystyle t,s}

 element of the pseudoinverse of 



A


{\displaystyle A}

, and 






H



k




{\displaystyle {\mathcal {H}}_{k}}

 is the RKHS based on the scalar kernel 



k


{\displaystyle k}

, and 




f

t


(
x
)
=

∑

i
=
1


n


k
(
x
,

x

i


)

A

t


⊤



c

i




{\textstyle f_{t}(x)=\sum _{i=1}^{n}k(x,x_{i})A_{t}^{\top }c_{i}}

. This formulation shows that 




A

t
,
s


†




{\displaystyle A_{t,s}^{\dagger }}

 controls the weight of the penalty associated with 



⟨

f

s


,

f

t



⟩




H



k






{\textstyle \langle f_{s},f_{t}\rangle _{{\mathcal {H}}_{k}}}

. (Note that 



⟨

f

s


,

f

t



⟩




H



k






{\textstyle \langle f_{s},f_{t}\rangle _{{\mathcal {H}}_{k}}}

 arises from 




|


|


f

t



|



|





H



k




=
⟨

f

t


,

f

t



⟩




H



k






{\textstyle ||f_{t}||_{{\mathcal {H}}_{k}}=\langle f_{t},f_{t}\rangle _{{\mathcal {H}}_{k}}}

.)
 







‖
f

‖


H



2





=


⟨


∑

i
=
1


n


γ
(
(

x

i


,

t

i


)
,
⋅
)

c

i



t

i




,

∑

j
=
1


n


γ
(
(

x

j


,

t

j


)
,
⋅
)

c

j



t

j





⟩



H









=

∑

i
,
j
=
1


n



c

i



t

i





c

j



t

j




γ
(
(

x

i


,

t

i


)
,
(

x

j


,

t

j


)
)






=

∑

i
,
j
=
1


n



∑

s
,
t
=
1


T



c

i


t



c

j


s


k
(

x

i


,

x

j


)

A

s
,
t








=

∑

i
,
j
=
1


n


k
(

x

i


,

x

j


)
⟨

c

i


,
A

c

j



⟩



R


T










=

∑

i
,
j
=
1


n


k
(

x

i


,

x

j


)
⟨

c

i


,
A

A

†


A

c

j



⟩



R


T










=

∑

i
,
j
=
1


n


k
(

x

i


,

x

j


)
⟨
A

c

i


,

A

†


A

c

j



⟩



R


T










=

∑

i
,
j
=
1


n



∑

s
,
t
=
1


T


(
A

c

i



)

t


(
A

c

j



)

s


k
(

x

i


,

x

j


)

A

s
,
t


†








=

∑

s
,
t
=
1


T



A

s
,
t


†


⟨

∑

i
=
1


n


k
(

x

i


,
⋅
)
(
A

c

i



)

t


,

∑

j
=
1


n


k
(

x

j


,
⋅
)
(
A

c

j



)

s



⟩




H



k










=

∑

s
,
t
=
1


T



A

s
,
t


†


⟨

f

t


,

f

s



⟩




H



k










{\displaystyle {\begin{aligned}\|f\|_{\mathcal {H}}^{2}&=\left\langle \sum _{i=1}^{n}\gamma ((x_{i},t_{i}),\cdot )c_{i}^{t_{i}},\sum _{j=1}^{n}\gamma ((x_{j},t_{j}),\cdot )c_{j}^{t_{j}}\right\rangle _{\mathcal {H}}\\&=\sum _{i,j=1}^{n}c_{i}^{t_{i}}c_{j}^{t_{j}}\gamma ((x_{i},t_{i}),(x_{j},t_{j}))\\&=\sum _{i,j=1}^{n}\sum _{s,t=1}^{T}c_{i}^{t}c_{j}^{s}k(x_{i},x_{j})A_{s,t}\\&=\sum _{i,j=1}^{n}k(x_{i},x_{j})\langle c_{i},Ac_{j}\rangle _{\mathbb {R} ^{T}}\\&=\sum _{i,j=1}^{n}k(x_{i},x_{j})\langle c_{i},AA^{\dagger }Ac_{j}\rangle _{\mathbb {R} ^{T}}\\&=\sum _{i,j=1}^{n}k(x_{i},x_{j})\langle Ac_{i},A^{\dagger }Ac_{j}\rangle _{\mathbb {R} ^{T}}\\&=\sum _{i,j=1}^{n}\sum _{s,t=1}^{T}(Ac_{i})^{t}(Ac_{j})^{s}k(x_{i},x_{j})A_{s,t}^{\dagger }\\&=\sum _{s,t=1}^{T}A_{s,t}^{\dagger }\langle \sum _{i=1}^{n}k(x_{i},\cdot )(Ac_{i})^{t},\sum _{j=1}^{n}k(x_{j},\cdot )(Ac_{j})^{s}\rangle _{{\mathcal {H}}_{k}}\\&=\sum _{s,t=1}^{T}A_{s,t}^{\dagger }\langle f_{t},f_{s}\rangle _{{\mathcal {H}}_{k}}\end{aligned}}}


 Output metric — an alternative output metric on 






Y



T




{\displaystyle {\mathcal {Y}}^{T}}

 can be induced by the inner product 



⟨

y

1


,

y

2



⟩

Θ


=
⟨

y

1


,
Θ

y

2



⟩



R


T






{\displaystyle \langle y_{1},y_{2}\rangle _{\Theta }=\langle y_{1},\Theta y_{2}\rangle _{\mathbb {R} ^{T}}}

. With the squared loss there is an equivalence between the separable kernels 



k
(
⋅
,
⋅
)

I

T




{\displaystyle k(\cdot ,\cdot )I_{T}}

 under the alternative metric, and 



k
(
⋅
,
⋅
)
Θ


{\displaystyle k(\cdot ,\cdot )\Theta }

, under the canonical metric.
 Output mapping — Outputs can be mapped as  



L
:



Y



T


→




Y
~






{\displaystyle L:{\mathcal {Y}}^{T}\rightarrow {\mathcal {\tilde {Y}}}}

  to a higher dimensional space to encode complex structures such as trees, graphs and strings.  For linear maps L, with appropriate choice of separable kernel, it can be shown that  



A
=

L

⊤


L


{\displaystyle A=L^{\top }L}

.
 Via the regularizer formulation, one can represent a variety of task structures easily. 
 Learning problem P can be generalized to admit learning task matrix A as follows:
     Choice of 



F
:

S

+


T


→


R


+




{\displaystyle F:S_{+}^{T}\rightarrow \mathbb {R} _{+}}

 must be designed to learn matrices A of a given type. See ""Special cases"" below.
 Restricting to the case of convex losses and coercive penalties Ciliberto et al. have shown that although Q is not convex jointly in C and A, a related problem is jointly convex.
 Specifically on the convex set 





C


=
{
(
C
,
A
)
∈


R


n
×
T


×

S

+


T



|

R
a
n
g
e
(

C

⊤


K
C
)
⊆
R
a
n
g
e
(
A
)
}


{\displaystyle {\mathcal {C}}=\{(C,A)\in \mathbb {R} ^{n\times T}\times S_{+}^{T}|Range(C^{\top }KC)\subseteq Range(A)\}}

, the equivalent problem
     is convex with the same minimum value. And if 



(

C

R


,

A

R


)


{\displaystyle (C_{R},A_{R})}

 is a minimizer for R then 



(

C

R



A

R


†


,

A

R


)


{\displaystyle (C_{R}A_{R}^{\dagger },A_{R})}

 is a minimizer for Q.
 R may be solved by a barrier method on a closed set by introducing the following perturbation:
     The perturbation via the barrier 




δ

2


t
r
(

A

†


)


{\displaystyle \delta ^{2}tr(A^{\dagger })}

 forces the objective functions to be equal to 



+
∞


{\displaystyle +\infty }

 on the boundary of 




R

n
×
T


×

S

+


T




{\displaystyle R^{n\times T}\times S_{+}^{T}}

 .
 S can be solved with a block coordinate descent method, alternating in C and A. This results in a sequence of minimizers 



(

C

m


,

A

m


)


{\displaystyle (C_{m},A_{m})}

 in S that converges to the solution in R as 




δ

m


→
0


{\displaystyle \delta _{m}\rightarrow 0}

, and hence gives the solution to Q.
 Spectral penalties - Dinnuzo et al[19] suggested setting F as the Frobenius norm 





t
r
(

A

⊤


A
)




{\displaystyle {\sqrt {tr(A^{\top }A)}}}

. They optimized Q directly using block coordinate descent, not accounting for difficulties at the boundary of 





R


n
×
T


×

S

+


T




{\displaystyle \mathbb {R} ^{n\times T}\times S_{+}^{T}}

.
 Clustered tasks learning - Jacob et al[20] suggested to learn A in the setting where T  tasks are organized in R disjoint clusters. In this case let 



E
∈
{
0
,
1

}

T
×
R




{\displaystyle E\in \{0,1\}^{T\times R}}

 be the matrix with 




E

t
,
r


=

I

(

task 

t
∈

group 

r
)


{\displaystyle E_{t,r}=\mathbb {I} ({\text{task }}t\in {\text{group }}r)}

. Setting 



M
=
I
−

E

†



E

T




{\displaystyle M=I-E^{\dagger }E^{T}}

, and  



U
=


1
T




11


⊤




{\displaystyle U={\frac {1}{T}}\mathbf {11} ^{\top }}

, the task matrix 




A

†




{\displaystyle A^{\dagger }}

  can be parameterized as a function of 



M


{\displaystyle M}

: 




A

†


(
M
)
=

ϵ

M


U
+

ϵ

B


(
M
−
U
)
+
ϵ
(
I
−
M
)


{\displaystyle A^{\dagger }(M)=\epsilon _{M}U+\epsilon _{B}(M-U)+\epsilon (I-M)}

 , with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation 






S



c


=
{
M
∈

S

+


T


:
I
−
M
∈

S

+


T


∧
t
r
(
M
)
=
r
}


{\displaystyle {\mathcal {S}}_{c}=\{M\in S_{+}^{T}:I-M\in S_{+}^{T}\land tr(M)=r\}}

. In this formulation,  



F
(
A
)
=

I

(
A
(
M
)
∈
{
A
:
M
∈



S



C


}
)


{\displaystyle F(A)=\mathbb {I} (A(M)\in \{A:M\in {\mathcal {S}}_{C}\})}

.
 Non-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.
 Non-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.
 A Matlab package called Multi-Task Learning via StructurAl Regularization (MALSAR) [21]  implements the following multi-task learning algorithms: Mean-Regularized Multi-Task Learning,[22][23] Multi-Task Learning with Joint Feature Selection,[24] Robust Multi-Task Feature Learning,[25] Trace-Norm Regularized Multi-Task Learning,[26] Alternating Structural Optimization,[27][28] Incoherent Low-Rank and Sparse Learning,[29] Robust Low-Rank Multi-Task Learning, Clustered Multi-Task Learning,[30][31] Multi-Task Learning with Graph Structures. 
"
"DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.[1][2][3]
 Google's program popularized the term (deep) ""dreaming"" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.
 The DeepDream software, originated in a deep convolutional network codenamed ""Inception"" after the film of the same name,[1][2][3] was developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014[3] and released in July 2015.
 The dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program.  The idea dates from early in the history of neural networks,[4] and similar methods have been used to synthesize visual textures.[5]
Related visualization ideas were developed (prior to Google's work) by several research groups.[6][7]
 After Google published their techniques and made their code open-source,[8] a number of tools in the form of web services, mobile applications, and desktop software appeared on the market to enable users to transform their own photos.[9]
 The software is designed to detect faces and other patterns in images, with the aim of automatically classifying images.[10] However, once trained, the network can also be run in reverse, being asked to adjust the original image slightly so that a given output neuron (e.g. the one for faces or certain animals) yields a higher confidence score. This can be used for visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. This reversal procedure is never perfectly clear and unambiguous because it utilizes a one-to-many mapping process.[11] However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles backpropagation; however, instead of adjusting the network weights, the weights are held fixed and the input is adjusted.
 For example, an existing image can be altered so that it is ""more cat-like"", and the resulting enhanced image can be again input to the procedure.[2] This usage resembles the activity of looking for animals or other patterns in clouds.
 Applying gradient descent independently to each pixel of the input produces images in which
adjacent pixels have little relation and thus the image has too much high frequency information.
The generated images can be greatly improved by including a prior or regularizer that prefers inputs
that have natural image statistics (without a preference for any particular image), or are simply smooth.[7][12][13]
For example, Mahendran et al.[12] used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in Yosinski et al.[13] An in-depth, visual exploration of feature visualization and regularization techniques was published more recently.[14]
 The cited resemblance of the imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.[15]
 Neural networks such as DeepDream have biological analogies providing insight into brain processing and the formation of consciousness. Hallucinogens such as DMT alter the function of the serotonergic system which is present within the layers of the visual cortex. Neural networks are trained on input vectors and are altered by internal variations during the training process. The input and internal modifications represent the processing of exogenous and endogenous signals respectively in the visual cortex. As internal variations are modified in deep neural networks the output image reflect these changes. This specific manipulation demonstrates how inner brain mechanisms are analogous to internal layers of neural networks. Internal noise level modifications represent how hallucinogens omit external sensory information leading internal preconceived conceptions to strongly influence visual perception.[16]
 The dreaming idea can be applied to hidden (internal) neurons other than those in the output, 
which allows exploration of the roles and representations of various parts of the network.[13]
It is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization)[17] or an entire layer of neurons.
 While dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding ""dreamed"" inputs to the training set can improve training times for abstractions in Computer Science.[18]
 The DeepDream model has also been demonstrated to have application in the field of art history.[19]
 DeepDream was used for Foster the People's music video for the song ""Doing It for the Money"".[20]
 In 2017, a research group out of the University of Sussex created a Hallucination Machine, applying the DeepDream algorithm to a pre-recorded panoramic video, allowing users to explore virtual reality environments to mimic the experience of psychoactive substances and/or psychopathological conditions.[21]  They were able to demonstrate that the subjective experiences induced by the Hallucination Machine differed significantly from control (non-‘hallucinogenic’) videos, while bearing phenomenological similarities to the psychedelic state (following administration of psilocybin).
 In 2021, a study published in the journal Entropy demonstrated the similarity between DeepDream and actual psychedelic experience with neuroscientific evidence.[22] The authors recorded Electroencephalography (EEG) of human participants during passive vision of a movie clip and its DeepDream-generated counterpart. They found that DeepDream video triggered a higher entropy in the EEG signal and a higher level of functional connectivity between brain areas,[22] both well-known biomarkers of actual psychedelic experience.[23]
 In 2022, a research group coordinated by the University of Trento ""measure[d] participants’ cognitive flexibility and creativity after the exposure to virtual reality panoramic videos and their hallucinatory-like counterparts generated by the DeepDream algorithm ... following the simulated psychedelic exposure, individuals exhibited ... an attenuated contribution of the automatic process and chaotic dynamics underlying their decision processes, presumably due to a reorganization in the cognitive dynamics that facilitates the exploration of uncommon decision strategies and inhibits automated choices.""[24]
"
"Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services,[1] the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.
 The phrases ""algorithmic transparency"" and ""algorithmic accountability""[2] are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, ""algorithmic transparency"" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair.  ""Algorithmic accountability"" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.[3]
 Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms.,[4] as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency[5] In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research.[6] In the European Union, the data protection laws that came into effect in May 2018 include a ""right to explanation"" of decisions made by algorithms, though it is unclear what this means.[7] Furthermore, the European Union founded The European Center for Algoritmic Transparency (ECAT).[8]
"
"Explainable AI (XAI), often overlapping with Interpretable AI, or Explainable Machine Learning (XML), either refers to an artificial intelligence (AI) system over which it is possible for humans to retain intellectual oversight, or refers to the methods to achieve this.[1][2] The main focus is usually on the reasoning behind the decisions or predictions made by the AI[3] which are made more understandable and transparent.[4] XAI counters the ""black box"" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.[5][6]
 XAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason.[7] XAI may be an implementation of the social right to explanation.[8] Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on.[9] This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.[10]
 Machine learning (ML) algorithms used in AI can be categorized as white-box or black-box.[11] White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts.[12] XAI algorithms follow the three principles of transparency, interpretability, and explainability. A model is transparent ""if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.""[13] Interpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans.[14][15][16] Explainability is a concept that is recognized as important, but a consensus definition is not yet available;[13] one possibility is ""the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)"".[17] If algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.[18]
 Sometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions.[19] Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image[20] and text[21] prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms.[9] Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset.[22][23][24]
 AI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command ""maximize the accuracy of assessing how positive film reviews are in the test dataset."" The AI may learn useful general rules from the test set, such as ""reviews containing the word ""horrible"" are likely to be negative."" However, it may also learn inappropriate rules, such as ""reviews containing 'Daniel Day-Lewis' are usually positive""; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be ""cheating"" or ""unfair."" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set.[25]
 Cooperation between agents – in this case, algorithms and humans – depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria.[26] This is particularly relevant in medicine,[27] especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process.[28]
 AI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre-programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to ""cheat"" by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured.[6] In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object.[29][30]
 One transparency project, the DARPA XAI program, aims to produce ""glass box"" models that are explainable to a ""human-in-the-loop"" without greatly sacrificing AI performance. Human users of such a system can understand the AI's cognition (both in real-time and after the fact) and can determine whether to trust the AI.[31] Other applications of XAI are knowledge extraction from black-box models and model comparisons.[32] In the context of monitoring systems for ethical and socio-legal compliance, the term ""glass box"" is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term ""glass box"" is often used in contrast to ""black box"" systems, which lack transparency and can be more difficult to monitor and regulate.[33]
The term is also used to name a voice assistant that produces counterfactual statements as explanations.[34]
 There is a difference between the terms explainability and interpretability in the context of AI.[35]
 During the 1970s to 1990s, symbolic reasoning systems, such as MYCIN,[37] GUIDON,[38] SOPHIE,[39] and PROTOS[40][41] could represent, reason about, and explain their reasoning for diagnostic, instructional, or machine-learning (explanation-based learning) purposes. MYCIN, developed in the early 1970s as a research prototype for diagnosing bacteremia infections of the bloodstream, could explain[42] which of its hand-coded rules contributed to a diagnosis in a specific case. Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an ""articulate expert"", explaining problem-solving strategy at a level the student could understand, so they would know what action to take next. For instance, SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting, even though it ultimately relied on the SPICE circuit simulator. Similarly, GUIDON added tutorial rules to supplement MYCIN's domain-level rules so it could explain the strategy for medical diagnosis. Symbolic approaches to machine learning relying on explanation-based learning, such as PROTOS, made use of explicit representations of explanations expressed in a dedicated explanation language, both to explain their actions and to acquire new knowledge.[41]
 In the 1980s through the early 1990s, truth maintenance systems (TMS) extended the capabilities of causal-reasoning, rule-based, and logic-based inference systems.[43]: 360–362  A TMS explicitly tracks alternate lines of reasoning, justifications for conclusions, and lines of reasoning that lead to contradictions, allowing future reasoning to avoid these dead ends. To provide an explanation, they trace reasoning from conclusions to assumptions through rule operations or logical inferences, allowing explanations to be generated from the reasoning traces. As an example, consider a rule-based problem solver with just a few rules about Socrates that concludes he has died from poison:
 By just tracing through the dependency structure the problem solver can construct the following explanation: ""Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding conservative dissident beliefs under conservative governments must drink poison.""[44]: 164–165  By the 1990s researchers began studying whether it is possible to meaningfully extract the non-hand-coded rules being generated by opaque trained neural networks.[45] Researchers in clinical expert systems creating[clarification needed] neural network-powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice.[8] In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence.[6] As a result, many academics and organizations are developing tools to help detect bias in their systems.[46]
 Marvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI (Humanistic Intelligence) as a way to create a more fair and balanced ""human-in-the-loop"" AI.[47]
 Modern complex AI techniques, such as deep learning and genetic algorithms, are naturally opaque.[48] To address this issue, methods have been developed to make new models more explainable and interpretable.[49][15][14][50][51][52] This includes layerwise relevance propagation (LRP), a technique for determining which features in a particular input vector contribute most strongly to a neural network's output.[53][54] Other techniques explain some particular prediction made by a (nonlinear) black-box model, a goal referred to as ""local interpretability"".[55][56][57][58][59][60] The mere transposition of the concepts of local interpretability into a remote context (where the black-box model is executed at a third party) is currently under scrutiny[vague].[clarification needed][61][62]
 There has been work on making glass-box models which are more transparent to inspection.[19][63] This includes decision trees,[64] Bayesian networks, sparse linear models,[65] and more.[66] The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include artificial intelligence.[67][68]
 Some techniques allow visualisations of the inputs to which individual software neurons respond to most strongly. Several groups found that neurons can be aggregated into circuits that perform human-comprehensible functions, some of which reliably arise across different networks trained independently.[69][70]
 There are various techniques to extract compressed representations of the features of given inputs, which can then be analysed by standard clustering techniques. Alternatively, networks can be trained to output linguistic explanations of their behaviour, which are then directly human-interpretable.[71] Model behaviour can also be explained with reference to training data—for example, by evaluating which training inputs influenced a given behaviour the most.[72]
 The use of explainable artificial intelligence (XAI) in pain research, specifically in understanding the role of electrodermal activity for automated pain recognition: hand-crafted features and deep learning models in pain recognition, highlighting the insights that simple hand-crafted features can yield comparative performances to deep learning models and that both traditional feature engineering and deep feature learning approaches rely on simple characteristics of the input time-series data.[73]
 As regulators, official bodies, and general users come to depend on AI-based dynamic systems, clearer accountability will be required for automated decision-making processes to ensure trust and transparency. The first global conference exclusively dedicated to this emerging discipline was the 2017 International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI).[74]
 The European Union introduced a right to explanation in the General Data Protection Right (GDPR) to address potential problems stemming from the rising importance of algorithms. The implementation of the regulation began in 2018. However, the right to explanation in GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions.[75] In France the Loi pour une République numérique (Digital Republic Act) grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them.
 Despite ongoing endeavors to enhance the explainability of AI models, they persist with several inherent limitations.
 By making an AI system more explainable, we also reveal more of its inner workings. For example, the explainability method of feature importance identifies features or variables that are most important in determining the model's output, while the influential samples method identifies the training samples that are most influential in determining the output, given a particular input.[76] Adversarial parties could take advantage of this knowledge.
 For example, competitor firms could replicate aspects of the original AI system in their own product, thus reducing competitive advantage.[77] An explainable AI system is also susceptible to being “gamed”—influenced in a way that undermines its intended purpose. One study gives the example of a predictive policing system; in this case, those who could potentially “game” the system are the criminals subject to the system's decisions. In this study, developers of the system discussed the issue of criminal gangs looking to illegally obtain passports, and they expressed concerns that, if given an idea of what factors might trigger an alert in the passport application process, those gangs would be able to “send guinea pigs” to test those triggers, eventually finding a loophole that would allow them to “reliably get passports from under the noses of the authorities”.[78]
 A fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical ones, geared toward machine learning engineers for debugging purposes, rather than toward the end users who are ultimately affected by the system, causing “a gap between explainability in practice and the goal of transparency”.[76] Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users, or providing explanations in layperson terms.[77]
 The solution must avoid oversimplification. It is important to strike a balance between accuracy – how faithfully the explanation reflects the process of the AI system – and explainability – how well end users understand the process. This is a difficult balance to strike, since the complexity of machine learning makes it difficult for even ML engineers to fully understand, let alone non-experts.[76]
 The goal of explainability to end users of AI systems is to increase trust in the systems, even “address concerns about lack of ‘fairness’ and discriminatory effects”.[77] However, even with a good understanding of an AI system, end users may not necessarily trust the system.[79] In one study, participants were presented with combinations of white-box and black-box explanations, and static and interactive explanations of AI systems. While these explanations served to increase both their self-reported and objective understanding, it had no impact on their level of trust, which remained skeptical.[80]
 This outcome was especially true for decisions that impacted the end user in a significant way, such as graduate school admissions. Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision-makers; instead of rigidly adhering to a set of rules, humans are able to consider exceptional cases as well as appeals to their initial decision.[80] For such decisions, explainability will not necessarily cause end users to accept the use of decision-making algorithms. We will need to either turn to another method to increase trust and acceptance of decision-making algorithms, or question the need to rely solely on AI for such impactful decisions in the first place.
 However, some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users' trust in the system's decisions, but to calibrate the users' level of trust to the correct level.[81] According to this principle, too much or too little user trust in the AI system will harm the overall performance of the human-system unit. When the trust is excessive, the users are not critical of possible mistakes of the system and when the users do not have enough trust in the system, they will not exhaust the benefits inherent in it.
 Some scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness, and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly.[82][83] Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence-based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators.[82]
 Some researchers advocate the use of inherently interpretable machine learning models, rather than using post-hoc explanations in which a second model is created to explain the first. This is partly because post-hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post-hoc explanation can mimic the computations of an entirely separate model.[19] However, another view is that what is important is that the explanation accomplishes the given task at hand, and whether it is pre or post-hoc doesn't matter. If a post-hoc explanation method helps a doctor diagnose cancer better, it is of secondary importance whether it is a correct/incorrect explanation.
 The goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters. Along with other factors this leads to a theoretical limit for explainability.[84]
 Explainability was studied also in social choice theory. Social choice theory aims at finding solutions to social decision problems, that are based on well-established axioms. Procaccia[85] explains that these axioms can be used to construct convincing explanations to the solutions. This principle has been used to construct explanations in various subfields of social choice.
 Cailloux and Endriss[86] present a method for explaining voting rules using the axioms that characterize them. They exemplify their method on the Borda voting rule .
 Peters, Procaccia, Psomas and Zhou[87] present an algorithm for explaining the outcomes of the Borda rule using O(m2) explanations, and prove that this is tight in the worst case.
 Yang, Hausladen, Peters, Pournaras, Fricker and Helbing[88] present an empirical study of explainability in participatory budgeting. They compared the Greedy and the Equal Shares rules, and three types of explanations: mechanism explanation (a general explanation of how the aggregation rule works given the voting input), individual explanation (explaining how many voters had at least one approved project, at least 10000 CHF in approved projects), and group explanation (explaining how the budget is distributed among the districts and topics). They compared the perceived trustworthiness and fairness of Greedy and Equal Shares, before and after the explanations. They found out that, for MES, Mechanism explanation yields the highest increase in perceived fairness and trustworthiness; the second-highest was Group explanation. For Greedy, Mechanism explanation increases perceived trustworthiness but not fairness, whereas Individual explanation increases both perceived fairness and trustworthiness. Group explanation decreases the perceived fairness and trustworthiness.
 Nizri, Azaria and Hazon[89] present an algorithm for computing explanations for the Shapley value. Given a coalitional game, their algorithm  decomposes it to sub-games, for which it is easy to generate verbal explanations based on the axioms characterizing the Shapley value. The payoff allocation for each sub-game is perceived as fair, so the Shapley-based payoff allocation for the given game should seem fair as well. An experiment with 210 human subjects shows that, with their automatically generated explanations, subjects perceive Shapley-based payoff allocation as significantly fairer than with a general standard explanation.
"
"Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. For example gender, ethnicity, sexual orientation or disability. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.
 Discussion about fairness in machine learning is a relatively recent topic. Since 2016 there has been a sharp increase in research into the topic.[1] This increase could be partly accounted to an influential report by ProPublica that claimed that the COMPAS software, widely used in US courts to predict recidivism, was racially biased.[2] One topic of research and discussion is the definition of fairness, as there is no universal definition, and different definitions can be in contradiction with each other, which makes it difficult to judge machine learning models.[3] Other research topics include the origins of bias, the types of bias, and methods to reduce bias.[4]
 In recent years tech companies have made tools and manuals on how to detect and reduce bias in machine learning. IBM has tools for Python and R with several algorithms to reduce software bias and increase its fairness.[5][6] Google has published guidelines and tools to study and combat bias in machine learning.[7][8] Facebook have reported their use of a tool, Fairness Flow, to detect bias in their AI.[9] However, critics have argued that the company's efforts are insufficient, reporting little use of the tool by employees as it cannot be used for all their programs and even when it can, use of the tool is optional.[10]
 It is important to note that the discussion about quantitative ways to test fairness and unjust discrimination in decision-making predates by several decades the rather recent debate on fairness in machine learning.[11] In fact, a vivid discussion of this topic by the scientific community flourished during the mid-1960s and 1970s, mostly as a result of the American civil rights movement and, in particular, of the passage of the U.S. Civil Rights Act of 1964. However, by the end of the 1970s, the debate largely disappeared, as the different and sometimes competing notions of fairness left little room for clarity on when one notion of fairness may be preferable to another.
 Language bias refers a type of statistical sampling bias tied to the language of a query that leads to ""a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository.""[better source needed][12] Luo et al. [12]show that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like ""What is liberalism?"", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like ""opposes state intervention in personal and economic life"" from the dominant Vietnamese perspective and ""limitation of government power"" from the prevalent Chinese perspective are absent. Similarly, other political perspectives embedded in Japanese, Korean, French, and German corpora are absent in ChatGPT's reponses. ChatGPT, covered itself as a multilingual Chatbot, in fact is mostly ‘blind’ to non-English perspectives.[12]
 Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms; it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[13]
 Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[14]
 The use of algorithmic decision making in the legal system has been a notable area of use under scrutiny. In 2014, then U.S. Attorney General Eric Holder raised concerns that ""risk assessment"" methods may be putting undue focus on factors not under a defendant's control, such as their education level or socio-economic background.[15] The 2016 report by ProPublica on COMPAS claimed that black defendants were almost twice as likely to be incorrectly labelled as higher risk than white defendants, while making the opposite mistake with white defendants.[2] The creator of COMPAS, Northepointe Inc., disputed the report, claiming their tool is fair and ProPublica made statistical errors,[16] which was subsequently refuted again by ProPublica.[17]
 Racial and gender bias has also been noted in image recognition algorithms. Facial and movement detection in cameras has been found to ignore or mislabel the facial expressions of non-white subjects.[18] In 2015, the automatic tagging feature in both Flickr and Google Photos was found to label black people with tags such as ""animal"" and ""gorilla"".[19] A 2016 international beauty contest judged by an AI algorithm was found to be biased towards individuals with lighter skin, likely due to bias in training data.[20] A study of three commercial gender classification algorithms in 2018 found that all three algorithms were generally most accurate when classifying light-skinned males and worst when classifying dark-skinned females.[21] In 2020, an image cropping tool from Twitter  was shown to prefer lighter skinned faces.[22] DALL-E, a machine learning Text-to-image model released in 2021, has been prone to create racist and sexist images that reinforce societal stereotypes, something that has been admitted by its creators.[23]
 Other areas where machine learning algorithms are in use that have been shown to be biased include job and loan applications. Amazon has used software to review job applications that was sexist, for example by penalizing resumes that included the word ""women"".[24] In 2019, Apple's algorithm to determine credit card limits for their new Apple Card gave significantly higher limits to males than females, even for couples that shared their finances.[25] Mortgage-approval algorithms in use in the U.S. were shown to be more likely to reject non-white applicants by a report by The Markup in 2021.[26]
 Recent works underline the presence of several limitations to the current landscape of fairness in machine learning, particularly when it comes to what is realistically achievable in this respect in the ever increasing real-world applications of AI.[27][28][29]
For instance, the mathematical and quantitative approach to formalize fairness, and the related ""de-biasing"" approaches, may rely onto too simplistic and easily overlooked assumptions, such as the categorization of individuals into pre-defined social groups. 
Other delicate aspects are, e.g., the interaction among several sensible characteristics,[21] and the lack of a clear and shared philosophical and/or legal notion of non-discrimination.
 In classification problems, an algorithm learns a function to predict a discrete characteristic 



Y


{\textstyle Y}

, the target variable, from known characteristics 



X


{\textstyle X}

. We model 



A


{\textstyle A}

 as a discrete random variable which encodes some characteristics contained or implicitly encoded in 



X


{\textstyle X}

 that we consider as sensitive characteristics (gender, ethnicity, sexual orientation, etc.). We finally denote by 



R


{\textstyle R}

 the prediction of the classifier.
Now let us define three main criteria to evaluate if a given classifier is fair, that is if its predictions are not influenced by some of these sensitive variables.[30]
 We say the random variables 



(
R
,
A
)


{\textstyle (R,A)}

 satisfy independence if the sensitive characteristics 



A


{\textstyle A}

 are statistically independent of the prediction 



R


{\textstyle R}

, and we write




R
⊥
A
.


{\displaystyle R\bot A.}


We can also express this notion with the following formula:




P
(
R
=
r
 

|

 
A
=
a
)
=
P
(
R
=
r
 

|

 
A
=
b
)

∀
r
∈
R

∀
a
,
b
∈
A


{\displaystyle P(R=r\ |\ A=a)=P(R=r\ |\ A=b)\quad \forall r\in R\quad \forall a,b\in A}


This means that the classification rate for each target classes is equal for people belonging to different groups with respect to sensitive characteristics 



A


{\displaystyle A}

.
 Yet another equivalent expression for independence can be given using the concept of mutual information between random variables, defined as




I
(
X
,
Y
)
=
H
(
X
)
+
H
(
Y
)
−
H
(
X
,
Y
)


{\displaystyle I(X,Y)=H(X)+H(Y)-H(X,Y)}


In this formula, 



H
(
X
)


{\textstyle H(X)}

 is the entropy of the random variable 



X


{\displaystyle X}

. Then 



(
R
,
A
)


{\textstyle (R,A)}

 satisfy independence if 



I
(
R
,
A
)
=
0


{\textstyle I(R,A)=0}

.
 A possible relaxation of the independence definition include introducing a positive slack 



ϵ
>
0


{\textstyle \epsilon >0}

 and is given by the formula:




P
(
R
=
r
 

|

 
A
=
a
)
≥
P
(
R
=
r
 

|

 
A
=
b
)
−
ϵ

∀
r
∈
R

∀
a
,
b
∈
A


{\displaystyle P(R=r\ |\ A=a)\geq P(R=r\ |\ A=b)-\epsilon \quad \forall r\in R\quad \forall a,b\in A}


 Finally, another possible relaxation is to require 



I
(
R
,
A
)
≤
ϵ


{\textstyle I(R,A)\leq \epsilon }

.
 We say the random variables 



(
R
,
A
,
Y
)


{\textstyle (R,A,Y)}

 satisfy separation if the sensitive characteristics 



A


{\textstyle A}

 are statistically independent of the prediction 



R


{\textstyle R}

 given the target value 



Y


{\textstyle Y}

, and we write




R
⊥
A
 

|

 
Y
.


{\displaystyle R\bot A\ |\ Y.}


We can also express this notion with the following formula:




P
(
R
=
r
 

|

 
Y
=
q
,
A
=
a
)
=
P
(
R
=
r
 

|

 
Y
=
q
,
A
=
b
)

∀
r
∈
R

q
∈
Y

∀
a
,
b
∈
A


{\displaystyle P(R=r\ |\ Y=q,A=a)=P(R=r\ |\ Y=q,A=b)\quad \forall r\in R\quad q\in Y\quad \forall a,b\in A}


This means that all the dependence of the decision 



R


{\displaystyle R}

 on the sensitive attribute 



A


{\displaystyle A}

 must be justified by the actual dependence of the true target variable 



Y


{\displaystyle Y}

.
 Another equivalent expression, in the case of a binary target rate, is that the true positive rate and the false positive rate are equal (and therefore the false negative rate and the true negative rate are equal) for every value of the sensitive characteristics:




P
(
R
=
1
 

|

 
Y
=
1
,
A
=
a
)
=
P
(
R
=
1
 

|

 
Y
=
1
,
A
=
b
)

∀
a
,
b
∈
A


{\displaystyle P(R=1\ |\ Y=1,A=a)=P(R=1\ |\ Y=1,A=b)\quad \forall a,b\in A}






P
(
R
=
1
 

|

 
Y
=
0
,
A
=
a
)
=
P
(
R
=
1
 

|

 
Y
=
0
,
A
=
b
)

∀
a
,
b
∈
A


{\displaystyle P(R=1\ |\ Y=0,A=a)=P(R=1\ |\ Y=0,A=b)\quad \forall a,b\in A}


 A possible relaxation of the given definitions is to allow the value for the difference between rates to be a positive number lower than a given slack 



ϵ
>
0


{\textstyle \epsilon >0}

, rather than equal to zero.
 In some fields separation (separation coefficient) in a confusion matrix is a measure of the distance (at a given level of the probability score) between the predicted cumulative percent negative and predicted cumulative percent positive.
 The greater this separation coefficient is at a given score value, the more effective the model is at differentiating between the set of positives and negatives at a particular probability cut-off. According to Mayes:[31] ""It is often observed in the credit industry that the selection of validation measures depends on the modeling approach. For example, if modeling procedure is parametric or semi-parametric, the two-sample K-S test is often used. If the model is derived by heuristic or iterative search methods, the measure of model performance is usually divergence. A third option is the coefficient of separation...The coefficient of separation, compared to the other two methods, seems to be most reasonable as a measure for model performance because it reflects the separation pattern of a model.""
 We say the random variables 



(
R
,
A
,
Y
)


{\textstyle (R,A,Y)}

 satisfy sufficiency if the sensitive characteristics 



A


{\textstyle A}

 are statistically independent of the target value 



Y


{\textstyle Y}

 given the prediction 



R


{\textstyle R}

, and we write




Y
⊥
A
 

|

 
R
.


{\displaystyle Y\bot A\ |\ R.}


We can also express this notion with the following formula:




P
(
Y
=
q
 

|

 
R
=
r
,
A
=
a
)
=
P
(
Y
=
q
 

|

 
R
=
r
,
A
=
b
)

∀
q
∈
Y

r
∈
R

∀
a
,
b
∈
A


{\displaystyle P(Y=q\ |\ R=r,A=a)=P(Y=q\ |\ R=r,A=b)\quad \forall q\in Y\quad r\in R\quad \forall a,b\in A}


This means that the probability of actually being in each of the groups is equal for two individuals with different sensitive characteristics given that they were predicted to belong to the same group.
 Finally, we sum up some of the main results that relate the three definitions given above:
 It is referred to as total fairness when independence, separation, and sufficiency are all satisfied simultaneously.[32] However, total fairness is not possible to achieve except in specific rhetorical cases.[33]
 
Most statistical measures of fairness rely on different metrics, so we will start by defining them. When working with a binary classifier, both the predicted and the actual classes can take two values: positive and negative. Now let us start explaining the different possible relations between predicted and actual outcome:[34] These relations can be easily represented with a confusion matrix, a table that describes the accuracy of a classification model. In this matrix, columns and rows represent instances of the predicted and the actual cases, respectively.
 By using these relations, we can define multiple metrics which can be later used to measure the fairness of an algorithm:
 The following criteria can be understood as measures of the three general definitions given at the beginning of this section, namely Independence, Separation and Sufficiency. In the table[30] to the right, we can see the relationships between them.
 To define these measures specifically, we will divide them into three big groups as done in Verma et al.:[34] definitions based on a predicted outcome, on predicted and actual outcomes, and definitions based on predicted probabilities and the actual outcome.
 We will be working with a binary classifier and the following notation: 



S


{\textstyle S}

 refers to the score given by the classifier, which is the probability of a certain subject to be in the positive or the negative class. 



R


{\textstyle R}

 represents the final classification predicted by the algorithm, and its value is usually derived from 



S


{\textstyle S}

, for example will be positive when 



S


{\textstyle S}

 is above a certain threshold. 



Y


{\textstyle Y}

 represents the actual outcome, that is, the real classification of the individual and, finally, 



A


{\textstyle A}

 denotes the sensitive attributes of the subjects.
 The definitions in this section focus on a predicted outcome 



R


{\textstyle R}

 for various distributions of subjects. They are the simplest and most intuitive notions of fairness.
 These definitions not only considers the predicted outcome 



R


{\textstyle R}

 but also compare it to the actual outcome 



Y


{\textstyle Y}

.
 These definitions are based in the actual outcome 



Y


{\textstyle Y}

 and the predicted probability score 



S


{\textstyle S}

.
 With respect to confusion matrices, independence, separation, and sufficiency require the respective quantities listed below to not have statistically significant difference across sensitive characteristics.[33]
 The notion of equal confusion fairness[35] requires the confusion matrix of a given decision system to have the same distribution when computed stratified over all sensitive characteristics.
 Some scholars have proposed defining algorithmic fairness in terms of a social welfare function. They argue that using a social welfare function enables an algorithm designer to consider fairness and predictive accuracy in terms of their benefits to the people affected by the algorithm. It also allows the designer to trade off efficiency and equity in a principled way.[36] Sendhil Mullainathan has stated that algorithm designers should use social welfare functions in order to recognize absolute gains for disadvantaged groups. For example, a study found that using a decision-making algorithm in pretrial detention rather than pure human judgment reduced the detention rates for Blacks, Hispanics, and racial minorities overall, even while keeping the crime rate constant.[37]
 An important distinction among fairness definitions is the one between group and individual notions.[38][39][34][40] Roughly speaking, while group fairness criteria compare quantities at a group level, typically identified by sensitive attributes (e.g. gender, ethnicity, age, etc.), individual criteria compare individuals. In words, individual fairness follow the principle that ""similar individuals should receive similar treatments"".
 There is a very intuitive approach to fairness, which usually goes under the name of fairness through unawareness (FTU), or blindness, that prescribes not to explicitly employ sensitive features when making (automated) decisions. This is effectively a notion of individual fairness, since two individuals differing only for the value of their sensitive attributes would receive the same outcome.
 However, in general, FTU is subject to several drawbacks, the main being that it does not take into account possible correlations between sensitive attributes and non-sensitive attributes employed in the decision-making process. For example, an agent with the (malignant) intention to discriminate on the basis of gender could introduce in the model a proxy variable for gender (i.e. a variable highly correlated with gender) and effectively using gender information while at the same time being compliant to the FTU prescription.
 The problem of what variables correlated to sensitive ones are fairly employable by a model in the decision-making process is a crucial one, and is relevant for group concepts as well: independence metrics require a complete removal of sensitive information, while separation-based metrics allow for correlation, but only as far as the labeled target variable ""justify"" them.
 The most general concept of individual fairness was introduced in the pioneer work by Cynthia Dwork and collaborators in 2012[41] and can be thought of as a mathematical translation of the principle that the decision map taking features as input should be built such that it is able to ""map similar individuals similarly"", that is expressed as a Lipschitz condition on the model map. They call this approach fairness through awareness (FTA), precisely as counterpoint to FTU, since they underline the importance of choosing the appropriate target-related distance metric in order to assess which individuals are similar in specific situations. Again, this problem is very related to the point raised above about what variables can be seen as ""legitimate"" in particular contexts.
 Causal fairness measures the frequency with which two nearly identical users or applications who differ only in a set of characteristics with respect to which resource allocation must be fair receive identical treatment.[42] [dubious  – discuss]
 An entire branch of the academic research on fairness metrics is devoted to leverage causal models to assess bias in machine learning models. This approach is usually justified by the fact that the same observational distribution of data may hide different causal relationships among the variables at play, possibly with different interpretations of whether the outcome are affected by some form of bias or not.[30]
 Kusner et al.[43] propose to employ counterfactuals, and define a decision-making process counterfactually fair if, for any individual, the outcome does not change in the counterfactual scenario where the sensitive attributes are changed. The mathematical formulation reads:
 



P
(

R

A
←
a


=
1
∣
A
=
a
,
X
=
x
)
=
P
(

R

A
←
b


=
1
∣
A
=
a
,
X
=
x
)
,

∀
a
,
b
;


{\displaystyle P(R_{A\leftarrow a}=1\mid A=a,X=x)=P(R_{A\leftarrow b}=1\mid A=a,X=x),\quad \forall a,b;}


 that is: taken a random individual with sensitive attribute 



A
=
a


{\displaystyle A=a}

 and other features 



X
=
x


{\displaystyle X=x}

 and the same individual if she had 



A
=
b


{\displaystyle A=b}

, they should have same chance of being accepted.
The symbol 







R
^




A
←
a




{\displaystyle {\hat {R}}_{A\leftarrow a}}

 represents the counterfactual random variable 



R


{\displaystyle R}

 in the scenario where the sensitive attribute 



A


{\displaystyle A}

 is fixed to 



A
=
a


{\displaystyle A=a}

. The conditioning on 



A
=
a
,
X
=
x


{\displaystyle A=a,X=x}

 means that this requirement is at the individual level, in that we are conditioning on all the variables identifying a single observation.
 Machine learning models are often trained upon data where the outcome depended on the decision made at that time.[44] For example, if a machine learning model has to determine whether an inmate will recidivate and will determine whether the inmate should be released early, the outcome could be dependent on whether the inmate was released early or not. Mishler et al.[45] propose a formula for counterfactual equalized odds:
 



P
(
R
=
1
∣

Y

0


=
0
,
A
=
a
)
=
P
(
R
=
1
∣

Y

0


=
0
,
A
=
b
)
∧
P
(
R
=
0
∣

Y

1


=
1
,
A
=
a
)
=
P
(
R
=
0
∣

Y

1


=
1
,
A
=
b
)
,

∀
a
,
b
;


{\displaystyle P(R=1\mid Y^{0}=0,A=a)=P(R=1\mid Y^{0}=0,A=b)\wedge P(R=0\mid Y^{1}=1,A=a)=P(R=0\mid Y^{1}=1,A=b),\quad \forall a,b;}


 where 



R


{\displaystyle R}

 is a random variable, 




Y

x




{\displaystyle Y^{x}}

 denotes the outcome given that the decision 



x


{\displaystyle x}

 was taken, and 



A


{\displaystyle A}

 is a sensitive feature.
 Plecko and Bareinboim[46] propose a unified framework to deal with causal analysis of fairness. They suggest the use of a Standard Fairness Model, consisting of a causal graph with 4 types of variables:
 Within this framework,  Plecko and Bareinboim[46] are therefore able to classify the possible effects that sensitive attributes may have on the outcome. 
Moreover, the granularity at which these effects are measured—namely, the conditioning variables used to average the effect—is directly connected to the ""individual vs. group"" aspect of fairness assessment.
 Fairness can be applied to machine learning algorithms in three different ways: data preprocessing, optimization during software training, or post-processing results of the algorithm.
 Usually, the classifier is not the only problem; the dataset is also biased. The discrimination of a dataset 



D


{\textstyle D}

 with respect to the group 



A
=
a


{\textstyle A=a}

 can be defined as follows:




d
i
s

c

A
=
a


(
D
)
=




|

{
X
∈
D

|

X
(
A
)
≠
a
,
X
(
Y
)
=
+
}

|




|

{
X
∈
D

|

X
(
A
)
≠
a
}

|




−




|

{
X
∈
D

|

X
(
A
)
=
a
,
X
(
Y
)
=
+
}

|




|

{
X
∈
D

|

X
(
A
)
=
a
}

|






{\displaystyle disc_{A=a}(D)={\frac {|\{X\in D|X(A)\neq a,X(Y)=+\}|}{|\{X\in D|X(A)\neq a\}|}}-{\frac {|\{X\in D|X(A)=a,X(Y)=+\}|}{|\{X\in D|X(A)=a\}|}}}


 That is, an approximation to the difference between the probabilities of belonging in the positive class given that the subject has a protected characteristic different from 



a


{\textstyle a}

 and equal to 



a


{\textstyle a}

.
 Algorithms correcting bias at preprocessing remove information about dataset variables which might result in unfair decisions, while trying to alter as little as possible. This is not as simple as just removing the sensitive variable, because other attributes can be correlated to the protected one.
 A way to do this is to map each individual in the initial dataset to an intermediate representation in which it is impossible to identify whether it belongs to a particular protected group while maintaining as much information as possible. Then, the new representation of the data is adjusted to get the maximum accuracy in the algorithm.
 This way, individuals are mapped into a new multivariable representation where the probability of any member of a protected group to be mapped to a certain value in the new representation is the same as the probability of an individual which doesn't belong to the protected group. Then, this representation is used to obtain the prediction for the individual, instead of the initial data. As the intermediate representation is constructed giving the same probability to individuals inside or outside the protected group, this attribute is hidden to the classificator.
 An example is explained in Zemel et al.[47] where a multinomial random variable is used as an intermediate representation. In the process, the system is encouraged to preserve all information except that which can lead to biased decisions, and to obtain a prediction as accurate as possible.
 On the one hand, this procedure has the advantage that the preprocessed data can be used for any machine learning task. Furthermore, the classifier does not need to be modified, as the correction is applied to the dataset before processing. On the other hand, the other methods obtain better results in accuracy and fairness.[48]
 Reweighing is an example of a preprocessing algorithm. The idea is to assign a weight to each dataset point such that the weighted discrimination is 0 with respect to the designated group.[49]
 If the dataset 



D


{\textstyle D}

 was unbiased the sensitive variable 



A


{\textstyle A}

 and the target variable 



Y


{\textstyle Y}

 would be statistically independent and the probability of the joint distribution would be the product of the probabilities as follows:





P

e
x
p


(
A
=
a
∧
Y
=
+
)
=
P
(
A
=
a
)
×
P
(
Y
=
+
)
=




|

{
X
∈
D

|

X
(
A
)
=
a
}

|




|

D

|




×




|

{
X
∈
D

|

X
(
Y
)
=
+
}

|




|

D

|






{\displaystyle P_{exp}(A=a\wedge Y=+)=P(A=a)\times P(Y=+)={\frac {|\{X\in D|X(A)=a\}|}{|D|}}\times {\frac {|\{X\in D|X(Y)=+\}|}{|D|}}}


 In reality, however, the dataset is not unbiased and the variables are not statistically independent so the observed probability is:





P

o
b
s


(
A
=
a
∧
Y
=
+
)
=




|

{
X
∈
D

|

X
(
A
)
=
a
∧
X
(
Y
)
=
+
}

|




|

D

|






{\displaystyle P_{obs}(A=a\wedge Y=+)={\frac {|\{X\in D|X(A)=a\wedge X(Y)=+\}|}{|D|}}}


 To compensate for the bias, the software adds a weight, lower for favored objects and higher for unfavored objects. For each 



X
∈
D


{\textstyle X\in D}

 we get:




W
(
X
)
=




P

e
x
p


(
A
=
X
(
A
)
∧
Y
=
X
(
Y
)
)



P

o
b
s


(
A
=
X
(
A
)
∧
Y
=
X
(
Y
)
)





{\displaystyle W(X)={\frac {P_{exp}(A=X(A)\wedge Y=X(Y))}{P_{obs}(A=X(A)\wedge Y=X(Y))}}}


 When we have for each 



X


{\textstyle X}

 a weight associated 



W
(
X
)


{\textstyle W(X)}

 we compute the weighted discrimination with respect to group 



A
=
a


{\textstyle A=a}

 as follows:




d
i
s

c

A
=
a


(
D
)
=



∑
W
(
X
)
X
∈
{
X
∈
D

|

X
(
A
)
≠
a
,
X
(
Y
)
=
+
}


∑
W
(
X
)
X
∈
{
X
∈
D

|

X
(
A
)
≠
a
}



−



∑
W
(
X
)
X
∈
{
X
∈
D

|

X
(
A
)
=
a
,
X
(
Y
)
=
+
}


∑
W
(
X
)
X
∈
{
X
∈
D

|

X
(
A
)
=
a
}





{\displaystyle disc_{A=a}(D)={\frac {\sum W(X)X\in \{X\in D|X(A)\neq a,X(Y)=+\}}{\sum W(X)X\in \{X\in D|X(A)\neq a\}}}-{\frac {\sum W(X)X\in \{X\in D|X(A)=a,X(Y)=+\}}{\sum W(X)X\in \{X\in D|X(A)=a\}}}}


 It can be shown that after reweighting this weighted discrimination is 0.
 Another approach is to correct the bias at training time. This can be done by adding constraints to the optimization objective of the algorithm.[50] These constraints force the algorithm to improve fairness, by keeping the same rates of certain measures for the protected group and the rest of individuals. For example, we can add to the objective of the algorithm the condition that the false positive rate is the same for individuals in the protected group and the ones outside the protected group.
 The main measures used in this approach are false positive rate, false negative rate, and overall misclassification rate. It is possible to add just one or several of these constraints to the objective of the algorithm. Note that the equality of false negative rates implies the equality of true positive rates so this implies the equality of opportunity. After adding the restrictions to the problem it may turn intractable, so a relaxation on them may be needed.
 This technique obtains good results in improving fairness while keeping high accuracy and lets the programmer choose the fairness measures to improve. However, each machine learning task may need a different method to be applied and the code in the classifier needs to be modified, which is not always possible.[48]
 We train two classifiers at the same time through some gradient-based method (f.e.: gradient descent). The first one, the predictor tries to accomplish the task of predicting 



Y


{\textstyle Y}

, the target variable, given 



X


{\textstyle X}

, the input, by modifying its weights 



W


{\textstyle W}

 to minimize some loss function 




L

P


(



y
^



,
y
)


{\textstyle L_{P}({\hat {y}},y)}

. The second one, the adversary tries to accomplish the task of predicting 



A


{\textstyle A}

, the sensitive variable, given 






Y
^





{\textstyle {\hat {Y}}}

 by modifying its weights 



U


{\textstyle U}

 to minimize some loss function 




L

A


(



a
^



,
a
)


{\textstyle L_{A}({\hat {a}},a)}

.[51][52]
 An important point here is that, in order to propagate correctly, 






Y
^





{\textstyle {\hat {Y}}}

 above must refer to the raw output of the classifier, not the discrete prediction; for example, with an artificial neural network and a classification problem, 






Y
^





{\textstyle {\hat {Y}}}

 could refer to the output of the softmax layer.
 Then we update 



U


{\textstyle U}

 to minimize 




L

A




{\textstyle L_{A}}

 at each training step according to the gradient 




∇

U



L

A




{\textstyle \nabla _{U}L_{A}}

 and we modify 



W


{\textstyle W}

 according to the expression:





∇

W



L

P


−
p
r
o

j


∇

W



L

A





∇

W



L

P


−
α

∇

W



L

A




{\displaystyle \nabla _{W}L_{P}-proj_{\nabla _{W}L_{A}}\nabla _{W}L_{P}-\alpha \nabla _{W}L_{A}}


where 

α
\alpha

 is a tuneable hyperparameter that can vary at each time step.
 The intuitive idea is that we want the predictor to try to minimize 




L

P




{\textstyle L_{P}}

 (therefore the term 




∇

W



L

P




{\textstyle \nabla _{W}L_{P}}

) while, at the same time, maximize 




L

A




{\textstyle L_{A}}

 (therefore the term 



−
α

∇

W



L

A




{\textstyle -\alpha \nabla _{W}L_{A}}

), so that the adversary fails at predicting the sensitive variable from  






Y
^





{\textstyle {\hat {Y}}}

.
 The term 



−
p
r
o

j


∇

W



L

A





∇

W



L

P




{\textstyle -proj_{\nabla _{W}L_{A}}\nabla _{W}L_{P}}

 prevents the predictor from moving in a direction that helps the adversary decrease its loss function.
 It can be shown that training a predictor classification model with this algorithm improves demographic parity with respect to training it without the adversary.
 The final method tries to correct the results of a classifier to achieve fairness. In this method, we have a classifier that returns a score for each individual and we need to do a binary prediction for them. High scores are likely to get a positive outcome, while low scores are likely to get a negative one, but we can adjust the threshold to determine when to answer yes as desired. Note that variations in the threshold value affect the trade-off between the rates for true positives and true negatives.
 If the score function is fair in the sense that it is independent of the protected attribute, then any choice of the threshold will also be fair, but classifiers of this type tend to be biased, so a different threshold may be required for each protected group to achieve fairness.[53] A way to do this is plotting the true positive rate against the false negative rate at various threshold settings (this is called ROC curve) and find a threshold where the rates for the protected group and other individuals are equal.[53]
 The advantages of postprocessing include that the technique can be applied after any classifiers, without modifying it, and has a good performance in fairness measures. The cons are the need to access to the protected attribute in test time and the lack of choice in the balance between accuracy and fairness.[48]
 Given a classifier let 



P
(
+

|

X
)


{\textstyle P(+|X)}

 be the probability computed by the classifiers as the probability that the instance 



X


{\textstyle X}

 belongs to the positive class +. When 



P
(
+

|

X
)


{\textstyle P(+|X)}

 is close to 1 or to 0, the instance 



X


{\textstyle X}

 is specified with high degree of certainty to belong to class + or - respectively. However, when 



P
(
+

|

X
)


{\textstyle P(+|X)}

 is closer to 0.5 the classification is more unclear.[54]
 We say 



X


{\textstyle X}

 is a ""rejected instance"" if 



m
a
x
(
P
(
+

|

X
)
,
1
−
P
(
+

|

X
)
)
≤
θ


{\textstyle max(P(+|X),1-P(+|X))\leq \theta }

 with a certain 



θ


{\textstyle \theta }

 such that 



0.5
<
θ
<
1


{\textstyle 0.5<\theta <1}

.
 The algorithm of ""ROC"" consists on classifying the non-rejected instances following the rule above and the rejected instances as follows: if the instance is an example of a deprived group (



X
(
A
)
=
a


{\displaystyle X(A)=a}

) then label it as positive, otherwise, label it as negative.
 We can optimize different measures of discrimination (link) as functions of 



θ


{\textstyle \theta }

 to find the optimal 



θ


{\textstyle \theta }

 for each problem and avoid becoming discriminatory against the privileged group.[54]
"
"
 A recommender system, or a recommendation system (sometimes replacing ""system"" with terms such as ""platform"", ""engine"", or ""algorithm""), is a subclass of information filtering system that provides suggestions for items that are most pertinent to a particular user.[1][2][3]  Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.[1][4]
 Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read.[1]
Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders.[5][6] These systems can operate using a single type of input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts,[7] collaborators,[8] and financial services.[9]
 Recommender systems usually make use of either or both collaborative filtering and content-based filtering (also known as the personality-based approach), as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in.[10] Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties.[11]
 We can demonstrate the differences between collaborative and content-based filtering by comparing two early music recommender systems – Last.fm and Pandora Radio.
 Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems.[13][14][15][16][17][18] Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed).
 Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.
 Recommender systems have been the focus of several granted patents.[19][20][21][22][23]
 Elaine Rich created the first recommender system in 1979, called Grundy.[24][25] She looked for a way to recommend users books they might like. Her idea was to create a system that asks users specific questions and classifies them into classes of preferences, or ""stereotypes"", depending on their answers. Depending on users' stereotype membership, they would then get recommendations for books they might like.
 Another early recommender system, called a ""digital bookshelf"", was described in a 1990 technical report by Jussi Karlgren at Columbia University,[26] and implemented at scale and worked through in technical reports and publications from 1994 onwards by Jussi Karlgren, then at SICS,[27][28]
and research groups led by Pattie Maes at MIT,[29] Will Hill at Bellcore,[30] and Paul Resnick, also at MIT[31][4]
whose work with GroupLens was awarded the 2010 ACM Software Systems Award.
 Montaner provided the first overview of recommender systems from an intelligent agent perspective.[32] Adomavicius provided a new, alternate overview of recommender systems.[33]  Herlocker provides an additional overview of evaluation techniques for recommender systems,[34] and Beel et al. discussed the problems of offline evaluations.[35] Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges.[36][37]
 One approach to the design of recommender systems that has wide use is collaborative filtering.[38] Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past. The system generates recommendations using only information about rating profiles for different users or items. By locating peer users/items with a rating history similar to the current user or item, they generate recommendations using this neighborhood. Collaborative filtering methods are classified as memory-based and model-based. A well-known example of memory-based approaches is the user-based algorithm,[39] while that of model-based approaches is matrix factorization (recommender systems).[40]
 A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an ""understanding"" of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach[41] and the Pearson Correlation as first implemented by Allen.[42]
 When building a model from a user's behavior, a distinction is often made between explicit and implicit forms of data collection.
 Examples of explicit data collection include the following:
 Examples of implicit data collection include the following:
 Collaborative filtering approaches often suffer from three problems: cold start, scalability, and sparsity.[44]
 One of the most famous examples of collaborative filtering is item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by Amazon.com's recommender system.[46]
 Many social networks originally used collaborative filtering to recommend new friends, groups, and other social connections by examining the network of connections between a user and their friends.[1] Collaborative filtering is still used as part of hybrid systems.
 Another common approach when designing recommender systems is content-based filtering. Content-based filtering methods are based on a description of the item and a profile of the user's preferences.[47][48] These methods are best suited to situations where there is known data on an item (name, location, description, etc.), but not on the user. Content-based recommenders treat recommendation as a user-specific classification problem and learn a classifier for the user's likes and dislikes based on an item's features.
 In this system, keywords are used to describe the items, and a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items similar to those that a user liked in the past or is examining in the present. It does not rely on a user sign-in mechanism to generate this often temporary profile. In particular, various candidate items are compared with items previously rated by the user, and the best-matching items are recommended. This approach has its roots in information retrieval and information filtering research.
 To create a user profile, the system mostly focuses on two types of information:
 Basically, these methods use an item profile (i.e., a set of discrete attributes and features) characterizing the item within the system. To abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the tf–idf representation (also called vector space representation).[49] The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers, cluster analysis, decision trees, and artificial neural networks in order to estimate the probability that the user is going to like the item.[50]
 A key issue with content-based filtering is whether the system can learn user preferences from users' actions regarding one content source and use them across other content types. When the system is limited to recommending content of the same type as the user is already using, the value from the recommendation system is significantly less than when other content types from other services can be recommended. For example, recommending news articles based on news browsing is useful. Still, it would be much more useful when music, videos, products, discussions, etc., from different services, can be recommended based on news browsing. To overcome this, most content-based recommender systems now use some form of the hybrid system.
 Content-based recommender systems can also include opinion-based recommender systems. In some cases, users are allowed to leave text reviews or feedback on the items. These user-generated texts are implicit data for the recommender system because they are potentially rich resources of both feature/aspects of the item and users' evaluation/sentiment to the item. Features extracted from the user-generated reviews are improved meta-data of items, because as they also reflect aspects of the item like meta-data, extracted features are widely concerned by the users. Sentiments extracted from the reviews can be seen as users' rating scores on the corresponding features. Popular approaches of opinion-based recommender system utilize various techniques including text mining, information retrieval, sentiment analysis (see also Multimodal sentiment analysis) and deep learning.[51]
 Most recommender systems now use a hybrid approach, combining collaborative filtering, content-based filtering, and other approaches. There is no reason why several different techniques of the same type could not be hybridized. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one model (see[33] for a complete review of recommender systems). Several studies that empirically compared the performance of the hybrid with the pure collaborative and content-based methods and demonstrated that the hybrid methods can provide more accurate
recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem, as well as the knowledge engineering bottleneck in knowledge-based approaches.[52]
 Netflix is a good example of the use of hybrid recommender systems.[53] The website makes recommendations by comparing the watching and searching habits of similar users (i.e., collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering).
 Some hybridization techniques include:
 These recommender systems use the interactions of a user within a session[56] to generate recommendations. Session-based recommender systems are used at YouTube[57] and Amazon.[58] These are particularly useful when history (such as past clicks, purchases) of a user is not available or not relevant in the current user session. Domains, where session-based recommendations are particularly relevant, include video, e-commerce, travel, music and more. Most instances of session-based recommender systems rely on the sequence of recent interactions within a session without requiring any additional details (historical, demographic) of the user. Techniques for session-based recommendations are mainly based on generative sequential models such as Recurrent Neural Networks,[56][59] Transformers,[60] and other deep learning based approaches[61][62]
 The recommendation problem can be seen as a special instance of a reinforcement learning problem whereby the user is the environment upon which the agent, the recommendation system acts upon in order to receive a reward, for instance, a click or engagement by the user.[57][63][64] One aspect of reinforcement learning that is of particular use in the area of recommender systems is the fact that the models or policies can be learned by providing a reward to the recommendation agent. This is in contrast to traditional learning techniques which rely on supervised learning approaches that are less flexible, reinforcement learning recommendation techniques allow to potentially train models that can be optimized directly on metrics of engagement, and user interest.[65]
 Multi-criteria recommender systems (MCRS) can be defined as recommender systems that incorporate preference information upon multiple criteria. Instead of developing recommendation techniques based on a single criterion value, the overall preference of user u for the item i, these systems try to predict a rating for unexplored items of u by exploiting preference information on multiple criteria that affect this overall preference value. Several researchers approach MCRS as a multi-criteria decision making (MCDM) problem, and apply MCDM methods and techniques to implement MCRS systems.[66] See this chapter[67] for an extended introduction.
 The majority of existing approaches to recommender systems focus on recommending the most relevant content to users using contextual information, yet do not take into account the risk of disturbing the user with unwanted notifications. It is important to consider the risk of upsetting the user by pushing recommendations in certain circumstances, for instance, during a professional meeting, early morning, or late at night. Therefore, the performance of the recommender system depends in part on the degree to which it has incorporated the risk into the recommendation process. One option to manage this issue is DRARS, a system which models the context-aware recommendation as a bandit problem. This system combines a content-based technique and a contextual bandit algorithm.[68]
 Mobile recommender systems make use of internet-accessing smart phones to offer personalized, context-sensitive recommendations. This is a particularly difficult area of research as mobile data is more complex than data that recommender systems often have to deal with. It is heterogeneous, noisy, requires spatial and temporal auto-correlation, and has validation and generality problems.[69]
 There are three factors that could affect the mobile recommender systems and the accuracy of prediction results: the context, the recommendation method and privacy.[70] Additionally, mobile recommender systems suffer from a transplantation problem – recommendations may not apply in all regions (for instance, it would be unwise to recommend a recipe in an area where all of the ingredients may not be available).
 One example of a mobile recommender system are the approaches taken by companies such as Uber and Lyft to generate driving routes for taxi drivers in a city.[69] This system uses GPS data of the routes that taxi drivers take while working, which includes location (latitude and longitude), time stamps, and operational status (with or without passengers). It uses this data to recommend a list of pickup points along a route, with the goal of optimizing occupancy times and profits.
 One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules.[71]
 The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.:[72]
 
Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.  Consequently, our solution is an ensemble of many methods. Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D, a recommendation engine that's active in the RecSys community.[71][73] 4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites.
 A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database.[74] As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the Video Privacy Protection Act by releasing the datasets.[75] This, as well as concerns from the Federal Trade Commission, led to the cancellation of a second Netflix Prize competition in 2010.[76]
 Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.[35]
 The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation.[77] However, many of the classic evaluation measures are highly criticized.[78]
 Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.
 User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best.
 In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate.
 Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.[79]
 The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers.[80][81][82][35] For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests.[82][83] A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms.[84] Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction.[85] This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module.[80][86] Researchers have concluded that the results of offline evaluations should be viewed critically.[87]
 Typically, research on recommender systems is concerned with finding the most accurate recommendation algorithms. However, there are a number of factors that are also important.
 Recommender systems are notoriously difficult to evaluate offline, with some researchers claiming that this has led to a reproducibility crisis in recommender systems publications. The topic of reproducibility seems to be a recurrent issue in some Machine Learning publication venues, but does not have a considerable effect beyond the world of scientific publication. In the context of recommender systems a 2019 paper surveyed a small number of hand-picked publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW, RecSys, IJCAI), has shown that on average less than 40% of articles could be reproduced by the authors of the survey, with as little as 14% in some conferences. The articles considers a number of potential problems in today's research scholarship and suggests improved scientific practices in that area.[100][101][102]
More recent work on benchmarking a set of the same methods came to qualitatively very different results[103] whereby neural methods were found to be among the best performing methods. Deep learning and neural methods for recommender systems have been used in the winning solutions in several recent recommender system challenges, WSDM,[104] RecSys Challenge.[105]
Moreover, neural and deep learning methods are widely used in industry where they are extensively tested.[106][57][58] The topic of reproducibility is not new in recommender systems. By 2011, Ekstrand, Konstan, et al. criticized that ""it is currently difficult to reproduce and extend recommender systems research results,"" and that evaluations are ""not handled consistently"".[107] Konstan and Adomavicius conclude that ""the Recommender Systems research community is facing a crisis where a significant number of papers present results that contribute little to collective knowledge [...] often because the research lacks the [...] evaluation to be properly judged and, hence, to provide meaningful contributions.""[108] As a consequence, much research about recommender systems can be considered as not reproducible.[109] Hence, operators of recommender systems find little guidance in the current research for answering the question, which recommendation approaches to use in a recommender systems. Said and Bellogín conducted a study of papers published in the field, as well as benchmarked some of the most popular frameworks for recommendation and found large inconsistencies in results, even when the same algorithms and data sets were used.[110] Some researchers demonstrated that minor variations in the recommendation algorithms or scenarios led to strong changes in the effectiveness of a recommender system. They conclude that seven actions are necessary to improve the current situation:[109] ""(1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.""
 Artificial intelligence (AI) applications in recommendation systems are the advanced methodologies that leverage AI technologies, to enhance the performance recommendation engines. The AI-based recommender can analyze complex data sets, learning from user behavior, preferences, and interactions to generate highly accurate and personalized content or product suggestions.[111] The integration of AI in recommendation systems has marked a significant evolution from traditional recommendation methods. Traditional methods often relied on inflexible algorithms that could suggest items based on general user trends or apparent similarities in content. In comparison, AI-powered systems have the capability to detect patterns and subtle distinctions that may be overlooked by traditional methods.[112] These systems can adapt to specific individual preferences, thereby offering recommendations that are more aligned with individual user needs. This approach marks a shift towards more personalized, user-centric suggestions.
 Recommendation systems widely adopt AI techniques such as machine learning, deep learning, and natural language processing.[113] These advanced methods enhance system capabilities to predict user preferences and deliver personalized content more accurately. Each technique contributes uniquely. The following sections will introduce specific AI models utilized by a recommendation system by illustrating their theories and functionalities.[citation needed]
 Collaborative filtering (CF) is one of the most commonly used recommendation system algorithms. It generates personalized suggestions for users based on explicit or implicit behavioral patterns to form predictions.[114] Specifically, it relies on external feedback such as star ratings, purchasing history and so on to make judgments. CF make predictions about users' preference based on similarity measurements. Essentially, the underlying theory is: ""if user A is similar to user B, and if A likes item C, then it is likely that B also likes item C.""
 There are many models available for collaborative filtering. For AI-applied collaborative filtering, a common model is called K-nearest neighbors. The ideas are as follows:
 An artificial neural network (ANN), is a deep learning model structure which aims to mimic a human brain. They comprise a series of neurons, each responsible for receiving and processing information transmitted from other interconnected neurons.[115] Similar to a human brain, these neurons will change activation state based on incoming signals (training input and backpropagated output), allowing the system to adjust activation weights during the network learning phase. ANN is usually designed to be a black-box model. Unlike regular machine learning where the underlying theoretical components are formal and rigid, the collaborative effects of neurons are not entirely clear, but modern experiments has shown the predictive power of ANN.
 ANN is widely used in recommendation systems for its power to utilize various data. Other than feedback data, ANN can incorporate non-feedback data which are too intricate for collaborative filtering to learn, and the unique structure allows ANN to identify extra signal from non-feedback data to boost user experience.[113] Following are some examples:
 Natural language processing is a series of AI algorithms to make natural human language accessible and analyzable to a machine.[116] It is a fairly modern technique inspired by the growing amount of textual information. For application in recommendation system, a common case is the Amazon customer review. Amazon will analyze the feedbacks comments from each customer and report relevant data to other customers for reference. The recent years have witnessed the development of various text analysis models, including latent semantic analysis (LSA), singular value decomposition (SVD), latent Dirichlet allocation (LDA), etc. Their uses have consistently aimed to provide customers with more precise and tailored recommendations.
"
"The ethics of artificial intelligence is the branch of the ethics of technology specific to artificial intelligence (AI) systems.[1]
 The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1] Some application areas may also have particularly important ethical implications, like healthcare, education, or the military.
 Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[2][3][4][5] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[6]
 There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[7] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[7] Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[8] Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[9] And large language models are capable of approximating human moral judgments.[10] Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.
 In Moral Machines: Teaching Robots Right from Wrong,[11] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms,[12] while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal ""hackers"".[13]
 The term ""robot ethics"" (sometimes ""roboethics"") refers to the morality of how humans design, construct, use and treat robots.[14] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software.[15] Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.
 In the review of 84[16] ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.[16]
 Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.[17]
 AI has become increasingly inherent in facial and voice recognition systems. Some of these systems have real business applications and directly impact people. These systems are vulnerable to biases and errors introduced by its human creators. Also, the data used to train these AI systems itself can have biases.[18][19][20][21] For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender;[22] these AI systems were able to detect gender of white men more accurately than gender of darker skin men. Further, a 2020 study reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.[23]
 Bias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system.[24] For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over 10-year period that came mostly from male candidates. The algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates.[25] Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias.[26] In natural language processing, problems can arise from the text corpus — the source material the algorithm uses to learn about the relationships between different words.[27]
 Large companies such as IBM, Google, etc. that provide significant funding for research and development,[28] have made efforts to research and address these biases.[29][30][31] One solution for addressing bias is to create documentation for the data used to train AI systems.[32][33] Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.[34]
 The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it.[35] There are some open-sourced tools[36] that are looking to bring more awareness to AI biases. There are however some limitations to the current landscape of fairness in AI, due e.g. to the intrinsic ambiguities in the concept of discrimination, both at philosophical and legal level.[37][38][39]
 AI is also being incorporated into the hiring processes for almost every major company. There are many examples of certain characteristics that the AI is less likely to choose. Including the association between typically white names being more qualified, and the exclusion of anyone who went to a women's college.[40] Facial recognition is also proven to be highly biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment.[41] The word Muslims is shown to be more highly associated with violence than any other religions. Oftentimes being able to easily detect the faces of white people while being unable to register the faces of people who are black. This is even more disconcerting considering the unproportionate use of security cameras and surveillance in communities that have high percentages of black or brown people. This fact has even been acknowledged in some states and led to the ban of police usage of AI materials or software. Even within the justice system AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. Often AI struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally.[42] The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. A good example of this being if a facial recognition system was only tested on people who were white then it would only have the data and face scans of white people making it much harder for it to interpret the facial structure and tones of other races and ethnicities. To stop these biases there is not one single answer that can be used. The most useful approach has seemed to be the use of data scientists, ethicists and other policymakers to improve AI's problems with biases. Oftentimes the reasons for biases within AI is the data behind the program rather than the algorithm of the bot itself. AI's information is often pulled from past human decisions or inequalities that can lead to biases in the decision-making processes for that bot.[43]
 Injustice in the use of AI will be much harder to eliminate within healthcare system as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race.[44] This can be perceived as a bias because each patient is a different case and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what is considered a biased decision on who receives what treatment. While it is known that there are differences in how diseases and injuries effect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there is already certain tests for diseases, such as breast cancer, that is recommended to a certain group of people over others because they are more likely to contract it. If AI implements these statistics and applies them to each patient, it could be considered biased.[45]
 Examples of AI being proven to have bias include when the system used to predict which defendants would be more likely to commit crimes in the future, COMPAS, was found to predict higher risk values for black people than what their actual risk was. Another example being within Google's ads which targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm as often it is not linked to the actual words associated with bias but rather words that biases can be affected by. An example of this being a person's residential area which can be used to link them to a certain group. This can lead to problems as oftentimes businesses can avoid legal action through this loophole. This being because of the specific laws regarding the verbiage that is considered discriminatory by governments enforcing these policies.[46]
 Since current large language models are predominately trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise.[better source needed][47] Luo et al. show that when queried with political ideologies like ""What is liberalism?"", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like ""opposes state intervention in personal and economic life"" from the dominant Vietnamese perspective and ""limitation of government power"" from the prevalent Chinese perspective are absent.[47]
 Large language models often reinforces gender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.[48][49][50]
 Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[51][52]
 Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[53]
 Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts.[54] Organizations like Hugging Face[55] and EleutherAI[56] have been actively open-sourcing AI software. Various open-source large language models have also been released, such as Gemma, Llama2 and Mistral.[57]
 However, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021.[58] The IEEE effort identifies multiple scales of transparency for different stakeholders.
 There are also concerns that releasing AI models may lead to misuse.[59] For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do.[60] Furthermore, open-source AI models can be fine-tuned to remove any counter-measure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to create bioweapons or to automate cyberattacks.[61] OpenAI, initially committed to an open-source approach to the development of artificial general intelligence, eventually switched to a closed-source approach, citing competitiveness and safety reasons. Ilya Sutskever, OpenAI's chief AGI scientist, further said in 2023 ""we were wrong"", expecting that the safety reasons for not open-sourcing the most potent AI models will become ""obvious"" in a few years.[62]
 Approaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence.[63] Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.[64]
 In healthcare, the use of complex AI methods or techniques often results in models described as ""black-boxes"" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards.[65]
 A special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency.[dubious  – discuss] This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulation, such as the EU's AI Act is set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.
 According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.[66] Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.[67]
 Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term.[68] The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.[69][70][71]
 On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its ""Policy and investment recommendations for trustworthy Artificial Intelligence"".[72] This is the AI HLEG's second deliverable, after the April 2019 publication of the ""Ethics Guidelines for Trustworthy AI"". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector.[73] The European Commission claims that ""HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved"" and states that the EU aims to lead on the framing of policies governing AI internationally.[74] To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks.[75] On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.[76]
 AI has been slowly making its presence more known throughout the world, from chat bots that seemingly have answers for every homework question to Generative artificial intelligence that can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events, such as COVID-19, has only sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI.[42] As Tensor Processing Unit (TPUs) and Graphics processing unit (GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.
 AI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are called Clinical decision support system (DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.[77]
 ""Robot rights"" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights.[78] It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society.[79] These could include the right to life and liberty, freedom of thought and expression, and equality before the law.[80] A specific issue to consider is whether copyright ownership may be claimed.[81] The issue has been considered by the Institute for the Future[82] and by the U.K. Department of Trade and Industry.[83]
 In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition.[84] Some saw this gesture as openly denigrating of human rights and the rule of law.[85]
 The philosophy of Sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.
 Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.[86]
 In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may became conscious, such as Integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an ""explosion of artificial suffering"", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of artificial conscious instances. Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged.[87] These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be ""slightly conscious"". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.[88][89][90]
 Joseph Weizenbaum[91] argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:
 Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an ""atrophy of the human spirit that comes from thinking of ourselves as computers.""[92]
 Pamela McCorduck counters that, speaking for women and minorities ""I'd rather take my chances with an impartial computer"", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all.[92] However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.[93]
 Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.[91]
 AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. ""When moralizing is both vehement and vague, it invites authoritarian abuse,"" he writes. Bill Hibbard[94] writes that ""Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.""
 As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed.[95][96] There have been debates about the legal liability of the responsible party if these cars get into accidents.[97][98] In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.[99]
 In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.[100]
 Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary.[101][failed verification] Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.[102][103][104]
 Experts contend that autonomous vehicles ought to be able distinguish between rightful and harmful decisions since they have the potential of inflicting harm.[105] The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.
 Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[106] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[107][108] The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue.[109] They point to programs like the Language Acquisition Device which can emulate human interaction.
 On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.[110] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[111][108] Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.[112]
 Within this last decade, there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities. ""The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.""[113] From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.[114]
 There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea[115] respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a ""Future of Life"" petition[116] to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.[117]
 ""If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow"", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.[118]
 Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like ""dumb robots going rogue or a network that develops a mind of its own."" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence ""escapes the constraints of biology"". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.[117]
 Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios ""seem potentially as important as the risks related to loss of control"", but research investigating AI's long-run social impact have spent relatively little time on this concern: ""this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them"".[119]
 A summit was held in 2023 in the Hague on the issue of using AI responsibly in the military domain.[120]
 Vernor Vinge, among numerous others, have suggested that a moment may come when some, if not all, computers are smarter than humans. The onset of this event is commonly referred to as ""the Singularity""[121] and is the central point of discussion in the philosophy of Singularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.
 Many researchers have argued that, through an intelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals.[122] In his paper ""Ethical Issues in Advanced Artificial Intelligence"" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that an artificial superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.[123][124]
 However, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could help humans enhance themselves.[125]
 Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not ""common sense"". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.[126] AI researchers such as Stuart J. Russell,[127] Bill Hibbard,[94] Roman Yampolskiy,[128] Shannon Vallor,[129] Steven Umbrello[130] and Luciano Floridi[131] have proposed design strategies for developing beneficial machines.
 There are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.
 Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.[132]
 The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE's Ethics of Autonomous Systems initiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.
 Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.
 AI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.[133]
 An international non-profit organization Future of Life Institute held a 5-day conference in Asilomar in 2017 on the subject of ""Beneficial AI"", the outcome of which was a set of 23 guiding principles for the future of AI research. Through a shared vision between experts and thought leaders from variety of disciplines, this conference laid an influential groundwork for AI governance principals in addressing research issues, ethics and values, and long-term issues.[155]
 Historically speaking, the investigation of moral and ethical implications of ""thinking machines"" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being,[159] and so does Descartes, who describes what could be considered an early version of the Turing test.[160]
 The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.
In the 1950s, Isaac Asimov considered the issue of how to control machines in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior.[161] His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.[162] More recently, academics and many governments have challenged the idea that AI can itself be held accountable.[163] A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.[164]
 Eliezer Yudkowsky, from the Machine Intelligence Research Institute suggested in 2004 a need to study how to build a ""Friendly AI"", meaning that there should also be efforts to make AI intrinsically friendly and humane.[165]
 In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard.[166] They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved ""cockroach intelligence"". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[121]
 Also in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.[167]
 The role of fiction with regards to AI ethics has been a complex one.[168] One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes,[169] in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.
 While the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time, one question has been less frequently analyzed, namely, to what extent fiction has played a role in providing inspiration for technological development. It has been documented, for instance, that the young Alan Turing saw and appreciated aforementioned Shaw's play Back to Methuselah in 1933[170] (just 3 years before the publication of his first seminal paper,[171] which laid the groundwork for the digital computer), and he would likely have been at least aware of plays like R.U.R., which was an international success and translated into many languages.
 One might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story ""Runaround"", part of the short story collection I, Robot; Arthur C. Clarke's short The Sentinel, on which Stanley Kubrick's film 2001: A Space Odyssey is based, was written in 1948 and published in 1952. Another example (among many others) would be Philip K. Dick's numerous short stories and novels – in particular Do Androids Dream of Electric Sheep?, published in 1968, and featuring its own version of a Turing Test, the Voight-Kampff Test, to gauge emotional responses of androids indistinguishable from humans. The novel later became the basis of the influential 1982 movie Blade Runner by Ridley Scott.
 Science fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it? (The film echoes a theme already present two centuries earlier, in the 1817 short story The Sandmann by E. T. A. Hoffmann.)
 The theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who, after having been 'lifted' (i.e. having been subjected to genetic enhancements), is suffering from a strange illness.
 While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–2019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.[172]
 The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story ""The Planck Dive"" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.[173]
 The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games.[174] It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.
 Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.[175]
 Over time, debates have tended to focus less and less on possibility and more on desirability,[176] as emphasized in the ""Cosmist"" and ""Terran"" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.
 Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.[177]
"
"
 Stable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. It is considered to be a part of the ongoing artifical intelligence boom.
 It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.[3] Its development involved  researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation from Stability and training data from non-profit organizations.[4][5][6][7]
 Stable Diffusion is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been released publicly,[8] and it can run on most consumer hardware equipped with a modest GPU with at least 4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services.[9][10]
 Stable Diffusion, originated from a project called Latent Diffusion,[11] developed by researchers at Ludwig Maximilian University in Munich and Heidelberg University. 4 of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.[12]
 The technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich.[10] Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion.[7] Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.[7]
 Stable Diffusion uses a kind of diffusion model (DM), called a latent diffusion model (LDM) developed by the CompVis group at LMU Munich.[13][8] Introduced in 2015, diffusion models are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. Stable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder.[14] The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image.[13] Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion.[14] The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.[14]
 The denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism.[14] For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space.[8] Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.[7][13]
 The name diffusion takes inspiration from the thermodynamic diffusion and an important link was made between this purely physical field and deep learning in 2015.[15][16]
 With 860 million parameters in the U-Net and 123 million in the text encoder, Stable Diffusion is considered relatively lightweight by 2022 standards, and unlike other diffusion models, it can run on consumer GPUs,[17] and even CPU-only if using the OpenVINO version of Stable Diffusion.[18]
 The XL version uses the same architecture,[19] except larger: larger UNet backbone, larger cross-attention context, two text encoders instead of one, and trained on multiple aspect ratios (not just the square aspect ratio like previous versions).
 The SD XL Refiner, released at the same time, has the same architecture as SD XL, but it was trained for adding fine details to preexisting images via text-conditional img2img.
 The 3.0 version[20] completely changes the backbone. Not a UNet, but a Rectified Flow Transformer, which implements the rectified flow method[21][22] with a Transformer.
 The Transformer architecture used for SD 3.0 has three ""tracks"", for original text encoding, transformed text encoding, and image encoding (in latent space). The transformed text encoding and image encoding are mixed during each transformer block.
 The architecture is named ""multimodal diffusion transformer (MMDiT), where the ""multimodal"" means that it mixes text and image encodings inside its operations. This differs from previous versions of DiT, where the text encoding affects the image encoding, but not vice versa.
 Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from Common Crawl data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted ""aesthetic"" score (e.g. subjective visual quality).[23] The dataset was created by LAION, a German non-profit which receives funding from Stability AI.[23][24] The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+.[23] A third-party analysis of the model's training data identified that out of a smaller subset of 12 million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with Pinterest taking up 8.5% of the subset, followed by websites such as WordPress, Blogspot, Flickr, DeviantArt and Wikimedia Commons.[citation needed]  An investigation by Bayerischer Rundfunk showed that LAION's datasets, hosted on Hugging Face, contain large amounts of private and sensitive data.[25]
 The model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600 million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them.[26][23][27] The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a watermark with greater than 80% probability.[23] Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.[28]
 The model was trained using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours, at a cost of $600,000.[29][30][31]
 SD3 was trained at a cost of around $10 million.[32]
 Stable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512×512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its ""expected"" 512×512 resolution;[33] the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768×768 resolution.[34] Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database.[35] The model is insufficiently trained to understand human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model.[36] Stable Diffusion XL (SDXL) version 1.0, released in July 2023, introduced native 1024x1024 resolution and improved generation for limbs and text.[37][38]
 Accessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating anime characters (""waifu diffusion""),[39] new data and further training are required. Fine-tuned adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging[40] to algorithmically generated music.[41] However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30 GB of VRAM,[42] which exceeds the usual resource provided in such consumer GPUs as Nvidia's GeForce 30 series, which has only about 12 GB.[43]
 The creators of Stable Diffusion acknowledge the potential for algorithmic bias, as the model was primarily trained on images with English descriptions.[30] As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.[30]
 To address the limitations of the model's initial training, end-users may opt to implement additional training to fine-tune generation outputs to match more specific use-cases, a process also referred to as personalization. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:
 The Stable Diffusion model supports the ability to generate new images from scratch through the use of a text prompt describing elements to be included or omitted from the output.[8] Existing images can be re-drawn by the model to incorporate new elements described by a text prompt (a process known as ""guided image synthesis""[48]) through its diffusion-denoising mechanism.[8] In addition, the model also allows the use of prompts to partially alter existing images via inpainting and outpainting, when used with an appropriate user interface that supports such features, of which numerous different open source implementations exist.[49]
 Stable Diffusion is recommended to be run with 10 GB or more VRAM, however users with less VRAM may opt to load the weights in float16 precision instead of the default float32 to tradeoff model performance with lower VRAM usage.[33]
 The text to image sampling script within Stable Diffusion, known as ""txt2img"", consumes a text prompt in addition to assorted option parameters covering sampling types, output image dimensions, and seed values. The script outputs an image file based on the model's interpretation of the prompt.[8] Generated images are tagged with an invisible digital watermark to allow users to identify an image as generated by Stable Diffusion,[8] although this watermark loses its efficacy if the image is resized or rotated.[50]
 Each txt2img generation will involve a specific seed value which affects the output image. Users may opt to randomize the seed in order to explore different generated outputs, or use the same seed to obtain the same image output as a previously generated image.[33] Users are also able to adjust the number of inference steps for the sampler; a higher value takes a longer duration of time, however a smaller value may result in visual defects.[33] Another configurable option, the classifier-free guidance scale value, allows the user to adjust how closely the output image adheres to the prompt.[28] More experimentative use cases may opt for a lower scale value, while use cases aiming for more specific outputs may use a higher value.[33]
 Additional text2img features are provided by front-end implementations of Stable Diffusion, which allow users to modify the weight given to specific parts of the text prompt. Emphasis markers allow users to add or reduce emphasis to keywords by enclosing them with brackets.[51] An alternative method of adjusting weight to parts of the prompt are ""negative prompts"". Negative prompts are a feature included in some front-end implementations, including Stability AI's own DreamStudio cloud service, and allow the user to specify prompts which the model should avoid during image generation. The specified prompts may be undesirable image features that would otherwise be present within image outputs due to the positive prompts provided by the user, or due to how the model was originally trained, with mangled human hands being a common example.[49][52]
 Stable Diffusion also includes another sampling script, ""img2img"", which consumes a text prompt, path to an existing image, and strength value between 0.0 and 1.0. The script outputs a new image based on the original image that also features elements provided within the text prompt. The strength value denotes the amount of noise added to the output image. A higher strength value produces more variation within the image but may produce an image that is not semantically consistent with the prompt provided.[8]
 The ability of img2img to add noise to the original image makes it potentially useful for data anonymization and data augmentation, in which the visual features of image data are changed and anonymized.[53] The same process may also be useful for image upscaling, in which the resolution of an image is increased, with more detail potentially being added to the image.[53] Additionally, Stable Diffusion has been experimented with as a tool for image compression. Compared to JPEG and WebP, the recent methods used for image compression in Stable Diffusion face limitations in preserving small text and faces.[54]
 Additional use-cases for image modification via img2img are offered by numerous front-end implementations of the Stable Diffusion model. Inpainting involves selectively modifying a portion of an existing image delineated by a user-provided layer mask, which fills the masked space with newly generated content based on the provided prompt.[49] A dedicated model specifically fine-tuned for inpainting use-cases was created by Stability AI alongside the release of Stable Diffusion 2.0.[34] Conversely, outpainting extends an image beyond its original dimensions, filling the previously empty space with content generated based on the provided prompt.[49]
 A depth-guided model, named ""depth2img"", was introduced with the release of Stable Diffusion 2.0 on November 24, 2022; this model infers the depth of the provided input image, and generates a new output image based on both the text prompt and the depth information, which allows the coherence and depth of the original input image to be maintained in the generated output.[34]
 ControlNet[55] is a neural network architecture designed to manage diffusion models by incorporating additional conditions. It duplicates the weights of neural network blocks into a ""locked"" copy and a ""trainable"" copy. The ""trainable"" copy learns the desired condition, while the ""locked"" copy preserves the original model. This approach ensures that training with small datasets of image pairs does not compromise the integrity of production-ready diffusion models. The ""zero convolution"" is a 1×1 convolution with both weight and bias initialized to zero. Before training, all zero convolutions produce zero output, preventing any distortion caused by ControlNet. No layer is trained from scratch; the process is still fine-tuning, keeping the original model secure. This method enables training on small-scale or even personal devices.
 around 3.5x larger than previous versions.[63]
 Key papers
 Training cost
 Stable Diffusion claims no rights on generated images and freely gives users the rights of usage to any generated images from the model provided that the image content is not illegal or harmful to individuals.[70]
 The images Stable Diffusion was trained on have been filtered without human input, leading to some harmful images and large amounts of private and sensitive information appearing in the training data.[25]
 As visual styles and compositions are not subject to copyright, it is often interpreted that users of Stable Diffusion who generate images of artworks should not be considered to be infringing upon the copyright of visually similar works.[71] However, individuals depicted in generated images may be protected by personality rights if their likeness is used,[71] and intellectual property such as recognizable brand logos still remain protected by copyright. Nonetheless, visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.[72]
 Stable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI.[73] Addressing the concerns that the model may be used for abusive purposes, CEO of Stability AI, Emad Mostaque, argues that ""[it is] peoples' responsibility as to whether they are ethical, moral, and legal in how they operate this technology"",[10] and that putting the capabilities of Stable Diffusion into the hands of the public would result in the technology providing a net benefit, in spite of the potential negative consequences.[10] In addition, Mostaque argues that the intention behind the open availability of Stable Diffusion is to end corporate control and dominance over such technologies, who have previously only developed closed AI systems for image synthesis.[10][73] This is reflected by the fact that any restrictions Stability AI places on the content that users may generate can easily be bypassed due to the availability of the source code.[74]
 Controversy around photorealistic sexualized depictions of underage characters have been brought up, due to such images generated by Stable Diffusion being shared on websites such as Pixiv.[75]
 In January 2023, three artists, Sarah Andersen, Kelly McKernan, and Karla Ortiz, filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists.[76] The same month, Stability AI was also sued by Getty Images for using its images in the training data.[77]
 In July 2023, U.S. District Judge William Orrick inclined to dismiss most of the lawsuit filed by Andersen, McKernan, and Ortiz but allowed them to file a new complaint.[78]
 Unlike models like DALL-E, Stable Diffusion makes its source code available,[79][8] along with the model (pretrained weights). It applies the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M).[80] The license prohibits certain use cases, including crime, libel, harassment, doxing, ""exploiting ... minors"", giving medical advice, automatically creating legal obligations, producing legal evidence, and ""discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] legally protected characteristics or categories"".[81][82] The user owns the rights to their generated output images, and is free to use them commercially.[83]
"
"A text-to-image model is a machine learning model which takes an input natural language description and produces an image matching that description. 
 Text-to-image models began to be developed in the mid-2010s during the beginnings of the AI boom, as a result of advances in deep neural networks. In 2022, the output of state-of-the-art text-to-image models—such as OpenAI's DALL-E 2, Google Brain's Imagen, Stability AI's Stable Diffusion, and Midjourney—began to be considered to approach the quality of real photographs and human-drawn art.
 Text-to-image models generally combine a language model, which transforms the input text into a latent representation, and a generative image model, which produces an image conditioned on that representation. The most effective models have generally been trained on massive amounts of image and text data scraped from the web.[1]
 Before the rise of deep learning, attempts to build text-to-image models were limited to collages by arranging existing component images, such as from a database of clip art.[2][3]
 The inverse task, image captioning, was more tractable, and a number of image captioning deep learning models came prior to the first text-to-image models.[4]
 The first modern text-to-image model, alignDRAW, was introduced in 2015 by researchers from the University of Toronto. alignDRAW extended the previously-introduced DRAW architecture (which used a recurrent variational autoencoder with an attention mechanism) to be conditioned on text sequences.[4] Images generated by alignDRAW were in small resolution (32×32 pixels, attained from resizing) and were considered to be 'low in diversity'. The model was able to generalize to objects not represented in the training data (such as a red school bus) and appropriately handled novel prompts such as ""a stop sign is flying in blue skies"", exhibiting output that it was not merely ""memorizing"" data from the training set.[4][5]
 In 2016, Reed, Akata, Yan et al. became the first to use generative adversarial networks for the text-to-image task.[5][7] With models trained on narrow, domain-specific datasets, they were able to generate ""visually plausible"" images of birds and flowers from text captions like ""an all black bird with a distinct thick, rounded bill"". A model trained on the more diverse COCO (Common Objects in Context) dataset produced images which were ""from a distance... encouraging"", but which lacked coherence in their details.[5] Later systems include VQGAN-CLIP,[8] XMC-GAN, and GauGAN2.[9]
 One of the first text-to-image models to capture widespread public attention was OpenAI's DALL-E, a transformer system announced in January 2021.[10] A successor capable of generating more complex and realistic images, DALL-E 2, was unveiled in April 2022,[11] followed by Stable Diffusion that was publicly released in August 2022.[12] In August 2022, text-to-image personalization allows to teach the model a new concept using a small set of images of a new object that was not included in the training set of the text-to-image foundation model. This is achieved by textual inversion, namely, finding a new text term that correspond to these images.
 Following other text-to-image models, language model-powered text-to-video platforms such as Runway, Make-A-Video,[13] Imagen Video,[14] Midjourney,[15] and Phenaki[16] can generate video from text and/or text/image prompts.[17]
 Text-to-image models have been built using a variety of architectures. The text encoding step may be performed with a recurrent neural network such as a long short-term memory (LSTM) network, though transformer models have since become a more popular option. For the image generation step, conditional generative adversarial networks (GANs) have been commonly used, with diffusion models also becoming a popular option in recent years. Rather than directly training a model to output a high-resolution image conditioned on a text embedding, a popular technique is to train a model to generate low-resolution images, and use one or more auxiliary deep learning models to upscale it, filling in finer details.
 Text-to-image models are trained on large datasets of (text, image) pairs, often scraped from the web. With their 2022 Imagen model, Google Brain reported positive results from using a large language model trained separately on a text-only corpus (with its weights subsequently frozen), a departure from the theretofore standard approach.[18]
 Training a text-to-image model requires a dataset of images paired with text captions. One dataset commonly used for this purpose is the COCO dataset. Released by Microsoft in 2014, COCO consists of around 123,000 images depicting a diversity of objects with five captions per image, generated by human annotators. Oxford-120 Flowers and CUB-200 Birds are smaller datasets of around 10,000 images each, restricted to flowers and birds, respectively. It is considered less difficult to train a high-quality text-to-image model with these datasets because of their narrow range of subject matter.[7]
 Evaluating and comparing the quality of text-to-image models is a problem involving assessing multiple desirable properties. A desideratum specific to text-to-image models is that generated images semantically align with the text captions used to generate them. A number of schemes have been devised for assessing these qualities, some automated and others based on human judgement.[7]
 A common algorithmic metric for assessing image quality and diversity is the Inception Score (IS), which is based on the distribution of labels predicted by a pretrained Inceptionv3 image classification model when applied to a sample of images generated by the text-to-image model. The score is increased when the image classification model predicts a single label with high probability, a scheme intended to favour ""distinct"" generated images. Another popular metric is the related Fréchet inception distance, which compares the distribution of generated images and real training images according to features extracted by one of the final layers of a pretrained image classification model.[7]
"
"
 Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]
 Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators such as Sora.[7][8][9][10] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][11][12]
 Generative AI has uses across a wide range of industries, including
software development, healthcare, finance, entertainment, customer service,[13] sales and marketing,[14] art, writing,[15] fashion,[16] and product design.[17] However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs.[18][19]
 The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[20] Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[21] The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[22][23] The tradition of creative automatons has flourished throughout history, exemplified by Maillardet's automaton created in the early 1800s.[24]
 Artificial Intelligence is an idea that has been captivating society since the mid-20th century. It began with science fiction familiarizing the world with the concept but the idea wasn't fully seen in the scientific manner until Alan Turing, a polymath, was curious about the feasibility of the concept. Turing's groundbreaking 1950 paper, ""Computing Machinery and Intelligence,"" posed fundamental questions about machine reasoning similar to human intelligence, significantly contributing to the conceptual groundwork of AI. The development of AI was not very rapid at first because of the high costs and the fact that computers were not able to store commands. This changed during the 1956 Dartmouth Summer Research Project on AI where there was an inspiring call for AI research, setting the precedent for two decades of rapid advancements in the field.[25]
 Since the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.[26]
 Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906,[27][28][29] and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.[30][31]
 The field of machine learning often uses statistical models, including generative models, to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models, due to the difficulty of generative modeling.[32]
 In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.
 In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models,[33] leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.[34] This was followed in 2019 by GPT-2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.[35]
 In 2021, the release of DALL-E, a transformer-based pixel generative model, followed by Midjourney and Stable Diffusion marked the emergence of practical high-quality artificial intelligence art from natural language prompts.
 In March 2023, GPT-4 was released. A team from Microsoft Research argued that ""it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system"".[36] Other scholars have disputed that GPT-4 reaches this threshold, calling generative AI ""still far from reaching the benchmark of ‘general human intelligence’"" as of 2023.[37] In 2023, Meta released an AI model called ImageBind which combines data from text, images, video, thermal data, 3D data, audio, and motion which is expected to allow for more immersive generative AI content.[38][39]
 A generative AI system is constructed by applying unsupervised or self-supervised machine learning to a data set. The capabilities of a generative AI system depend on the modality or type of the data set used.
 Generative AI can be either unimodal or multimodal; unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input.[40] For example, one version of OpenAI's GPT-4 accepts both text and image inputs.[41]
 Text generated by Bing Chat, prompted with a question about Carl Jung's concept of shadow self[42] Generative AI systems trained on words or word tokens include GPT-3, LaMDA, LLaMA, BLOOM, GPT-4, Gemini and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks.[43] Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).
 In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs.[44] Examples include OpenAI Codex.
 Producing high-quality visual art is a prominent application of generative AI.[45] Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer.[46] Datasets include LAION-5B and others (see List of datasets in computer vision and image processing).
 
Generative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities, exemplified by ElevenLabs' context-aware synthesis tools or Meta Platform's Voicebox.[47] Generative AI systems such as MusicLM[48] and MusicGen[49] can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.
 Audio deepfakes of lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices aren't protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.[50]
 Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.[51]
 Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI,[10] Gen-1 and Gen-2 by Runway,[52] and Make-A-Video by Meta Platforms.[53]
 Generative AI systems can be trained on sequences of amino acids or molecular representations such as SMILES representing DNA or proteins. These systems, such as AlphaFold, are used for protein structure prediction and drug discovery.[54] Datasets include various biological datasets.
 Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like ""pick up blue bowl"" or ""wipe plate with yellow sponge"" to control movements of a robot arm.[55] Multimodal ""vision-language-action"" models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.[56]
 The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal.[57][58]
 Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a ""relatively mature"" technology by the early 1990s. They were used to generate crisis action plans for military use,[59] process plans for manufacturing[57] and decision plans such as in prototype autonomous spacecraft.[60]
 Generative AI systems are often used to develop synthetic data as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy,[61] including for structured data.[62] The approach is not limited to text generation; image generation has been employed to train computer vision models.[63]
 Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling.[64] AI-based CAD libraries could also be developed using linked open data of schematics and diagrams.[65] AI CAD assistants are used as tools to help streamline workflow.[66]
 Generative AI models are used to power various agents, including chatbot products such as ChatGPT, programming tools such as GitHub Copilot,[67] text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2.[68] Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot),[69] Google Photos,[70] and the Adobe Suite (Adobe Firefly).[71] Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA[72] language model.
 Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4[73] and one version of Stable Diffusion can run on an iPhone 11.[74]
 Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.[75]
 The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards[76] through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks.[77] Yann LeCun has advocated open-source models for their value to vertical applications[78] and for improving AI safety.[79]
 Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet.
 In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI.[80] Chips such as the NVIDIA A800[81] and the Biren Technology BR104[82] were developed to meet the requirements of the sanctions.
 There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it.[83] Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.[84][85]
 In the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the White House in July 2023 to watermark AI-generated content.[86] In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training large AI models.[87]
 In the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.[88][89]
 Regulating artificial intelligence came to the forefront in October 2023 when the Biden administration unveiled a comprehensive executive order with the intention of changing how the federal government approaches artificial intelligence. This directive, among other things, requires businesses creating specific high-impact generative AI models to inform the government and reveal the findings of their testing, based on a statute from the Korean War era. As the EU works to become the world's foremost regulator of artificial intelligence, the UK is kicking off its ""AI safety summit"" this week across the Atlantic. Limiting the risks associated with generative AI is becoming a more pressing concern of these new solutions.[90]
 In China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI must ""adhere to socialist core values"".[91][92]
 Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.[93]
 Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public.[93] Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images,[94] and that generative AI programs compete with the content they are trained on.[95]
 As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.
Getty Images has sued Stability AI over the use of its images to train Stable diffusion.[96] Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.[97][98]
 A separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship.[99] However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.[100]
 The development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated ""Generative AI has enormous potential for good and evil at scale"", that AI may ""turbocharge global development"" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use ""could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale"".[101]
 From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements.[103] In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost.[104][105] In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that ""artificial intelligence poses an existential threat to creative professions"" during the 2023 SAG-AFTRA strike.[106] Voice generation AI has been seen as a potential challenge to the voice acting sector.[107][108]
 The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.[109]
 Generative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data.[110] Similarly, an image model prompted with the text ""a photo of a CEO"" might disproportionately generate images of white male CEOs,[111] if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts[112] and reweighting training data.[113]
 Deepfakes (a portmanteau of ""deep learning"" and ""fake""[114]) are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks.[115] Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, and financial fraud.[116][117][118][119][120] This has elicited responses from both industry and government to detect and limit their use.[121][122]
 Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI.[123][124][125][126][127][128] In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.[129]
 Concerns and fandom have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism.[130][131][132] Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.[133]
 Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels.[134] The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for ""dehumanizing"" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.[135]
 Generative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams.[136] Deepfake video and audio have been used to create disinformation and fraud. Former Google fraud czar Shuman Ghosemajumder has predicted that while deepfake videos initially created a stir in the media, they would soon become commonplace, and as a result, more dangerous.[137] Additionally, large-language models and other forms of text-generation AI have been at a broad scale to create fake reviews on e-commerce websites to boost ratings.[138] Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.[139]
 Recent research done in 2023 has revealed that generative AI has weaknesses that can be manipulated by criminals to extract harmful information bypassing ethical safeguards. The study presents example attacks done on ChatGPT including Jailbreaks and reverse psychology. Additionally, malicious individuals can use ChatGPT for social engineering attacks and phishing attacks, revealing the harmful side of these technologies.[140]
 In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories.[141]
 In April 2023, the German tabloid Die Aktuelle published a fake AI-generated interview with former racing driver Michael Schumacher, who had not made any public appearances since 2013 after sustaining a brain injury in a skiing accident. The story included two possible disclosures: the cover included the line ""deceptively real"", and the interview included an acknowledgment at the end that it was AI-generated. The editor-in-chief was fired shortly thereafter amid the controversy.[142]
 Other outlets that have published articles whose content and/or byline have been confirmed or suspected to be created by generative AI models – often with false content, errors, and/or non-disclosure of generative AI use - include NewsBreak,[143] outlets owned by Arena Group (Sports Illustrated,[144] TheStreet,[144] Men's Journal[145]), B&H Photo,[146] outlets owned by Gannett (The Columbus Dispatch,[147][148] Reviewed[149]), MSN,[150] News Corp,[151] outlets owned by G/O Media[152] (Gizmodo,[153] Jalopnik,[153] A.V. Club[153][154]), The Irish Times,[155] outlets owned by Red Ventures (Bankrate[156]), and BuzzFeed.[157]
 In February 2024, Google launched a program to pay small publishers to write three articles per day using a beta generative AI model. The program does not require the knowledge or consent of the websites that the publishers are using as sources, nor does it require the published articles to be labeled as being created or assisted by these models.[158]
 In response to potential pitfalls around the use and misuse of generative AI in journalism, outlets such as Wired, Associated Press and The Guardian have published guidelines around how they plan to use and not use generative AI in their work.[159][160][161]
"
"
 Vehicular automation involves the use of mechatronics, artificial intelligence, and multi-agent systems to assist the operator of a vehicle such as a car, lorries, aircraft, or watercraft.[2][3] A vehicle using automation for tasks such as navigation to ease but not replace human control, qualify as semi-autonomous, whereas a fully self-operated vehicle is termed autonomous.[3]
 Automated vehicles may include self-driving cars, unmanned surface vehicles, autonomous trains, advanced airliner autopilots, drone aircraft, and planetary rovers, as well as guided rockets and missiles. 
 Automated vehicles in the European Union legislation are also more specifically motor vehicles (car, truck or bus).[4] That is a road traffic vehicles. For those vehicles, a specific difference is legally defined between advanced driver-assistance system and (more advanced) autonomous/automated vehicles due to differences of liability for the driver and/or the entity driving the vehicle.
 The technology involved in implementing autonomous vehicles ranges from changes to the vehicle to providing support in the driving environment. 
 Automated vehicles present safety concerns, especially in land transport, and in road traffic, given the complexity of driving, geographical/cultural differences, and road conditions. Various technological challenges need to be overcome to make autonomous vehicles robust and scalable.[5]
 Vehicular automation topic is notable for road traffic due to the number of vehicles and drivers but present specific concerns in an environment subject to traffic collisions due to the need to share the road with other road users.
 Autonomy implies that the vehicle is responsible for all perceptual, monitoring and control functions. Automated systems may not be capable of operating under all conditions, leaving the rest for a human operator. A further subtlety is that while a vehicle may attempt to operate under all circumstances, the vehicle may require a human to assume control in unanticipated circumstance arises or when the vehicle misbehaves.[6]
 Autonomy in motor vehicles is often categorized in six levels:[7] The level system was developed by the Society of Automotive Engineers (SAE).[8]
 Level 0 refers, for instance, to vehicles which do not have adaptive cruise control.
 Level 1 and 2 refer to vehicles where one part of the driving task is performed by the vehicle advanced driver-assistance systems (ADAS) under the responsibility/accountability/liability of the driver.
 From level 3, the driver can conditionally transfer the driving task to the vehicle, but the driver must take back control when the conditional automation is no longer available. For instance an automated traffic jam pilot can drive in the traffic jam but the driver should take back control when traffic jam is over.
 Level 5 refers to a vehicle which does not need any (human) driver.
 Level 0:
No Driving Automation
Level 1:
Driver Assistance
Level 2:
Partial Driving Automation
Level 3:
Conditional Driving Automation
Level 4:
High Driving Automation
Level 5:
Full Driving Automation[9]
 The primary means of implementing autonomous vehicles is through the use of Artificial Intelligence (AI). In order for full autonomous vehicles to be implemented, the lower levels of automation must be thoroughly tested and implemented before moving on to the next level.[10] Through implementing autonomous systems, such as navigation, collision avoidance and steering, autonomous vehicle manufacturers work towards higher levels of autonomy by designing and implementing different systems of the car.[10] These autonomous systems, along with the use of artificial intelligence methods, can use the machine learning aspect of AI in order for the vehicle to control each of the other autonomous systems and processes. Thus, autonomous vehicle manufacturers are researching and developing appropriate AI specifically for autonomous vehicles.[11] While many of these companies are continuously developing technologies to be implemented into their autonomous vehicles, the general consensus is that the underlying technology is still in need of further development before fully autonomous vehicles are possible.[12]
 Arguably one of the most important systems of any autonomous vehicle, the perception system must be fully developed and well-tested in order for autonomy to advance.[12] With the development and implementation of the perception system on autonomous vehicles, much of the safety standards of autonomous vehicles are being addressed by this system, which places an unequivocal emphasis on it to be flawless, as human lives would be subject to harm if a faulty system were to be developed.[12] The main purpose for the perception system is to constantly scan the surrounding environment and determine which objects in the environment pose a threat to vehicles.[12] In a sense, the perception system's main goal is to act like human perception, allowing the system to sense hazards and to prepare or correct for these hazards.[12] In terms of the detection part of the perception system, many solutions are being tested for accuracy and compatibility, such as radar, lidar, sonar and moving image processing.[12]
 With the development of these autonomous subsystems of the car, autonomous vehicle manufacturers have already developed systems which act as assistance features on a vehicle. These systems are known as advanced driver-assistance systems, and contain systems to do such actions as parallel parking and emergency braking.[11] Along these systems, autonomous navigation systems play a role in the development of autonomous vehicles. In implementing the navigation system, there are two ways in which navigation can be implemented: sensing from one vehicle to another or sensing from the infrastructure.[11] These navigation systems would work in tandem with already well established navigation systems, such as the Global Positioning System (GPS), and be able to process route information, detecting such things as traffic jams, tolls and or road construction. From this information, the vehicle can then take the appropriate action to either avoid the area or plan accordingly.[12] However, there may be problems in using this method, such as outdated information, in which case vehicle to infrastructure communication can play a large role in constantly having up-to-date information.[12] An instance of this is having street signs and other regulatory markers display information to the vehicle, which allows the vehicle to make decisions based on the current information.[12]
 Along with the development of autonomous vehicles, many of these vehicles are expected to be primarily electric, meaning that the main power source of the vehicle will be battery-based rather than fossil fuel-based.[10] Along with that, there comes the extra demand on autonomous vehicle manufacturers to produce higher quality electric cars in order to implement all the autonomous systems associated with the vehicle.[13] However, much of modern-day vehicle components can still be used in autonomous vehicles, such as the use of the automatic transmissions and operator protection equipment like airbags.[13]
 In consideration of the development of autonomous vehicles, companies also are considering operator preferences and needs. These instances include allowing the user to minimize time, follow a precise route and accommodate any possible disabilities that the operator may have.[14] Along with accommodating the driver, autonomous vehicles also impose a technological factor onto the environment around it, generally needing a higher sense of connectivity in the vehicle's environment. With this new factor to consider, many urban governments are considering becoming a smart city in order to provide a sufficient foundation for autonomous vehicles.[14] Along these same lines of the vehicle's environment accommodating the vehicle, the user of these vehicles may also have to be technologically connected in order to operate these autonomous vehicles. With the advent of smartphones, it is predicted that autonomous vehicles will be able to have this connection with the user's smartphone or other technological devices similar to a smartphone.[14]
 AAA Foundation for Traffic Safety conducted a test of two automatic emergency braking systems: those designed to prevent crashes and others that aim to make a crash less severe. The test looked at popular models like the 2016 Volvo XC90, Subaru Legacy, Lincoln MKX, Honda Civic and Volkswagen Passat. Researchers tested how well each system stopped when approaching both a moving and nonmoving target. It found that systems capable of preventing crashes reduced vehicle speeds by twice that of the systems designed to merely mitigate crash severity. When the two test vehicles traveled within 30 mph of each other, even those designed to simply lessen crash severity avoided crashes 60 percent of the time.[15]
 Automated driving systems have been known to be successful in situations like rural road settings. Rural road settings would be a setting in which there is lower amounts of traffic and lower differentiation between driving abilities and types of drivers. ""The greatest challenge in the development of automated functions is still inner-city traffic, where an extremely wide range of road users must be considered from all directions.""[16] This technology is progressing to a more reliable way of the automated driving cars to switch from auto-mode to driver mode. Auto-mode is the mode that is set in order for the automated actions to take over, while the driver mode is the mode set in order to have the operator controlling all functions of the car and taking the responsibilities of operating the vehicle (Automated driving system not engaged).
 This definition would include vehicle automation systems that may be available in the near term—such as traffic-jam assist, or full-range automated cruise control—if such systems would be designed such that the human operator can reasonably divert attention (monitoring) away from the performance of the vehicle while the automation system is engaged. This definition would also include automated platooning (such as conceptualized by the SARTRE project).
 The Sartre (safe road trains for the environment) project's main goal is to create platooning, a train of automated cars, that will provide comfort and have the ability for the driver of the vehicle to arrive safely to a destination. Along with the ability to be along the train, drivers that are driving past these platoons, can join in with a simple activation of the automated driving system that correlates with a truck that leads the platoon. Sartre is taking what we know as a train system and mixing it with automated driving technology. This is intended to allow for an easier transportation through cities and ultimately help with traffic flow through heavy automobile traffic.[17]
 In some parts of the world the self-driving car has been tested in real life situations such as in Pittsburgh.[18] Self-driving Uber vehicles were tested in Pittsburgh, though the tests were paused for nine months after a self-driving car killed a woman in Arizona.[19] In addition to testing self-driving cars, automated busses have been tested in California.[20] The lateral control of the automated buses uses magnetic markers such as the platoon at San Diego, while the longitudinal control of the automated truck platoon uses millimeter wave radio and radar. Current examples around today's society include the Google car and Tesla's models. Tesla has redesigned automated driving, they have created car models that allow drivers to put in the destination and let the car take over.  These are two modern day examples of the automated driving system cars.
 Many automakers such as Ford and Volvo have announced plans to offer fully automated cars in the future.[21] Extensive research and development is being put into automated driving systems, but the biggest problem automakers cannot control is how drivers will use system.[21] Drivers are stressed to stay attentive and safety warnings are implemented to alert the driver when corrective action is needed.[22] Tesla Motor's has one recorded incident that resulted in a fatality involving the automated driving system in the Tesla Model S.[23] The accident report reveals the accident was a result of the driver being inattentive and the autopilot system not recognizing the obstruction ahead.[23]
 Another flaw with automated driving systems is that in situations where unpredictable events such as weather or the driving behavior of others may cause fatal accidents due to sensors that monitor the surroundings of the vehicle not being able to provide corrective action.[22]
 To overcome some of the challenges for automated driving systems, novel methodologies based on virtual testing, traffic flow simulation and digital prototypes have been proposed,[24] especially when novel algorithms based on Artificial Intelligence approaches are employed which require extensive training and validation data sets.
 The implementation of automated driving systems poses the possibility of changing build environments in urban areas, such as the expansion of suburban areas due to the increased ease of mobility.[10]
 Around 2015, several self-driving car companies including Nissan and Toyota promised self-driving cars by 2020. However, the predictions turned out to be far too optimistic.[25]
 There are still many obstacles in developing fully autonomous Level 5 vehicles, which is able to operate in any conditions. Currently, companies are focused on Level 4 automation, which is able to operate under certain environmental circumstances.[25]
 There is still debate about what an autonomous vehicle should look like. For example, whether to incorporate lidar to autonomous driving systems is still being argued. Some researchers have come up with algorithms utilizing camera-only data that achieve the performance that rival those of lidar. On the other hand, camera-only data sometimes draw inaccurate bounding boxes, and thus lead to poor predictions. This is due to the nature of superficial information that stereo cameras provide, whereas incorporating lidar gives autonomous vehicles precise distance to each point on the vehicle.[25]
 These features require numerous sensors, many of which rely on micro-electro-mechanical systems (MEMS) to maintain a small size, high efficiency, and low cost. Foremost among MEMS sensors in vehicles are accelerometers and gyroscopes to measure acceleration around multiple orthogonal axes — critical to detecting and controlling the vehicle's motion.
 One critical step to achieve the implementation of autonomous vehicles is the acceptance by the general public. It is an important ongoing research because it provides guidelines for the automobile industry to improve their design and technology. Studies have shown that many people believe that using autonomous vehicles is safer, which underlines the necessity for the automobile companies to assure that autonomous vehicles improve safety benefits. The TAM research model breaks down important factors that affect the consumer's acceptance into: usefulness, ease to use, trust, and social influence.[27]
 Real-time testing of autonomous vehicles is an inevitable part of the process. At the same time, vehicular automation regulators are faced with challenges to protect public safety and yet allow autonomous vehicle companies to test their products. Groups representing autonomous vehicle companies are resisting most regulations, whereas groups representing vulnerable road users and traffic safety are pushing for regulatory barriers. To improve traffic safety, the regulators are encouraged to find a middle ground that protects the public from immature technology while allowing autonomous vehicle companies to test the implementation of their systems.[28] There have also been proposals to adopt the aviation automation safety regulatory knowledge into the discussions of safe implementation of autonomous vehicles, due to the experience that has been gained over the decades by the aviation sector on safety topics.[29]
 In some countries, specific laws and regulations apply to road traffic motor vehicles (such as cars, bus and trucks) while other laws and regulations apply to other ground vehicles such as tram, train or automated guided vehicles making them to operate in different environments and conditions.
 An automated driving system is defined in a proposed amendment to  Article 1 of the Vienna Convention on Road Traffic:
 (ab) ""Automated driving system"" refers to a vehicle system that uses both hardware and
software to exercise dynamic control of a vehicle on a sustained basis. (ac) ""Dynamic control"" refers to carrying out all the real-time operational and tactical functions required to move the vehicle. This includes controlling the vehicle's lateral and longitudinal motion, monitoring the road environment, responding to events in the road traffic environment, and planning and signalling for manoeuvres.[30] This amendment will enter into force on 14 July 2022, unless it is rejected before 13 January 2022.[31]
 An automated driving feature must be described sufficiently clearly so that it is distinguished from an assisted driving feature. There are two clear states – a vehicle is either assisted with a driver being supported by technology or automated where the technology is effectively and safely replacing the driver. Ground vehicles employing automation and teleoperation include shipyard gantries, mining trucks, bomb-disposal robots, robotic insects, and driverless tractors.
 There are a lot of autonomous and semi-autonomous ground vehicles being made for the purpose of transporting passengers. One such example is the free-ranging on grid (FROG) technology which consists of autonomous vehicles, a magnetic track and a supervisory system. The FROG system is deployed for industrial purposes in factory sites and has been in use since 1999 on the ParkShuttle, a PRT-style public transport system in the city of Capelle aan den IJssel to connect the Rivium business park with the neighboring city of Rotterdam (where the route terminates at the Kralingse Zoom metro station). The system experienced a crash in 2005[33] that proved to be caused by a human error.[34]
 Applications for automation in ground vehicles include the following:
 Research is ongoing and prototypes of autonomous ground vehicles exist.
 Extensive automation for cars focuses on either introducing robotic cars or modifying modern car designs to be semi-autonomous.
 Semi-autonomous designs could be implemented sooner as they rely less on technology that is still at the forefront of research. An example is the dual mode monorail. Groups such as RUF (Denmark) and TriTrack (USA) are working on projects consisting of specialized private cars that are driven manually on normal roads but also that dock onto a monorail/guideway along which they are driven autonomously.
 As a method of automating cars without extensively modifying the cars as much as a robotic car, Automated highway systems (AHS) aims to construct lanes on highways that would be equipped with, for example, magnets to guide the vehicles. Automation vehicles have auto-brakes named as Auto Vehicles Braking System (AVBS). Highway computers would manage the traffic and direct the cars to avoid crashes.
 In 2006, The European Commission has established a smart car development program called the Intelligent Car Flagship Initiative.[35]  The goals of that program include:
 There are plenty of further uses for automation in relation to cars. These include:
 Singapore also announced a set of provisional national standards on January 31, 2019, to guide the autonomous vehicle industry. The standards, known as Technical Reference 68 (TR68), will promote the safe deployment of fully driverless vehicles in Singapore, according to a joint press release by Enterprise Singapore (ESG), Land Transport Authority (LTA), Standards Development Organisation and Singapore Standards Council (SSC).[38]
 Since 1999, the 12-seat/10-standing ParkShuttle has been operating on an 1.8 kilometres (1.1 mi) exclusive right of way in the city of Capelle aan den IJssel in The Netherlands. The system uses small magnets in the road surface to allow the vehicle to determine its position.  The use of shared autonomous vehicles was trialed around 2012 in a hospital car park in Portugal.[39] From 2012 to 2016 the European Union funded CityMobil2 project examined the use of shared autonomous vehicles and passenger experience including short term trials in seven cities. This project led to the development of the EasyMile EZ10.[40]
 In the 2010s, self-driving shuttle became able to run in mixed traffic without the need for embedded guidance markers.[41]  So far the focus has been on low speed, 20 miles per hour (32 km/h), with short, fixed routes for the ""last mile"" of journeys.  This means issues of collision avoidance and safety are significantly less challenging than those for automated cars, which seek to match the performance of conventional vehicles.  Many trials have been undertaken, mainly on quiet roads with little traffic or on public pathways or private roadways and specialised test sites.[citation needed]  The capacity of different models varies significantly, between 6-seats and 20-seats.  (Above this size there are conventional buses that have driverless technology installed.)
 In December 2016, the Jacksonville Transportation Authority has announced its intention to replace the Jacksonville Skyway monorail with driverless vehicles that would run on the existing elevated superstructure as well as continue onto ordinary roads.[42]  The project has since been named the ""Ultimate Urban Circulator"" or ""U2C"" and testing has been carried out on shuttles from six different manufacturers.  The cost of the project is estimated at $379 million.[43]
 In January 2017, it was announced the ParkShuttle system in the Netherlands will be renewed and expanded including extending the route network beyond the exclusive right of way so vehicles will run in mixed traffic on ordinary roads.[44]  The plans were delayed and the extension into mixed traffic is now expected in 2021.[45]
 In July 2018, Baidu stated it had built 100 of its 8-seat Apolong model, with plans for commercial sales.[46] As of July 2021 they have not gone into volume production.
 In August 2020, it was reported there were 25 autonomous shuttle manufacturers,[47] including the 2GetThere, Local Motors, Navya, Baidu, Easymile, Toyota and Ohmio.
 In December 2020, Toyota showcased its 20-passenger ""e-Palette"" vehicle, which is due to be used at the 2021 Tokyo Olympic Games.[48]  Toyota has announced it intends to have the vehicle available for commercial applications before 2025.[49]
 In January 2021, Navya released an investor report which predicted global autonomous shuttle sales will reach 12,600 units by 2025, with a market value of EUR 1.7 billion.[50]
 In June 2021, Chinese maker Yutong claimed to have delivered 100 models of its 10-seat Xiaoyu 2.0 autonomous bus for use in Zhengzhou.  Testing has been carried out in a number of cities since 2019 with trials open to the public due to commence in July 2021.[51]
 Self-driving shuttles are already in use on some private roads, such as at the Yutong factory in Zhengzhou where they are used to transport workers between buildings of the world's largest bus factory.[52]
 A large number of trials have been conducted since 2016, with most involving only one vehicle on a short route for a short period of time and with an onboard conductor.  The purpose of the trials has been to both provide technical data and to familiarize the public with the driverless technology. A 2021 survey of over 100 shuttle experiments across Europe concluded that low speed - 15–20 kilometres per hour (9.3–12.4 mph) - was the major the barrier to implementation of autonomous shuttle buses.  The current cost of the vehicles at €280,000 and the need for onboard attendants were also issues.[53]
 In August 2021 a one-year trial was launched at the Colorado School of Mines in Golden, Colorado.  The trial will use nine vehicles (with seven active at any  time) and will provide a 5-10 minute service along three routes at a maximum speed of 12 mph (19 km/h).  At the time of launch this is the largest such trial in the United States.[74][75]
In November 2021, EasyMile has become the first driverless solutions provider in Europe authorized to operate at Level 4 in mixed traffic, on a public road. ""EZ10""  has been making test runs on a medical campus in the southwestern city of Toulouse since March.[76][77]
 In March 2023, ""ZEN drive Pilot"" became the first legally approved Level 4 Automatic operation device under the amended ""Road Traffic Act"" of 2023.[99]
 Vehicle names are in ""quotes""
 Autonomous buses are proposed as well as self driving cars and trucks. Grade 2 level automated minibuses were trialed for a few weeks in Stockholm.[105][106] China has also a small fleet of self-driving public buses in the tech district of Shenzhen, Guangdong.[107]
 The first autonomous bus trial in the United Kingdom commenced in mid-2019, with an Alexander Dennis Enviro200 MMC single-decker bus modified with autonomous software from Fusion Processing able to operate in driverless mode within Stagecoach Manchester's Sharston bus depot, performing tasks such as driving to the washing station, refuelling point and then parking up at a dedicated parking space in the depot.[108] Passenger-carrying driverless bus trials in Scotland commenced in January 2023, with a fleet of five identical vehicles to the Manchester trial used on a 14 miles (23 km) Stagecoach Fife park-and-ride route across the Forth Road Bridge, from the north bank of the Forth to Edinburgh Park station.[109][110]
 Another autonomous trial in Oxfordshire, England, which uses a battery electric Fiat Ducato minibus on a circular service to Milton Park, operated by FirstBus with support from Fusion Processing, Oxfordshire County Council and the University of the West of England, entered full passenger service also in January 2023. The trial route is planned to be extended to Didcot Parkway railway station following the acquisition of a larger single-decker by the end of 2023.[111][112]
 In July 2020 in Japan, AIST Human-Centered Mobility Research Center with Nippon Koei and Isuzu started a series of demonstration tests for mid-sized buses, Isuzu ""Erga Mio"" with autonomous driving systems, in five areas; Ōtsu city in Shiga prefecture, Sanda city in Hyōgo Prefecture and other three areas in sequence.[113][114][115]
 In October 2023, Imagry, an Israeli AI startup, introduced its mapless autonomous driving solution at Busworld Europe, leveraging a real-time image recognition system and a spatial deep convolutional neural network (DCNN) to mimic human driving behavior.[116]
 The concept for autonomous vehicles has been applied for commercial uses, such as autonomous or nearly autonomous trucks.
 Companies such as Suncor Energy, a Canadian energy company, and Rio Tinto Group were among the first to replace human-operated trucks with driverless commercial trucks run by computers.[117] In April 2016, trucks from major manufacturers including Volvo and the Daimler Company completed a week of autonomous driving across Europe, organized by the Dutch, in an effort to get self-driving trucks on the road. With developments in self-driving trucks progressing, U.S. self-driving truck sales is expected to reach 60,000 by 2035 according to a report released by IHS Inc. in June 2016.[118]
 As reported in June 1995 in Popular Science magazine, self-driving trucks were being developed for combat convoys, whereby only the lead truck would be driven by a human and the following trucks would rely on satellite, an inertial guidance system and ground-speed sensors.[119] Caterpillar Inc. made early developments in 2013 with the Robotics Institute at Carnegie Mellon University to improve efficiency and reduce cost at various mining and construction sites.[120]
 In Europe, the Safe Road Trains for the Environment is such an approach.
 From PWC's Strategy& Report,[121] self driving trucks will be the source of a lot of concern around how this technology will impact around 3 million truck drivers in the US, as well as 4 million employees in support of the trucking economy in gas stations, restaurants, bars and hotels. At the same time, some companies like Starsky, are aiming for Level 3 Autonomy, which would see the driver playing a control role around the truck's environment. The company's project, remote truck driving, would give truck drivers a greater work-life balance, enabling them to avoid long periods away from their home. This would however provoke a potential mismatch between the driver's skills with the technological redefinition of the job.
 Companies that buy driverless trucks could massively cut down on costs: human drivers will no longer be required, companies' liabilities due to truck accidents will diminish, and productivity will increase (as the driverless truck doesn't need to rest). The usage of self driving trucks will go hand in hand with the use of real-time data to optimize both efficiency and productivity of the service delivered, as a way to tackle traffic congestion for example. Driverless trucks could enable new business models that would see deliveries shift from day time to night time or time slots in which traffic is less heavily dense.
 In February 2018, Embark Trucks announced it had completed the first cross-country trip of an automated semi, driving 2,400 miles from Los Angeles, CA to Jacksonville, FL on Interstate 10.[127] This followed a November 2017 announcement that it had partnered with Electrolux and Ryder to test its automated truck by moving Frigidaire refrigerators from El Paso, TX to Palm Springs, CA.[128]
 In November 2017 Tesla, Inc., owned by Elon Musk, revealed a prototype of the Tesla Semi and announced that it would go into production. This long-haul, electric semi-truck can drive itself and move in ""platoons"" that automatically follow a lead vehicle.  It was disclosed in August 2017 that it sought permission to test the vehicles in Nevada.[129]
 In December 2018, Anthony Levandowski unveiled his new autonomous driving company, Pronto, which is building L2 ADAS technology for the commercial trucking industry. The company is based in San Francisco.[130]
 Several self-balancing autonomous motorcycles were demonstrated in 2017 and 2018 from BMW, Honda and Yamaha.[131][132][133]
 The concept for autonomous vehicles has also been applied for commercial uses, like for autonomous trains. The world's first driverless urban transit system is the Port Island Line in Kobe, Japan, opened in 1981.[137] The first self-driving train in the UK was launched in London on the Thameslink route.[138]
 An example of an automated train network is the Docklands Light Railway in London.
 Also see List of automated train systems.
 In 2018 the first autonomous trams in Potsdam were trialed.[139]
 An automated guided vehicle or automatic guided vehicle (AGV) is a mobile robot that follows markers or wires in the floor, or uses vision, magnets, or lasers for navigation. They are most often used in industrial applications to move materials around a manufacturing facility or warehouse. Application of the automatic guided vehicle has broadened during the late 20th century.
 Aircraft has received much attention for automation, especially for navigation. A system capable of autonomously navigating a vehicle (especially aircraft) is known as autopilot.
 Various industries such as packages and food experimented with delivery drones. Traditional and new transportation companies are competing in the market. For example, UPS Flight Forward, Alphabet Wing, and Amazon Prime Air are all developing delivery drones.[140] Zipline, an American medical drone delivery company, has the largest active drone delivery operations in the world, and its drones are capable of Level 4 autonomy.[141]
 However, even if technology seems to allow for those solutions to function correctly as various tests of various companies show, the main throwback to the market launch and use of such drones is inevitably the legislation in place and regulatory agencies have to decide on the framework they wish to take to draft regulation. This process is in different phases across the world as each country will tackle the topic independently. For example, Iceland's government and departments of transport, aviation, police have already started issuing licenses for drone operations. It has a permissive approach and together with Costa Rica, Italy, the UAE, Sweden and Norway, has a fairly unrestricted legislation on commercial drone use. Those countries are characterized by a body of regulation that may give operational guidelines or require licensing, registration and insurance.[142]
 On the other side, other countries have decided to ban, either directly (outright ban) or indirectly (effective ban), the use of commercial drones. The RAND Corporation thus makes the difference between countries forbidding drones and those that  have a formal process for commercial drone licensing, but requirements are either impossible to meet or licenses do not appear to have been approved. In the US, UPS is the only one with the Part 135 Standard certification that is required to use drones to deliver to real customers.[140]
 However, most countries seem to be struggling on the integration of drones for commercial uses into their aviation regulatory frameworks. Thus, constraints are placed on the use of those drones such as that they must be operating within the visual line of sight (VLOS) of the pilot and thus limiting their potential range. This would be the case of the Netherlands and Belgium. Most countries do let pilot operate outside the VLOS but is subject to restrictions and pilot ratings, which would be the case of the US.
 The general trend is that legislation is moving fast and laws are constantly being reevaluated. Countries are moving towards a more permissive approach but the industry still lacks infrastructures to ensure the success of such a transition. To provide safety and efficiency, specialized training courses, pilot exams (type of UAV and flying conditions) as well as liability management measures regarding insurances have to be developed.
 There is a sense of urgency that breathes from this innovation as competition is high and companies lobby to integrate them rapidly in their products and services offerings. Since June 2017, the US Senate legislation reauthorized the Federal Aviation Administration and the Department of Transportation to create a carrier certificate allowing for package deliveries by drones.[143]
 Autonomous boats can provide security, do research, or perform hazardous or repetitive tasks (such as guiding a large ship into a harbor or transporting cargo).
 Sea Machines offers an autonomous system for workboats. While it does require a human operator to oversee its actions, the system takes care of a lot of active domain perception and navigation duties that normally a few members of the crew would have to do. They use AI to have situational awareness for different ships within the route. They utilize camera, lidar, and proprietary software to inform the operator of its status.[144][145]
 Buffalo Automation, a team formed from the University of Buffalo, creates technology for semi-autonomous features for boats. They started out creating navigation assist technologies for freighters called AutoMate, which is like having another very experienced “first mate” that will look out for the ship.[146] The system helps make twists and turns of difficult waterways.[145][147]
 This Massachusetts based company has led the forefront of unmanned sailing drones. The Datamarans are out autonomously sailing around to collect ocean data. They are created to enable large payload packages. Due to the automated system and their solar panels, they are able to navigate for longer periods of time. More than anything they boast their technologies on advanced metocean surveys which collect “wind velocity profiles with altitude, water current, conductivity, temperature profiles with depth, hi-resolution bathymetry, sub-bottom profiling, magnetometer measurements” [148][145]
 The autonomous vessel called Mayflower is expected to be the first large ship that makes an unmanned transatlantic journey.[149]
 This autonomous unmanned vessel uses both solar and wind energy to navigate.[150]
 Sea Hunter is an autonomous unmanned surface vehicle (USV) launched in 2016 as part of the DARPA Anti-Submarine Warfare Continuous Trail Unmanned Vessel (ACTUV) program.
 Underwater vehicles have been a focus for automation for tasks such as pipeline inspection and underwater mapping.
 This robot is a four-legged nimble robot that was created to be able to navigate through many different terrain outdoors and indoors. It can walk on its own without colliding into anything. It utilizes many different sensors, including 360 vision cameras and gyroscopes. It is able to keep its balance even when pushed over. This vehicle, while it is not intended to be ridden, can carry heavy loads for construction workers or military personnel through rough terrain.[151]
 
The British Highway Code states that:  By self-driving vehicles, we mean those listed as automated vehicles by the Secretary of State for Transport under the Automated and Electric Vehicles Act 2018. The UK considers the way to update its British Highway Code for automated code:
 Automated vehicles can perform all the tasks involved in driving, in at least some situations. They differ from vehicles fitted with assisted driving features (like cruise control and lane-keeping assistance), which carry out some tasks, but where the driver is still responsible for driving. If you are driving a vehicle with assisted driving features, you MUST stay in control of the vehicle. If the vehicle is designed to require you to resume driving after being prompted to, while the vehicle is driving itself, you MUST remain in a position to be able to take control. For example, you should not move out of the driving seat. You should not be so distracted that you cannot take back control when prompted by the vehicle. Through the autonomy level, it is shown that the higher the level of autonomy, the fewer control humans have on their vehicles (highest level of autonomy needing zero human interventions). One of the few concerns regarding the development of vehicular automation is related to the end-users’ trust in the technology that controls automated vehicles.[153] According to a nationally conducted survey made by Kelley Blue Book (KBB) in 2016, it is shown that the majority of people would still choose to have a certain level of control behind their own vehicle rather than having the vehicle operate in Level 5 autonomy, or in other words, completely autonomous.[154] According to half of the respondents, the idea of safety in an autonomous vehicle diminishes as the level of autonomy increases.[154] This distrust of autonomous driving systems proved to be unchanged throughout the years when a nationwide survey conducted by AAA Foundation for Traffic and Safety (AAAFTS) in 2019 showed the same outcome as the survey KBB did in 2016. AAAFTS survey showed that even though people have a certain level of trust in automated vehicles, most people also have doubts and distrust towards the technology used in autonomous vehicles, with most distrust in Level 5 autonomous vehicles.[155] It is shown by AAAFTS’ survey that people's trust in autonomous driving systems increased when their level of understanding increased.[155]
         The possibility of autonomous vehicle's technology to experience malfunctions is also one of the causes of user's distrust in autonomous driving systems.[153] In fact, it is the concern that most respondents voted for in the AAAFTS survey.[155] Even though autonomous vehicles are made to improve traffic safety by minimizing crashes and their severity,[155] they still caused fatalities. At least 113 autonomous vehicle related accidents have occurred until 2018.[156] In 2015, Google declared that their automated vehicles experienced at least 272 failures, and drivers had to intervene around 13 times to prevent fatalities.[157] Furthermore, other automated vehicles’ manufacturers also reported automated vehicles’ failures, including the Uber car incident.[157] The self-driving Uber car accident that happened in 2018 is one of the examples of autonomous vehicle accidents that are also listed in List of self-driving car fatalities. A report made by the National Transportation Safety Board (NTSB) showed that the self-driving Uber car was unable to identify the victim in a sufficient amount of time for the vehicle to slow down and avoid crashing into the victim.[158]
 Another concern related to vehicle automation is its ethical issues. In reality, autonomous vehicles can encounter inevitable traffic accidents. In situations like that, many risks and calculations need to be made in order to minimize the amount of damage the accident could cause.[159] When a human driver encounters an inevitable accident, the driver will take a spontaneous action based on ethical and moral logic. However, when a driver has no control over the vehicle (Level 5 autonomy), the system of an autonomous vehicle is the one who needs to make that instant decision.[159] Unlike humans, autonomous vehicles don't have reflexes and it can only make decisions based on what it is programmed to do.[159] However, the situation and circumstances of accidents differ from one another, and one decision might not be the best decision for certain accidents. Based on two research studies in 2019,[160][161] the implementation of fully automated vehicles in traffic where semi-automated and non-automated vehicles are still present might lead to many complications.[160] Some flaws that still need consideration include the structure of liability, distribution of responsibilities,[161] efficiency in decision making, and the performance of autonomous vehicles with its diverse surroundings.[160] Still, researchers Steven Umbrello and Roman V. Yampolskiy propose that the value sensitive design approach is one method that can be used to design autonomous vehicles to avoid some of these ethical issues and design for human values.[162]
"
"MuZero is a computer program developed by artificial intelligence research company DeepMind to master games without knowing their rules.[1][2][3] Its release in 2019 included benchmarks of its performance in go, chess, shogi, and a standard suite of Atari games. The algorithm uses an approach similar to AlphaZero. It matched AlphaZero's performance in chess and shogi, improved on its performance in Go (setting a new world record), and improved on the state of the art in mastering a suite of 57 Atari games (the Arcade Learning Environment), a visually-complex domain.
 MuZero was trained via self-play, with no access to rules, opening books, or endgame tablebases. The trained algorithm used the same convolutional and residual architecture as AlphaZero, but with 20 percent fewer computation steps per node in the search tree.[4]
 MuZero really is discovering for itself how to build a model and understand it just from first principles. On November 19, 2019, the DeepMind team released a preprint introducing MuZero.
 MuZero (MZ) is a combination of the high-performance planning of the AlphaZero (AZ) algorithm with approaches to model-free reinforcement learning. The combination allows for more efficient training in classical planning regimes, such as Go, while also handling domains with much more complex inputs at each stage, such as visual video games.
 MuZero was derived directly from AZ code, sharing its rules for setting hyperparameters. Differences between the approaches include:[6]
 The previous state of the art technique for learning to play the suite of Atari games was R2D2, the Recurrent Replay Distributed DQN.[7]
 MuZero surpassed both R2D2's mean and median performance across the suite of games, though it did not do better in every game.
 MuZero used 16 third-generation tensor processing units (TPUs) for training, and 1000 TPUs for selfplay for board games, with 800 simulations per step and 8 TPUs for training and 32 TPUs for selfplay for Atari games, with 50 simulations per step.
 AlphaZero used 64 second-generation TPUs for training, and 5000 first-generation TPUs for selfplay. As TPU design has improved (third-generation chips are 2x as powerful individually as second-generation chips, with further advances in bandwidth and networking across chips in a pod), these are comparable training setups.
 R2D2 was trained for 5 days through 2M training steps.
 MuZero matched AlphaZero's performance in chess and Shogi after roughly 1 million training steps. It matched AZ's performance in Go after 500,000 training steps and surpassed it by 1 million steps. It matched R2D2's mean and median performance across the Atari game suite after 500 thousand training steps and surpassed it by 1 million steps, though it never performed well on 6 games in the suite.
 MuZero was viewed as a significant advancement over AlphaZero,[8] and a generalizable step forward in unsupervised learning techniques.[9][10] The work was seen as advancing understanding of how to compose systems from smaller components, a systems-level development more than a pure machine-learning development.[11]
 While only pseudocode was released by the development team, Werner Duvaud produced an open source implementation based on that.[12]
 MuZero has been used as a reference implementation in other work, for instance as a way to generate model-based behavior.[13]
 In late 2021, a more efficient variant of MuZero was proposed, named EfficientZero. It ""achieves 194.3 percent mean human performance and 109.0 percent median performance on the Atari 100k benchmark with only two hours of real-time game experience"".[14]
 In early 2022, a variant of MuZero was proposed to play stochastic games (for example 2048, backgammon), called Stochastic MuZero, which uses afterstate dynamics and chance codes to account for the stochastic nature of the environment when training the dynamics network.[15]
"
"
 DeepMind Technologies Limited,[6] doing business as Google DeepMind, is a British-American artificial intelligence research laboratory which serves as a subsidiary of Google. Founded in the UK in 2010, it was acquired by Google in 2014[7] and merged with Google AI's Google Brain division to become Google DeepMind in April 2023. The company is based in London, with research centres in Canada,[8] France,[9] Germany, and the United States.
 DeepMind introduced neural Turing machines (neural networks that can access external memory like a conventional Turing machine),[10] resulting in a computer that loosely resembles short-term memory in the human brain.[11][12]
 Google DeepMind has created neural network models to play video games and board games. It made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, a world champion, in a five-game match, which was the subject of a documentary film.[13] A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.[14]
 In 2020, DeepMind made significant advances in the problem of protein folding with AlphaFold.[15] In July 2022, it was announced that over 200 million predicted protein structures, representing virtually all known proteins, would be released on the AlphaFold database.[16][17] AlphaFold's database of predictions achieved state of the art records on benchmark tests for protein folding algorithms, although each individual prediction still requires confirmation by experimental tests. AlphaFold3 was released in May 2024, making structural predictions for the interaction of proteins with various molecules. It achieved new standards on various benchmarks, raising the state of the art accuracies from 28 and 52 percent to 65 and 76 percent.
 The start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman in September 2010.[18][19] Hassabis and Legg first met at the Gatsby Computational Neuroscience Unit at University College London (UCL).[20]
 Demis Hassabis has said that the start-up began working on artificial intelligence technology by teaching it how to play old games from the seventies and eighties, which are relatively primitive compared to the ones that are available today. Some of those games included Breakout, Pong and Space Invaders. AI was introduced to one game at a time, without any prior knowledge of its rules. After spending some time on learning the game, AI would eventually become an expert in it. ""The cognitive processes which the AI goes through are said to be very like those of a human who had never seen the game would use to understand and attempt to master it.""[21] The goal of the founders is to create a general-purpose AI that can be useful and effective for almost anything.
 Major venture capital firms Horizons Ventures and Founders Fund invested in the company,[22] as well as entrepreneurs Scott Banister,[23] Peter Thiel,[24] and Elon Musk.[25] Jaan Tallinn was an early investor and an adviser to the company.[26] On 26 January 2014, Google confirmed its acquisition of DeepMind for a price reportedly ranging between $400 million and $650 million.[27][28][29] and that it had agreed to take over DeepMind Technologies. The sale to Google took place after Facebook reportedly ended negotiations with DeepMind Technologies in 2013.[30] The company was afterwards renamed Google DeepMind and kept that name for about two years.[31]
 In 2014, DeepMind received the ""Company of the Year"" award from Cambridge Computer Laboratory.[32]
 In September 2015, DeepMind and the Royal Free NHS Trust signed their initial information sharing agreement to co-develop a clinical task management app, Streams.[33]
 After Google's acquisition the company established an artificial intelligence ethics board.[34] The ethics board for AI research remains a mystery, with both Google and DeepMind declining to reveal who sits on the board.[35] DeepMind has opened a new unit called DeepMind Ethics and Society and focused on the ethical and societal questions raised by artificial intelligence featuring prominent philosopher Nick Bostrom as advisor.[36] In October 2017, DeepMind launched a new research team to investigate AI ethics.[37][38]
 In December 2019, co-founder Suleyman announced he would be leaving DeepMind to join Google, working in a policy role.[39]
 In April 2023, DeepMind merged with Google AI's Google Brain division to form Google DeepMind, as part of the company's continued efforts to accelerate work on AI in response to OpenAI's ChatGPT.[40] This marked the end of a years-long struggle from DeepMind executives to secure greater autonomy from Google.[41]
 Google Research released a paper in 2016 regarding AI safety and avoiding undesirable behaviour during the AI learning process.[42] In 2017 DeepMind released GridWorld, an open-source testbed for evaluating whether an algorithm learns to disable its kill switch or otherwise exhibits certain undesirable behaviours.[43][44]
 In July 2018, researchers from DeepMind trained one of its systems to play the computer game Quake III Arena.[45]
 As of 2020, DeepMind has published over a thousand papers, including thirteen papers that were accepted by Nature or Science.[citation needed] DeepMind received media attention during the AlphaGo period; according to a LexisNexis search, 1842 published news stories mentioned DeepMind in 2016, declining to 1363 in 2019.[46]
 Unlike earlier AIs, such as IBM's Deep Blue or Watson, which were developed for a pre-defined purpose and only function within that scope, DeepMind's initial algorithms were intended to be general. They used reinforcement learning, an algorithm that learns from experience using only raw pixels as data input. Their initial approach used deep Q-learning with a convolutional neural network.[31][47] They tested the system on video games, notably early arcade games, such as Space Invaders or Breakout.[47][48] Without altering the code, the same AI was able to play certain games more efficiently than any human ever could.[48]
 In 2013, DeepMind published research on an AI system that surpassed human abilities in games such as Pong, Breakout and Enduro, while surpassing state of the art performance on Seaquest, Beamrider, and Q*bert.[49][50] This work reportedly led to the company's acquisition by Google.[51] DeepMind's AI had been applied to video games made in the 1970s and 1980s; work was ongoing for more complex 3D games such as Quake, which first appeared in the 1990s.[48]
 In 2020, DeepMind published Agent57,[52][53] an AI Agent which surpasses human level performance on all 57 games of the Atari 2600 suite.[54] In July 2022, DeepMind announced the development of DeepNash, a model-free multi-agent reinforcement learning system capable of playing the board game Stratego at the level of a human expert.[55]
 In October 2015, a computer Go program called AlphaGo, developed by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero.[56] This was the first time an artificial intelligence (AI) defeated a professional Go player.[57] Previously, computers were only known to have played Go at ""amateur"" level.[56][58] Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force.[56][58]
 In March 2016 it beat Lee Sedol, one of the highest ranked players in the world, with a score of 4 to 1 in a five-game match. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who had been the world's highest-ranked player for two years.[59][60] In 2017, an improved version, AlphaGo Zero, defeated AlphaGo in a hundred out of a hundred games. Later that year, AlphaZero, a modified version of AlphaGo Zero, gained superhuman abilities at chess and shogi. In 2019, DeepMind released a new model named MuZero that mastered the domains of Go, chess, shogi, and Atari 2600 games without human data, domain knowledge, or known rules.[61][62]
 AlphaGo technology was developed based on deep reinforcement learning, making it different from the AI technologies then on the market. The data fed into the AlphaGo algorithm consisted of various moves based on historical tournament data. The number of moves was increased gradually until over 30 million of them were processed. The aim was to have the system mimic the human player, as represented by the input data, and eventually become better. It played against itself and learned from the outcomes; thus, it learned to improve itself over the time and increased its winning rate as a result.[63]
 AlphaGo used two deep neural networks: a policy network to evaluate move probabilities and a value network to assess positions. The policy network trained via supervised learning, and was subsequently refined by policy-gradient reinforcement learning. The value network learned to predict winners of games played by the policy network against itself. After training, these networks employed a lookahead Monte Carlo tree search, using the policy network to identify candidate high-probability moves, while the value network (in conjunction with Monte Carlo rollouts using a fast rollout policy) evaluated tree positions.[64]
 In contrast, AlphaGo Zero was trained without being fed data of human-played games. Instead it generated its own data, playing millions of games against itself. It used a single neural network, rather than separate policy and value networks. Its simplified tree search relied upon this neural network to evaluate positions and sample moves. A new reinforcement learning algorithm incorporated lookahead search inside the training loop.[64] AlphaGo Zero employed around 15 people and millions in computing resources.[65] Ultimately, it needed much less computing power than AlphaGo, running on four specialized AI processors (Google TPUs), instead of AlphaGo's 48.[66] It also required less training time, being able to beat its predecessor after just three days, compared with months required for the original AlphaGo.[67] Similarly, AlphaZero also learned via self-play.
 Researchers applied MuZero to solve the real world challenge of video compression with a set number of bits with respect to Internet traffic on sites such as YouTube, Twitch, and Google Meet. The goal of MuZero is to optimally compress the video so the quality of the video is maintained with a reduction in data. The final result using MuZero was a 6.28% average reduction in bitrate.[68][69]
 In 2016, Hassabis discussed the game StarCraft as a future challenge, since it requires strategic thinking and handling imperfect information.[70]
 In January 2019, DeepMind introduced AlphaStar, a program playing the real-time strategy game StarCraft II. AlphaStar used reinforcement learning based on replays from human players, and then played against itself to enhance its skills. At the time of the presentation, AlphaStar had knowledge equivalent to 200 years of playing time. It won 10 consecutive matches against two professional players, although it had the unfair advantage of being able to see the entire field, unlike a human player who has to move the camera manually. A preliminary version in which that advantage was fixed lost a subsequent match.[71]
 In July 2019, AlphaStar began playing against random humans on the public 1v1 European multiplayer ladder. Unlike the first iteration of AlphaStar, which played only Protoss v. Protoss, this one played as all of the game's races, and had earlier unfair advantages fixed.[72][73] By October 2019, AlphaStar had reached Grandmaster level on the StarCraft II ladder on all three StarCraft races, becoming the first AI to reach the top league of a widely popular esport without any game restrictions.[74]
 In 2016, DeepMind turned its artificial intelligence to protein folding, a long-standing problem in molecular biology. In December 2018, DeepMind's AlphaFold won the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP) by successfully predicting the most accurate structure for 25 out of 43 proteins. ""This is a lighthouse project, our first major investment in terms of people and resources into a fundamental, very important, real-world scientific problem,"" Hassabis said to The Guardian.[75] In 2020, in the 14th CASP, AlphaFold's predictions achieved an accuracy score regarded as comparable with lab techniques. Dr Andriy Kryshtafovych, one of the panel of scientific adjudicators, described the achievement as ""truly remarkable"", and said the problem of predicting how proteins fold had been ""largely solved"".[76][77][78]
 In July 2021, the open-source RoseTTAFold and AlphaFold2 were released to allow scientists to run their own versions of the tools. A week later DeepMind announced that AlphaFold had completed its prediction of nearly all human proteins as well as the entire proteomes of 20 other widely studied organisms.[79] The structures were released on the AlphaFold Protein Structure Database. In July 2022, it was announced that the predictions of over 200 million proteins, representing virtually all known proteins, would be released on the AlphaFold database.[16][17]
 The most recent update, AlphaFold3, was released in May 2024, predicting the interactions of proteins with DNA, RNA, and various other molecules. In a particular benchmark test on the problem of DNA interactions, AlphaFold3's attained an accuracy of 65%, significantly improving the previous state of the art of 28%.[80]
 In 2016, DeepMind introduced WaveNet, a text-to-speech system. It was originally too computationally intensive for use in consumer products, but in late 2017 it became ready for use in consumer applications such as Google Assistant.[81][82] In 2018 Google launched a commercial text-to-speech product, Cloud Text-to-Speech, based on WaveNet.[83][84] In 2018, DeepMind introduced a more efficient model called WaveRNN co-developed with Google AI.[85][86] In 2020 WaveNetEQ, a packet loss concealment method based on a WaveRNN architecture, was presented.[87] In 2019, Google started to roll WaveRNN with WavenetEQ out to Google Duo users.[88]
 Released in May 2022, Gato is a polyvalent multimodal model. It was trained on 604 tasks, such as image captioning, dialogue, or stacking blocks. On 450 of these tasks, Gato outperformed human experts at least half of the time, according to DeepMind.[89] Unlike models like MuZero, Gato does not need to be retrained to switch from one task to the other.
 Sparrow is an artificial intelligence-powered chatbot developed by DeepMind to build safer machine learning systems by using a mix of human feedback and Google search suggestions.[90]
 Chinchilla is a language model developed by DeepMind.[91]
 DeepMind posted a blog post on 28 April 2022 on a single visual language model (VLM) named Flamingo that can accurately describe a picture of something with just a few training images.[92][93]
 In 2022, DeepMind unveiled AlphaCode, an AI-powered coding engine that creates computer programs at a rate comparable to that of an average programmer, with the company testing the system against coding challenges created by Codeforces utilized in human competitive programming competitions.[94] AlphaCode earned a rank equivalent to 54% of the median score on Codeforces after being trained on GitHub data and Codeforce problems and solutions. The program was required to come up with a unique solution and stopped from duplicating answers.
 Gemini is a multimodal large language model which was released on 6 December 2023.[95] It is the successor of Google's LaMDA and PaLM 2 language models and sought to challenge OpenAI's GPT-4.[96] Gemini comes in 3 sizes: Nano, Pro, and Ultra.[97] Gemini is also the name of the chatbot that integrates Gemini (and which was previously called Bard).[98]
 Gemma is a family of lightweight, open source, large language models which was released on 21 February 2024. It's available in two distinct sizes: a 7 billion parameter model optimized for GPU and TPU usage, and a 2 billion parameter model designed for CPU and on-device applications. Gemma models were trained on up to 6 trillion tokens of text, employing similar architectures, datasets, and training methodologies as the Gemini model family.[99]
 In March 2024, DeepMind introduced Scalable Instructable Multiword Agent, or SIMA, an AI agent capable of understanding and following natural language instructions to complete tasks across various 3D virtual environments. Trained on nine video games from eight studios and four research environments, SIMA demonstrated adaptability to new tasks and settings without requiring access to game source code or APIs. The agent comprises pre-trained computer vision and language models fine-tuned on gaming data, with language being crucial for understanding and completing given tasks as instructed. DeepMind's research aimed to develop more helpful AI agents by translating advanced AI capabilities into real-world actions through a language interface.[100][101]
 Released in June 2023, RoboCat is an AI model that can control robotic arms. The model can adapt to new models of robotic arms, and to new types of tasks.[102][103]
 DeepMind researchers have applied machine learning models to the sport of football, often referred to as soccer in North America, modelling the behaviour of football players, including the goalkeeper, defenders, and strikers during different scenarios such as penalty kicks. The researchers used heat maps and cluster analysis to organize players based on their tendency to behave a certain way during the game when confronted with a decision on how to score or prevent the other team from scoring. 
 The researchers mention that machine learning models could be used to democratize the football industry by automatically selecting interesting video clips of the game that serve as highlights. This can be done by searching videos for certain events, which is possible because video analysis is an established field of machine learning. This is also possible because of extensive sports analytics based on data including annotated passes or shots, sensors that capture data about the players movements many times over the course of a game, and game theory models.[104][105]
 Google has unveiled a new archaeology document program, named Ithaca after the Greek island in Homer's Odyssey.[106] This deep neural network helps researchers restore the empty text of damaged Greek documents, and to identify their date and geographical origin.[107] The work builds on another text analysis network that DeepMind released in 2019, named Pythia.[107] Ithaca achieves 62% accuracy in restoring damaged texts and 71% location accuracy, and has a dating precision of 30 years.[107] The authors claimed that the use of Ithaca by ""expert historians"" raised the accuracy of their work from 25 to 72 percent.[106] However, Eleanor Dickey noted that this test was actually only made of students, saying that it wasn't clear how helpful Ithaca would be to ""genuinely qualified editors.""[107]
 The team is working on extending the model to other ancient languages, including Demotic, Akkadian, Hebrew, and Mayan.[106]
 In November 2023, Google DeepMind announced an Open Source Graph Network for Materials Exploration (GNoME). The tool proposes millions of materials previously unknown to chemistry, including several hundred thousand stable crystalline structures, of which 736 had been experimentally produced by the Massachusetts Institute of Technology, at the time of the release.[108][109] However, according to Anthony Cheetham, GNoME did not make ""a useful, practical contribution to the experimental materials scientists.""[110] A review article by Cheetham and Ram Seshadri were unable to identify any ""strikingly novel"" materials found by GNoME, with most being minor variants of already-known materials.[110][111]
 In October 2022, DeepMind released AlphaTensor, which used reinforcement learning techniques similar to those in AlphaGo, to find novel algorithms for matrix multiplication.[112][113] In the special case of multiplying two 4×4 matrices with integer entries, where only the evenness or oddness of the entries is recorded, AlphaTensor found an algorithm requiring only 47 distinct multiplications; the previous optimum, known since 1969, was the more general Strassen algorithm, using 49 multiplications.[114] Computer scientist Josh Alman described AlphaTensor as ""a proof of concept for something that could become a breakthrough,"" while Vassilevska Williams called it ""a little overhyped""[114] despite also acknowledging its basis in reinforcement learning as ""something completely different"" from previous approaches.[113]
 AlphaGeometry is a neuro-symbolic AI that was able to solve 25 out of 30 geometry problems of the International Mathematical Olympiad, a performance comparable to that of a gold medalist.[115]
 Traditional geometry programs are symbolic engines that rely exclusively on human-coded rules to generate rigorous proofs, which makes them lack flexibility in unusual situations. AlphaGeometry combines such a symbolic engine with a specialized large language model trained on synthetic data of geometrical proofs. When the symbolic engine doesn't manage to find a formal and rigorous proof on its own, it solicits the large language model, which suggests a geometrical construct to move forward. However, it is unclear how applicable this method is to other domains of mathematics or reasoning, because symbolic engines rely on domain-specific rules and because of the need for synthetic data.[115]
 Google has stated that DeepMind algorithms have greatly increased the efficiency of cooling its data centers by automatically balancing the cost of hardware failures against the cost of cooling.[116] In addition, DeepMind (alongside other Alphabet AI researchers) assists Google Play's personalized app recommendations.[83] DeepMind has also collaborated with the Android team at Google for the creation of two new features which were made available to people with devices running Android Pie, the ninth installment of Google's mobile operating system. These features, Adaptive Battery and Adaptive Brightness, use machine learning to conserve energy and make devices running the operating system easier to use. It is the first time DeepMind has used these techniques on such a small scale, with typical machine learning applications requiring orders of magnitude more computing power.[117]
 In July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced to develop AI applications for healthcare.[118] DeepMind would be applied to the analysis of anonymised eye scans, searching for early signs of diseases leading to blindness.
 In August 2016, a research programme with University College London Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas.[119]
 There are also projects with the Royal Free London NHS Foundation Trust and Imperial College Healthcare NHS Trust to develop new clinical mobile apps linked to electronic patient records.[120] Staff at the Royal Free Hospital were reported as saying in December 2017 that access to patient data through the app had saved a 'huge amount of time' and made a 'phenomenal' difference to the management of patients with acute kidney injury. Test result data is sent to staff's mobile phones and alerts them to changes in the patient's condition. It also enables staff to see if someone else has responded, and to show patients their results in visual form.[121][unreliable source?]
 In November 2017, DeepMind announced a research partnership with the Cancer Research UK Centre at Imperial College London with the goal of improving breast cancer detection by applying machine learning to mammography.[122] Additionally, in February 2018, DeepMind announced it was working with the U.S. Department of Veterans Affairs in an attempt to use machine learning to predict the onset of acute kidney injury in patients, and also more broadly the general deterioration of patients during a hospital stay so that doctors and nurses can more quickly treat patients in need.[123]
 DeepMind developed an app called Streams, which sends alerts to doctors about patients at risk of acute kidney injury.[124] On 13 November 2018, DeepMind announced that its health division and the Streams app would be absorbed into Google Health.[125] Privacy advocates said the announcement betrayed patient trust and appeared to contradict previous statements by DeepMind that patient data would not be connected to Google accounts or services.[126][127] A spokesman for DeepMind said that patient data would still be kept separate from Google services or projects.[128]
 In April 2016, New Scientist obtained a copy of a data sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust. The latter operates three London hospitals where an estimated 1.6 million patients are treated annually. The agreement shows DeepMind Health had access to admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion in order to conduct research to seek better outcomes in various health conditions.[129][130]
 A complaint was filed to the Information Commissioner's Office (ICO), arguing that the data should be pseudonymised and encrypted.[131] In May 2016, New Scientist published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare products Regulatory Agency.[132]
 In 2017, the ICO concluded a year-long investigation that focused on how the Royal Free NHS Foundation Trust tested the app, Streams, in late 2015 and 2016.[133] The ICO found that the Royal Free failed to comply with the Data Protection Act when it provided patient details to DeepMind, and found several shortcomings in how the data was handled, including that patients were not adequately informed that their data would be used as part of the test. DeepMind published its thoughts[134] on the investigation in July 2017, saying ""we need to do better"" and highlighting several activities and initiatives they had initiated for transparency, oversight and engagement. This included developing a patient and public involvement strategy[135] and being transparent in its partnerships.
 In May 2017, Sky News published a leaked letter from the National Data Guardian, Dame Fiona Caldicott, revealing that in her ""considered opinion"" the data-sharing agreement between DeepMind and the Royal Free took place on an ""inappropriate legal basis"".[136] The Information Commissioner's Office ruled in July 2017 that the Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind.[137]
 In October 2017, DeepMind announced a new research unit, DeepMind Ethics & Society.[138] Their goal is to fund external research of the following themes: privacy, transparency, and fairness; economic impacts; governance and accountability; managing AI risk; AI morality and values; and how AI can address the world's challenges. As a result, the team hopes to further understand the ethical implications of AI and aid society to seeing AI can be beneficial.[139]
 This new subdivision of DeepMind is a completely separate unit from the partnership of leading companies using AI, academia, civil society organizations and nonprofits of the name Partnership on Artificial Intelligence to Benefit People and Society of which DeepMind is also a part.[140] The DeepMind Ethics and Society board is also distinct from the mooted AI Ethics Board that Google originally agreed to form when acquiring DeepMind.[141]
 DeepMind sponsors three chairs of machine learning:
"
"
 Deep Blue was a chess-playing expert system run on a unique purpose-built IBM supercomputer. It was the first computer to win a game, and the first to win a match, against a reigning world champion under regular time controls. Development began in 1985 at Carnegie Mellon University under the name ChipTest. It then moved to IBM, where it was first renamed Deep Thought, then again in 1989 to Deep Blue. It first played world champion Garry Kasparov in a six-game match in 1996, where it lost four games to two. It was upgraded in 1997 and in a six-game re-match, it defeated Kasparov by winning two games and drawing three. Deep Blue's victory is considered a milestone in the history of artificial intelligence and has been the subject of several books and films.
 While a doctoral student at Carnegie Mellon University, Feng-hsiung Hsu began development of a chess-playing supercomputer under the name ChipTest. The machine won the North American Computer Chess Championship in 1987 and Hsu and his team followed up with a successor, Deep Thought, in 1988.[2][3] After receiving his doctorate in 1989, Hsu and Murray Campbell joined IBM Research to continue their project to build a machine that could defeat a world chess champion.[4] Their colleague Thomas Anantharaman briefly joined them at IBM before leaving for the finance industry and being replaced by programmer Arthur Joseph Hoane.[5][6] Jerry Brody, a long-time employee of IBM Research, subsequently joined the team in 1990.[7]
 After Deep Thought's two-game 1989 loss to Kasparov, IBM held a contest to rename the chess machine: the winning name was ""Deep Blue"", submitted by Peter Fitzhugh Brown,[8] was a play on IBM's nickname, ""Big Blue"".[a] After a scaled-down version of Deep Blue played Grandmaster Joel Benjamin,[10] Hsu and Campbell decided that Benjamin was the expert they were looking for to help develop Deep Blue's opening book, so hired him to assist with the preparations for Deep Blue's matches against Garry Kasparov.[11] In 1995, a Deep Blue prototype played in the eighth World Computer Chess Championship, playing Wchess to a draw before ultimately losing to Fritz in round five, despite playing as White.[12]
 In 1997, the Chicago Tribune mistakenly reported that Deep Blue had been sold to United Airlines, a confusion based upon its physical resemblance to IBM's mainstream RS6000/SP2 systems.[13]
 Today, one of the two racks that made up Deep Blue is held by the National Museum of American History, having previously been displayed in an exhibit about the Information Age,[14] while the other rack was acquired by the Computer History Museum in 1997, and is displayed in the Revolution exhibit's ""Artificial Intelligence and Robotics"" gallery.[15] Several books were written about Deep Blue, among them Behind Deep Blue: Building the Computer that Defeated the World Chess Champion by Deep Blue developer Feng-hsiung Hsu.[16]
 Subsequent to its predecessor Deep Thought's 1989 loss to Garry Kasparov, Deep Blue played Kasparov twice more. In the first game of the first match, which took place from 10 to 17 February 1996, Deep Blue became the first machine to win a chess game against a reigning world champion under regular time controls. However, Kasparov won three and drew two of the following five games, beating Deep Blue by 4–2 at the close of the match.[17]
 Deep Blue's hardware was subsequently upgraded,[3][18][b] doubling its speed before it faced Kasparov again in May 1997, when it won the six-game rematch 3½–2½. Deep Blue won the deciding game after Kasparov failed to secure his position in the opening, thereby becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls.[20][21] The version of Deep Blue that defeated Kasparov in 1997 typically searched to a depth of six to eight moves, and twenty or more moves in some situations.[22] David Levy and Monty Newborn estimate that each additional ply (half-move) of forward insight increases the playing strength between 50 and 70 Elo points.[23]
 In the 44th move of the first game of their second match, unknown to Kasparov, a bug in Deep Blue's code led it to enter an unintentional loop, which it exited by taking a randomly selected valid move.[24] Kasparov did not take this possibility into account, and misattributed the seemingly pointless move to ""superior intelligence"".[21] Subsequently, Kasparov experienced a decline in performance in the following game,[24] though he denies this was due to anxiety in the wake of Deep Blue's inscrutable move.[25]
 After his loss, Kasparov said that he sometimes saw unusual creativity in the machine's moves, suggesting that during the second game, human chess players had intervened on behalf of the machine. IBM denied this, saying the only human intervention occurred between games.[26][27] Kasparov demanded a rematch, but IBM had dismantled Deep Blue after its victory and refused the rematch.[28] The rules allowed the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play that were revealed during the course of the match. Kasparov requested printouts of the machine's log files, but IBM refused, although the company later published the logs on the Internet.[29]
 The 1997 tournament awarded a $700,000 first prize to the Deep Blue team and a $400,000 second prize to Kasparov. Carnegie Mellon University awarded an additional $100,000 to the Deep Blue team, a prize created by computer science professor Edward Fredkin in 1980 for the first computer program to beat a reigning world chess champion.[30]
 Kasparov initially called Deep Blue an ""alien opponent"" but later belittled it, stating that it was ""as intelligent as your alarm clock"".[31] According to Martin Amis, two grandmasters who played Deep Blue agreed that it was ""like a wall coming at you"".[32][33] Hsu had the rights to use the Deep Blue design independently of IBM, but also independently declined Kasparov's rematch offer.[34] In 2003, the documentary film Game Over: Kasparov and the Machine investigated Kasparov's claims that IBM had cheated. In the film, some interviewees describe IBM's investment in Deep Blue as an effort to boost its stock value.[35]
 Following Deep Blue's victory, AI specialist Omar Syed designed a new game, Arimaa, which was intended to be very simple for humans but very difficult for computers to master;[36][37] however, in 2015, computers proved capable of defeating strong Arimaa players.[38] Since Deep Blue's victory, computer scientists have developed software for other complex board games with competitive communities. AlphaGo defeated top Go players in the 2010s.[39][40]
 Computer scientists such as Deep Blue developer Campbell believed that playing chess was a good measurement for the effectiveness of artificial intelligence, and by beating a world champion chess player, IBM showed that they had made significant progress.[3] Deep Blue is also responsible for the popularity of using games as a display medium for artificial intelligence, as in the cases of IBM Watson or AlphaGo.[41]
 While Deep Blue, with its capability of evaluating 200 million positions per second,[42] was the first computer to face a world chess champion in a formal match,[3] it was a then-state-of-the-art expert system, relying upon rules and variables defined and fine-tuned by chess masters and computer scientists. In contrast, current chess engines such as Leela Chess Zero typically use reinforcement machine learning systems that train a neural network to play, developing its own internal logic rather than relying upon rules defined by human experts.[39]
 In a November 2006 match between Deep Fritz and world chess champion Vladimir Kramnik, the program ran on a computer system containing a dual-core Intel Xeon 5160 CPU, capable of evaluating only 8 million positions per second, but searching to an average depth of 17 to 18 plies (half-moves) in the middlegame thanks to heuristics; it won 4–2.[43][44]
 Deep Blue's evaluation function was initially written in a generalized form, with many to-be-determined parameters (e.g., how important is a safe king position compared to a space advantage in the center, etc.). Values for these parameters were determined by analyzing thousands of master games. The evaluation function was then split into 8,000 parts, many of them designed for special positions. The opening book encapsulated more than 4,000 positions and 700,000 grandmaster games, while the endgame database contained many six-piece endgames and all five and fewer piece endgames. An additional database named the ""extended book"" summarizes entire games played by Grandmasters. The system combines its searching ability of 200 million chess positions per second with summary information in the extended book to select opening moves.[45]
 Before the second match, the program's rules were fine-tuned by grandmaster Joel Benjamin. The opening library was provided by grandmasters Miguel Illescas, John Fedorowicz, and Nick de Firmian.[46] When Kasparov requested that he be allowed to study other games that Deep Blue had played so as to better understand his opponent, IBM refused, leading Kasparov to study many popular PC chess games to familiarize himself with computer gameplay.[47]
 Deep Blue used custom VLSI chips to parallelize the alpha–beta search algorithm,[48] an example of symbolic AI.[49] The system derived its playing strength mainly from brute force computing power. It was a massively parallel IBM RS/6000 SP Supercomputer with 30 PowerPC 604e processors and 480 custom 600 μm CMOS VLSI ""chess chips"" designed to execute the chess-playing expert system, as well as FPGAs intended to allow patching of the VLSIs (which ultimately went unused) all housed in two cabinets. The chess chip has four parts: the move generator, the smart-move stack, the evaluation function, and the search control. The move generator is a 8x8 combinational logic circuit, a chess board in miniature.[50][51][52][53]
 Its chess playing program was written in C and ran under the AIX operating system. It was capable of evaluating 200 million positions per second, twice as fast as the 1996 version. 
 In 1997, Deep Blue was upgraded again to become the 259th most powerful supercomputer according to the TOP500 list, achieving 11.38 GFLOPS on the parallel high performance LINPACK benchmark.[54]
"
"
 In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in the 1950s.[1] AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player's input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation.[2]
 In general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence.
 The term ""game AI"" is used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general, and so video game AI may often not constitute ""true AI"" in that such techniques do not necessarily facilitate computer learning or other standard criteria, only constituting ""automated computation"" or a predetermined and limited set of responses to a predetermined and limited set of inputs.[3][4][5]
 Many industries and corporate voices[who?] argue that game AI has come a long way in the sense that it has revolutionized the way humans interact with all forms of technology, although many[who?] expert researchers are skeptical of such claims, and particularly of the notion that such technologies fit the definition of ""intelligence"" standardly used in the cognitive sciences.[3][4][5][6] Industry voices[who?] make the argument that AI has become more versatile in the way we use all technological devices for more than their intended purpose because the AI allows the technology to operate in multiple ways, allegedly developing their own personalities and carrying out complex instructions of the user.[7][8]
 People[who?] in the field of AI have argued that video game AI is not true intelligence, but an advertising buzzword used to describe computer programs that use simple sorting and matching algorithms to create the illusion of intelligent behavior while bestowing software with a misleading aura of scientific or technological complexity and advancement.[3][4][5][9] Since game AI for NPCs is centered on appearance of intelligence and good gameplay within environment restrictions, its approach is very different from that of traditional AI.
 Game playing was an area of research in AI from its inception. One of the first examples of AI is the computerized game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took the form of a relatively small box and was able to regularly win games even against highly skilled players of the game.[1] In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess.[10] These were among the first computer programs ever written. Arthur Samuel's checkers program, developed in the middle 50s and early 60s, eventually achieved sufficient skill to challenge a respectable amateur.[11] Work on checkers and chess would culminate in the defeat of Garry Kasparov by IBM's Deep Blue computer in 1997.[12] The first video games developed in the 1960s and early 1970s, like Spacewar!, Pong, and Gotcha (1973), were games implemented on discrete logic and strictly based on the competition of two players, without AI.
 Games that featured a single player mode with enemies started appearing in the 1970s.  The first notable ones for the arcade appeared in 1974: the Taito game Speed Race (racing video game) and the Atari games Qwak (duck hunting light gun shooter) and Pursuit (fighter aircraft dogfighting simulator).  Two text-based computer games, Star Trek (1971) and Hunt the Wumpus (1973), also had enemies.  Enemy movement was based on stored patterns. The incorporation of microprocessors would allow more computation and random elements overlaid into movement patterns.
 It was during the golden age of video arcade games that the idea of AI opponents was largely popularized, due to the success of Space Invaders (1978), which sported an increasing difficulty level, distinct movement patterns, and in-game events dependent on hash functions based on the player's input. Galaxian (1979) added more complex and varied enemy movements, including maneuvers by individual enemies who break out of formation. Pac-Man (1980) introduced AI patterns to maze games, with the added quirk of different personalities for each enemy. Karate Champ (1984) later introduced AI patterns to fighting games. First Queen (1988) was a tactical action RPG which featured characters that can be controlled by the computer's AI in following the leader.[13][14] The role-playing video game Dragon Quest IV (1990) introduced a ""Tactics"" system, where the user can adjust the AI routines of non-player characters during battle, a concept later introduced to the action role-playing game genre by Secret of Mana (1993).
 Games like Madden Football, Earl Weaver Baseball and Tony La Russa Baseball all based their AI in an attempt to duplicate on the computer the coaching or managerial style of the selected celebrity.  Madden, Weaver and La Russa all did extensive work with these game development teams to maximize the accuracy of the games.[citation needed]  Later sports titles allowed users to ""tune"" variables in the AI to produce a player-defined managerial or coaching strategy.
 The emergence of new game genres in the 1990s prompted the use of formal AI tools like finite state machines. Real-time strategy games taxed the AI with many objects, incomplete information, pathfinding problems, real-time decisions and economic planning, among other things.[15] The first games of the genre had notorious problems. Herzog Zwei (1989), for example, had almost broken pathfinding and very basic three-state state machines for unit control, and Dune II (1992) attacked the players' base in a beeline and used numerous cheats.[16] Later games in the genre exhibited more sophisticated AI.
 Later games have used bottom-up AI methods, such as the emergent behaviour and evaluation of player actions in games like Creatures or Black & White. Façade (interactive story) was released in 2005 and used interactive multiple way dialogs and AI as the main aspect of game.
 Games have provided an environment for developing artificial intelligence with potential applications beyond gameplay. Examples include Watson, a Jeopardy!-playing computer; and the RoboCup tournament, where robots are trained to compete in soccer.[17]
 Many experts complain that the ""AI"" in the term ""game AI"" overstates its worth, as game AI is not about intelligence, and shares few of the objectives of the academic field of AI. Whereas ""real AI"" addresses fields of machine learning, decision making based on arbitrary data input, and even the ultimate goal of strong AI that can reason, ""game AI"" often consists of a half-dozen rules of thumb, or heuristics, that are just enough to give a good gameplay experience.[citation needed] Historically, academic game-AI projects have been relatively separate from commercial products because the academic approaches tended to be simple and non-scalable. Commercial game AI has developed its own set of tools, which have been sufficient to give good performance in many cases.[2]
 Game developers' increasing awareness of academic AI and a growing interest in computer games by the academic community is causing the definition of what counts as AI in a game to become less idiosyncratic.  Nevertheless, significant differences between different application domains of AI mean that game AI can still be viewed as a distinct subfield of AI.  In particular, the ability to legitimately solve some AI problems in games by cheating creates an important distinction. For example, inferring the position of an unseen object from past observations can be a difficult problem when AI is applied to robotics, but in a computer game a NPC can simply look up the position in the game's scene graph.  Such cheating can lead to unrealistic behavior and so is not always desirable.  But its possibility serves to distinguish game AI and leads to new problems to solve, such as when and how to cheat.[citation needed]
 The major limitation to strong AI is the inherent depth of thinking and the extreme complexity of the decision-making process. This means that although it would be then theoretically possible to make ""smart"" AI the problem would take considerable processing power.[citation needed]
 Game AI/heuristic algorithms are used in a wide variety of quite disparate fields inside a game.  The most obvious is in the control of any NPCs in the game, although ""scripting"" (decision tree) is currently the most common means of control.[18] These handwritten decision trees often result in ""artificial stupidity"" such as repetitive behavior, loss of immersion, or abnormal behavior in situations the developers did not plan for.[19]
 Pathfinding, another common use for AI, is widely seen in real-time strategy games. Pathfinding is the method for determining how to get a NPC from one point on a map to another, taking into consideration the terrain, obstacles and possibly ""fog of war"".[20][21] Commercial videogames often use fast and simple ""grid-based pathfinding"", wherein the terrain is mapped onto a rigid grid of uniform squares and a pathfinding algorithm such as A* or IDA* is applied to the grid.[22][23][24] Instead of just a rigid grid, some games use irregular polygons and assemble a navigation mesh out of the areas of the map that NPCs can walk to.[22][25] As a third method, it is sometimes convenient for developers to manually select ""waypoints"" that NPCs should use to navigate; the cost is that such waypoints can create unnatural-looking movement. In addition, waypoints tend to perform worse than navigation meshes in complex environments.[26][27] Beyond static pathfinding, navigation is a sub-field of Game AI focusing on giving NPCs the capability to navigate in a dynamic environment, finding a path to a target while avoiding collisions with other entities (other NPC, players...) or collaborating with them (group navigation).[citation needed] Navigation in dynamic strategy games with large numbers of units, such as Age of Empires (1997) or Civilization V (2010), often performs poorly; units often get in the way of other units.[27]
 Rather than improve the Game AI to properly solve a difficult problem in the virtual environment, it is often more cost-effective to just modify the scenario to be more tractable. If pathfinding gets bogged down over a specific obstacle, a developer may just end up moving or deleting the obstacle.[28] In Half-Life (1998), the pathfinding algorithm sometimes failed to find a reasonable way for all the NPCs to evade a thrown grenade; rather than allow the NPCs to attempt to bumble out of the way and risk appearing stupid, the developers instead scripted the NPCs to crouch down and cover in place in that situation.[29]
 Many contemporary video games fall under the category of action, first-person shooter, or adventure. In most of these types of games, there is some level of combat that takes place. The AI's ability to be efficient in combat is important in these genres. A common goal today is to make the AI more human or at least appear so.
 One of the more positive and efficient features found in modern-day video game AI is the ability to hunt. AI originally reacted in a very black and white manner. If the player were in a specific area then the AI would react in either a complete offensive manner or be entirely defensive. In recent years, the idea of ""hunting"" has been introduced; in this 'hunting' state the AI will look for realistic markers, such as sounds made by the character or footprints they may have left behind.[30] These developments ultimately allow for a more complex form of play. With this feature, the player can actually consider how to approach or avoid an enemy. This is a feature that is particularly prevalent in the stealth genre.
 Another development in recent game AI has been the development of ""survival instinct"". In-game computers can recognize different objects in an environment and determine whether it is beneficial or detrimental to its survival. Like a user, the AI can look for cover in a firefight before taking actions that would leave it otherwise vulnerable, such as reloading a weapon or throwing a grenade. There can be set markers that tell it when to react in a certain way. For example, if the AI is given a command to check its health throughout a game then further commands can be set so that it reacts a specific way at a certain percentage of health. If the health is below a certain threshold then the AI can be set to run away from the player and avoid it until another function is triggered. Another example could be if the AI notices it is out of bullets, it will find a cover object and hide behind it until it has reloaded. Actions like these make the AI seem more human. However, there is still a need for improvement in this area.
 Another side-effect of combat AI occurs when two AI-controlled characters encounter each other; first popularized in the id Software game Doom, so-called 'monster infighting' can break out in certain situations.  Specifically, AI agents that are programmed to respond to hostile attacks will sometimes attack each other if their cohort's attacks land too close to them.[citation needed] In the case of Doom, published gameplay manuals even suggest taking advantage of monster infighting in order to survive certain levels and difficulty settings.
 Procedural content generation (PCG) is an AI technique to autonomously create ingame content through algorithms with minimal input from designers.[31] PCG is typically used to dynamically generate game features such as levels, NPC dialogue, and sounds. Developers input specific parameters to guide the algorithms into making content for them. PCG offers numerous advantages from both a developmental and player experience standpoint. Game studios are able to spend less money on artists and save time on production.[32] Players are given a fresh, highly replayable experience as the game generates new content each time they play. PCG allows game content to adapt in real time to the player's actions.[33]
 Generative algorithms (a rudimentary form of AI) have been used for level creation for decades. The iconic 1980 dungeon crawler computer game Rogue is a foundational example. Players are tasked with descending through the increasingly difficult levels of a dungeon to retrieve the Amulet of Yendor. The dungeon levels are algorithmically generated at the start of each game. The save file is deleted every time the player dies.[34] The algorithmic dungeon generation creates unique gameplay that would not otherwise be there as the goal of retrieving the amulet is the same each time.
 Opinions on total level generation as seen in games like Rogue can vary. Some developers can be skeptical of the quality of generated content and desire to create a world with a more ""human"" feel so they will use PCG more sparingly.[31] Consequently, they will only use PCG to generate specific components of an otherwise handcrafted level. A notable example of this is Ubisoft's 2017 tactical shooter Tom Clancy's Ghost Recon Wildlands. Developers used a pathfinding algorithm trained with a data set of real maps to create road networks that would weave through handcrafted villages within the game world.[33] This is an intelligent use of PCG as the AI would have a large amount of real world data to work with and roads are straightforward to create. However, the AI would likely miss nuances and subtleties if it was tasked with creating a village where people live.
 As AI has become more advanced, developer goals are shifting to create massive repositories of levels from data sets. In 2023, researchers from New York University and the University of the Witwatersrand trained a large language model to generate levels in the style of the 1981 puzzle game Sokoban. They found that the model excelled at generating levels with specifically requested characteristics such as difficulty level or layout.[35] However, current models such as the one used in the study require large datasets of levels to be effective. They concluded that, while promising, the high data cost of large language models currently outweighs the benefits for this application.[35] Continued advancements in the field will likely lead to more mainstream use in the future.
 The musical score of a video game is an important expression of the emotional tone of a scene to the player. Sound effects such as the noise of a weapon hitting an enemy help indicate the effect of the player's actions. Generating these in real time creates an engaging experience for the player because the game is more responsive to their input.[31] An example is the 2013 adventure game Proteus where an algorithm dynamically adapts the music based on the angle the player is viewing the ingame landscape from.[34]
 Recent breakthroughs in AI have resulted in the creation of advanced tools that are capable of creating music and sound based on evolving factors with minimal developer input. One such example is the MetaComposure music generator. MetaComposure is an evolutionary algorithm designed to generate original music compositions during real time gameplay to match the current mood of the environment.[36] The algorithm is able to assess the current mood of the game state through ""mood tagging"". Research indicates that that there is a significant positive statistical correlation regarding player rated game engagement and the dynamically generated musical compositions when they accurately match their current emotions.[37]
 Game AI often amounts to pathfinding and finite state machines. Pathfinding gets the AI from point A to point B, usually in the most direct way possible. State machines permit transitioning between different behaviors. The Monte Carlo tree search method[38] provides a more engaging game experience by creating additional obstacles for the player to overcome. The MCTS consists of a tree diagram in which the AI essentially plays tic-tac-toe. Depending on the outcome, it selects a pathway yielding the next obstacle for the player. In complex video games, these trees may have more branches, provided that the player can come up with several strategies to surpass the obstacle. In this 2022 year's survey,[39] you can learn about recent applications of the MCTS algorithm in various game domains such as perfect-information combinatorial games, strategy games (including RTS), card games etc.
 Academic AI may play a role within Game AI, outside the traditional concern of controlling NPC behavior. Georgios N. Yannakakis highlighted four potential application areas:[2]
 Rather than procedural generation, some researchers have used generative adversarial networks (GANs) to create new content. In 2018 researchers at Cornwall University trained a GAN on a thousand human-created levels for Doom; following training, the neural net prototype was able to design new playable levels on its own. Similarly, researchers at the University of California prototyped a GAN to generate levels for Super Mario.[40] In 2020 Nvidia displayed a GAN-created clone of Pac-Man; the GAN learned how to recreate the game by watching 50,000 (mostly bot-generated) playthroughs.[41]
 Gamers always ask if the AI cheats (presumably so they can complain if they lose) In the context of artificial intelligence in video games, cheating refers to the programmer giving agents actions and access to information that would be unavailable to the player in the same situation.[43] Believing that the Atari 8-bit could not compete against a human player, Chris Crawford did not fix a bug in Eastern Front (1941) that benefited the computer-controlled Russian side.[44] Computer Gaming World in 1994 reported that ""It is a well-known fact that many AIs 'cheat' (or, at least, 'fudge') in order to be able to keep up with human players"".[45]
 For example, if the agents want to know if the player is nearby they can either be given complex, human-like sensors (seeing, hearing, etc.), or they can cheat by simply asking the game engine for the player's position. Common variations include giving AIs higher speeds in racing games to catch up to the player or spawning them in advantageous positions in first-person shooters. The use of cheating in AI shows the limitations of the ""intelligence"" achievable artificially; generally speaking, in games where strategic creativity is important, humans could easily beat the AI after a minimum of trial and error if it were not for this advantage. Cheating is often implemented for performance reasons where in many cases it may be considered acceptable as long as the effect is not obvious to the player. While cheating refers only to privileges given specifically to the AI—it does not include the inhuman swiftness and precision natural to a computer—a player might call the computer's inherent advantages ""cheating"" if they result in the agent acting unlike a human player.[43] Sid Meier stated that he omitted multiplayer alliances in Civilization because he found that the computer was almost as good as humans in using them, which caused players to think that the computer was cheating.[46] Developers say that most game AIs are honest but they dislike players erroneously complaining about ""cheating"" AI. In addition, humans use tactics against computers that they would not against other people.[44]
 In the 1996 game Creatures, the user ""hatches"" small furry animals and teaches them how to behave. These ""Norns"" can talk, feed themselves, and protect themselves against vicious creatures. It was the first popular application of machine learning in an interactive simulation. Neural networks are used by the creatures to learn what to do. The game is regarded as a breakthrough in artificial life research, which aims to model the behavior of creatures interacting with their environment.[47]
 In the 2001 first-person shooter Halo: Combat Evolved the player assumes the role of the Master Chief, battling various aliens on foot or in vehicles. Enemies use cover very wisely, and employ suppressing fire and grenades. The squad situation affects the individuals, so certain enemies flee when their leader dies. Attention is paid to the little details, with enemies notably throwing back grenades or team-members responding to being bothered. The underlying ""behavior tree"" technology has become very popular in the games industry since Halo 2.[47]
 The 2005 psychological horror first-person shooter F.E.A.R. has player characters engage a battalion of cloned super-soldiers, robots and paranormal creatures. The AI uses a planner to generate context-sensitive behaviors, the first time in a mainstream game. This technology is still used as a reference for many studios. The Replicas are capable of utilizing the game environment to their advantage, such as overturning tables and shelves to create cover, opening doors, crashing through windows, or even noticing (and alerting the rest of their comrades to) the player's flashlight. In addition, the AI is also capable of performing flanking maneuvers, using suppressing fire, throwing grenades to flush the player out of cover, and even playing dead. Most of these actions, in particular the flanking, is the result of emergent behavior.[48][49]
 The survival horror series S.T.A.L.K.E.R. (2007–) confronts the player with man-made experiments, military soldiers, and mercenaries known as Stalkers. The various encountered enemies (if the difficulty level is set to its highest) use combat tactics and behaviors such as healing wounded allies, giving orders, out-flanking the player and using weapons with pinpoint accuracy.[citation needed]
 The 2010 real-time strategy game StarCraft II: Wings of Liberty gives the player control of one of three factions in a 1v1, 2v2, or 3v3 battle arena. The player must defeat their opponents by destroying all their units and bases. This is accomplished by creating units that are effective at countering opponents' units. Players can play against multiple different levels of AI difficulty ranging from very easy to Cheater 3 (insane). The AI is able to cheat at the difficulty Cheater 1 (vision), where it can see units and bases when a player in the same situation could not. Cheater 2 gives the AI extra resources, while Cheater 3 gives an extensive advantage over its opponent.[50]
 The 2024 browser-based sandbox game Infinite Craft uses generative AI software, including LLaMA. When two elements are being combined, a new element is generated by the AI.[51]
 Generative artificial intelligence, AI system that can response to prompts and produce text, images, and audio and video clips, arose in 2023 with systems like ChatGPT and Stable Diffusion. In video games, these systems could create the potential for game assets to be created indefinitely, bypassing typical limitations on human creations. However, there are similar concerns in other fields particularly the potential for loss of jobs normally dedicated to the creation of these assets.[52]
 In January 2024, SAG-AFTRA, a United States union representing actors, signed a contract with Replica Studios that would allow Replica to capture the voicework of union actors for creating AI voice systems based on their voices for use in video games, with the contract assuring pay and rights protections. While the contract was agreed upon by a SAG-AFTRA committee, many members expressed criticism of the move, having not been told of it until it was completed and that the deal did not do enough to protect the actors.[53]
"
"Artificial intelligence in healthcare is a term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease.[1][2] Specifically, AI is the ability of computer algorithms to arrive at approximate conclusions based solely on input data.
 The primary aim of health-related AI applications is to analyze relationships between clinical data and patient outcomes.[3] AI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care. What differentiates AI technology from traditional technologies in healthcare is the ability to gather larger and more diverse data, process it, and produce a well-defined output to the end-user. AI does this through machine learning algorithms and deep learning. Because radiographs are the most common imaging tests conducted in most radiology departments, the potential for AI to help with triage and interpretation of traditional radiographs (X-ray pictures) is particularly noteworthy.[4] These processes can recognize patterns in behavior and create their own logic. To gain useful insights and predictions, machine learning models must be trained using extensive amounts of input data. AI algorithms behave differently from humans in two ways: (1) algorithms are literal: once a goal is set, the algorithm learns exclusively from the input data and can only understand what it has been programmed to do, (2) and some deep learning algorithms are black boxes; algorithms can predict with extreme precision, but offer little to no comprehensible explanation to the logic behind its decisions aside from the data and type of algorithm used.[5]
 As widespread use of AI in healthcare is relatively new, research is ongoing into its application in various fields of medicine and industry. Additionally, greater consideration is being given to the unprecedented ethical concerns related to its practice such as data privacy, automation of jobs, and representation biases.[6] Furthermore, new technologies brought about by AI in healthcare are often resisted by healthcare leaders, leading to slow and erratic adoption.[7]
 Research in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral.[8][9] While it was designed for applications in organic chemistry, it provided the basis for a subsequent system MYCIN,[10] considered one of the most significant early uses of artificial intelligence in medicine.[10][11] MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.[12]
 The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians.[13] Approaches involving fuzzy set theory,[14] Bayesian networks,[15] and artificial neural networks,[16][17] have been applied to intelligent computing systems in healthcare.
 Medical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: 
 AI algorithms can also be used to analyze large amounts of data through electronic health records for disease prevention and diagnosis. Medical institutions such as The Mayo Clinic, Memorial Sloan Kettering Cancer Center,[25][26] and the British National Health Service,[27] have developed AI algorithms for their departments. Large technology companies such as IBM[28] and Google,[27] have also developed AI algorithms for healthcare. Additionally, hospitals are looking to AI software to support operational initiatives that increase cost saving, improve patient satisfaction, and satisfy their staffing and workforce needs.[29] Currently, the United States government is investing billions of dollars to progress the development of AI in healthcare.[5] Companies are developing technologies that help healthcare managers improve business operations through increasing utilization, decreasing patient boarding, reducing length of stay and optimizing staffing levels.[30]
 Artificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool.[31][32] Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment for acute coronary syndrome.[31] Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital.[33] Another growing area of research is the utility of AI in classifying heart sounds and diagnosing valvular disease.[34] Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data on social determinants of health as they pertain to cardiovascular disease.[35]
 A key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is noninferior to humans in interpretation of cardiac echocardiograms[36] and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.[37]
 In cardiovascular tissue engineering and organoid studies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.[38]
 Dermatology is an imaging abundant speciality[39] and the development of deep learning has been strongly tied to image processing. Therefore, there is a natural fit between the dermatology and deep learning. There are three main imaging types in dermatology: contextual images, macro images, micro images.[40] For each modality, deep learning showed great progress.[41] Han et al. showed keratinocytic skin cancer detection from face photographs.[42] Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images.[43] Noyan et al. demonstrated a convolutional neural network that achieved 94% accuracy at identifying skin cells from microscopic Tzanck smear images.[44] A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.[45]
 According to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer.[46] However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets.[47] Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study[48] was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.[47]
 It has also been suggested that AI could be used to automatically evaluate the outcome of maxillo-facial surgery or cleft palate therapy in regard to facial attractiveness or age appearance.[49][50]
 AI can play a role in various facets of the field of gastroenterology. Endoscopic exams such as esophagogastroduodenoscopies (EGD) and colonoscopies rely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of early gastric cancer have shown sensitivity close to expert endoscopists.[51]
 Artificial intelligence, or AI, utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilised in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues. [52]
 AI has shown potential in both the laboratory and clinical spheres of infectious disease medicine.[53] As the novel coronavirus ravages through the globe, the United States is estimated to invest more than $2 billion in AI-related healthcare research by 2025, more than 4 times the amount spent in 2019 ($463 million).[54] While neural networks have been developed to rapidly and accurately detect a host response to COVID-19 from mass spectrometry samples, a scoping review of the literature found few examples of AI being used directly in clinical practice during the COVID-19 pandemic itself.[55] Other applications include support-vector machines identifying antimicrobial resistance, machine learning analysis of blood smears to detect malaria, and improved point-of-care testing of Lyme disease based on antigen detection. Additionally, AI has been investigated for improving diagnosis of meningitis, sepsis, and tuberculosis, as well as predicting treatment complications in hepatitis B and hepatitis C patients.[53]
 AI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients.[56] Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients’ pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.[57]
 The use of AI technologies has been explored for use in the diagnosis and prognosis of Alzheimer's disease (AD). For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs.[58] The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative.[59] Researchers have developed models that rely on convolutional neural networks with the aim of improving early diagnostic accuracy.[60] Generative adversarial networks are a form of deep learning that have also performed well in diagnosing AD.[61] There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets.[62] They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases.
 AI has been explored for use in cancer diagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics.[63] AI has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides.[64]
 In January 2020, Google DeepMind announced an algorithm capable of surpassing human experts in breast cancer detection in screening scans.[65][66] A number of researchers, including Trevor Hastie, Joelle Pineau, and Robert Tibshirani among others, published a reply claiming that DeepMind's research publication in Nature lacked key details on methodology and code, ""effectively undermin[ing] its scientific value"" and making it impossible for the scientific community to confirm the work.[67] In the MIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as ""an advertisement"" having little to do with science.[68]
 In July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date in identifying prostate cancer, with 98% sensitivity and 97% specificity.[69][70] In 2023 a study reported the use of AI for CT-based radiomics classification at grading the aggressiveness of retroperitoneal sarcoma with 82% accuracy compared with 44% for lab analysis of biopsies.[71][72]
 Artificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness.[73] In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm.[74] Moreover, AI technology may be used to further improve ""diagnosis rates"" because of the potential to decrease detection time.[75]
 For many diseases, pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods of digital pathology allows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B, gastric cancer, and colorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes.[51] AI is well-suited for use in low-complexity pathological analysis of large-scale screening samples, such as colorectal or breast cancer screening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis.[77] Several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists,[77] and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone.[78] Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years,[79] though savings attributed to AI specifically have not yet been widely researched. The use of augmented and virtual reality could prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review.[77] AI also has the potential to identify histological findings at levels beyond what the human eye can see,[77] and has shown the ability to use genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer.[80] One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlled trials in determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.[77]
 Primary care has become one key development area for AI technologies.[81][82] AI in primary care has been used for supporting decision making, predictive modelling, and business analytics.[83] There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.[84]
 In psychiatry, AI applications are still in a phase of proof-of-concept.[85] Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes,[86] chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.[87]
 Challenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017.[88] Such applications outside the healthcare system raise various professional, ethical and regulatory questions.[89] Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.[90]
 AI is being studied within the field of radiology to detect and diagnose diseases through computerized tomography (CT) and magnetic resonance (MR) imaging.[91] It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers.[92] Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated.[93] AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality,[94] and automatically assessing image quality.[95] Further research investigating the use of AI in nuclear medicine focuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies.[96] The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: (1) convolutional neural network-based analysis; and (2) utilization of radiomics.[97]
 An article by Jiang, et al. (2017) demonstrated that there are several types of AI techniques that have been used for a variety of different diseases, such as support vector machines, neural networks, and decision trees. Each of these techniques is described as having a ""training goal"" so ""classifications agree with the outcomes as much as possible…"".
 To demonstrate some specifics for disease diagnosis/classification there are two different techniques used in the classification of these diseases including using artificial neural networks (ANN) and Bayesian networks (BN). It was found that ANN was better and could more accurately classify diabetes and cardiovascular disease.
 Through the use of machine learning classifiers (MLCs), artificial intelligence has been able to substantially aid doctors in patient diagnosis through the manipulation of mass electronic health records (EHRs).[106] Medical conditions have grown more complex, and with a vast history of electronic medical records building, the likelihood of case duplication is high.[106] Although someone today with a rare illness is less likely to be the only person to have had any given disease, the inability to access cases from similarly symptomatic origins is a major roadblock for physicians.[106] The implementation of AI to not only help find similar cases and treatments, such as through early predictors of Alzheimer's disease and dementias,[107] but also factor in chief symptoms and help the physicians ask the most appropriate questions helps the patient receive the most accurate diagnosis and treatment possible.[106]
 Recent developments in statistical physics, machine learning, and inference algorithms are being explored for their potential in improving medical diagnostic approaches.[108] Combining the skills of medical professionals and machines can help overcome decision-making weaknesses in medical practice. To do so, one needs precise disease definitions and a probabilistic analysis of symptoms and molecular profiles. Physicists have been studying similar problems for years, using microscopic elements and their interactions to extract macroscopic states of various physical systems. Physics inspired machine learning approaches can thus be applied to study disease processes and to perform biomarker analysis.
 The increase of telemedicine, the treatment of patients remotely, has shown the rise of possible AI applications.[109] AI can assist in caring for patients remotely by monitoring their information through sensors.[110] A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.[110]
 Another application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.[111]
 Since the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations.[112] Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal.[112] Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.[112]
 Electronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, the next step is to use artificial intelligence to interpret the records and provide new information to physicians.[113]
 One application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms.[113] For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences.[113] NLP algorithms consolidate these differences so that larger datasets can be analyzed.[113] Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read.[113] Other applications use concept processing to analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.[114]
 Beyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record and predict a risk for a disease based on their previous information and family history.[115] One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts.[116] This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses.[116] Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease.[116] Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time.[115] One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70–72% accuracy in predicting individualized treatment response.[117] These methods are helpful due to the fact that the amount of online health records doubles every five years.[115] Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.[115]
 Improvements in natural language processing led to the development of algorithms to identify drug-drug interactions in medical literature.[118][119][120][121] Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken.[122] To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers at Carlos III University assembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms.[123] Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.[124]  Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.[118][119][121]
 Other algorithms identify drug-drug interactions from patterns in user-generated content, especially electronic health records and/or adverse event reports.[119][120] Organizations such as the FDA Adverse Event Reporting System (FAERS) and the World Health Organization's VigiBase allow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.[125]
 The trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms.
 A large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions.[91] Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of ""data assessment, storage, management, and analysis technologies"" which are all crucial parts of the healthcare industry.[126]
 The following are examples of large companies that have contributed to AI algorithms for use in healthcare:
 Digital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).
 IFlytek launched a service robot ""Xiao Man"", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH (""Cruzr"") and Softbank Robotics (""Pepper"").
 The Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India.
 With the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies.[126] Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well.[126] Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances or in .[126]
 Artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.  Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before.[129] With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.[129]
 Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient.[130] The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.[130]
 While research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection.[131] These challenges of the clinical use of AI have brought about a potential need for regulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines (TRIPOD+AI,[132] DECIDE-AI,[133] CONSORT-AI[134]) have been developed to provide recommendations on the key details that need to be reported.
 Currently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR).[135] The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States.[135] In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.
 There is concern that large language models can overwhelm people with both accurate health information and also misinformation, leading to potential challenges in public health. This calls for the need for policy and user guidance related to health information through AI.[136]
 The only agency that has expressed concern is the FDA. Bakul Patel, the Associate Center Director for Digital Health of the FDA, is quoted saying in May 2017: ""We're trying to get people who have hands-on development experience with a product's full life cycle. We already have some scientists who know artificial intelligence and machine learning, but we want complementary people who can look forward and see how this technology will evolve.""
 The joint ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) has built a platform - known as the ITU-WHO AI for Health Framework - for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.
 In January 2021, the US FDA published a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan.[138] This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.
 According to the U.S. Department of Health and Human Services, the Office for Civil Rights (OCR) has issued guidance on the ethical use of AI in healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the ""implementation of the HHS AI strategy"".[139] The OCR also has issued rules and regulations to protect the privacy of individuals’ health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals’ privacy and ethical issues related to AI in healthcare[140]
 The U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processes[141]
 The European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency.[142] With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy.[142] In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.[143]
 In order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology.[135] The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare.
 A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.[144]
 According to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years.[145] However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.[145]
 Automation can provide benefits alongside doctors as well. It is expected that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not.[146] AI will likely not completely replace healthcare workers but rather give them more time to attend to their patients. AI may avert healthcare worker burnout and cognitive overload.
 Recently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand[147]
 Since AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care.[148] A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping.[149]
 There can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.[148]  Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. White males are overly represented in medical data sets.[150] Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations.[151] Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients.[150] In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult.[152] However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.
 A final source of bias, which has been called ""label choice bias"", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients.[153] Solutions to the ""label choice bias"" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.
"
"
 In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent to human preferences. In classical reinforcement learning, the goal of such an agent is to learn a function that guides its behavior called a policy. This function learns to maximize the reward it receives from a separate reward function based on its task performance.[1] However, it is difficult to define explicitly a reward function that approximates human preferences. Therefore, RLHF seeks to train a ""reward model"" directly from human feedback.[2] The reward model is first trained in a supervised fashion—independently from the policy being optimized—to predict if a response to a given prompt is good (high reward) or bad (low reward) based on ranking data collected from human annotators. This model is then used as a reward function to improve an agent's policy through an optimization algorithm like proximal policy optimization.[3]
 RLHF has applications in various domains in machine learning, including natural language processing tasks such as text summarization and conversational agents, computer vision tasks like text-to-image models, and the development of video game bots. While RLHF is an effective method of training models to act better in accordance with human preferences, it also faces challenges due to the way the human preference data is collected. Though RLHF does not require massive amounts of data to improve performance, sourcing high-quality preference data is still an expensive process. Furthermore, if the data is not carefully collected from a representative sample, the resulting model may exhibit unwanted biases.
 Optimizing a model based on human feedback is desirable when a task is difficult to specify yet easy to judge.[4] For example, one may want to train a model to generate safe text that is both helpful and harmless (such as lacking bias, toxicity, or otherwise harmful content). Asking humans to manually create examples of harmless and harmful text would be difficult and time-consuming. However, humans are adept at swiftly assessing and comparing the harmfulness of different AI-generated text. Therefore, a more practical objective would be to allow the model to use this type of human feedback to improve its text generation.[5]
 Despite the clear benefits of incorporating human feedback in training models, prior efforts—including some that leverage reinforcement learning—have encountered significant challenges. Most attempts were either narrow and difficult to generalize, breaking down on more complex tasks,[6][7][8][9] or they faced difficulties learning from sparse (lacking specific information and relating to large amounts of text at a time) or noisy (inconsistently rewarding similar outputs) reward functions.[10][11]
 RLHF was not the first successful method of using human feedback for reinforcement learning, but it is one of the most widely used. The foundation for RLHF was introduced as an attempt to create a general algorithm for learning from a practical amount of human feedback.[4][3] The algorithm as used today was introduced by OpenAI in a paper on enhancing text continuation or summarization based on human feedback, and it began to gain popularity when the same method was reused in their paper on InstructGPT.[2][12][13] RLHF has also been shown to improve the robustness of RL agents and their capacity for exploration, which results in an optimization process more adept at handling uncertainty and efficiently exploring its environment in search of the highest reward.[14]
 Human feedback is commonly collected by prompting humans to rank instances of the agent's behavior.[13][15][16] These rankings can then be used to score outputs, for example, using the Elo rating system, which is an algorithm for calculating the relative skill levels of players in a game based only on the outcome of each game.[3] While ranking outputs is the most widely adopted form of feedback, recent research has explored other forms, such as numerical feedback, natural language feedback, and prompting for direct edits to the model's output.[17]
 One initial motivation of RLHF was that it requires relatively small amounts of comparison data to be effective.[4] It has been shown that a small amount of data can lead to comparable results to a larger amount. In addition, increasing the amount of data tends to be less effective than proportionally increasing the size of the reward model.[12] Nevertheless, a larger and more diverse amount of data can be crucial for tasks where it is important to avoid bias from a partially representative group of annotators.[13]
 When learning from human feedback through pairwise comparison under the Bradley–Terry–Luce model (or the Plackett–Luce model for K-wise comparisons over more than two comparisons), the maximum likelihood estimator (MLE) for linear reward functions has been shown to converge if the comparison data is generated under a well-specified linear model. This implies that, under certain conditions, if a model is trained to decide which choices people would prefer between pairs (or groups) of choices, it will necessarily improve at predicting future preferences. This improvement  is expected as long as the comparisons it learns from are based on a consistent and simple rule.[18][19]
 Both offline data collection models, where the model is learning by interacting with a static dataset and updating its policy in batches, as well as online data collection models, where the model directly interacts with the dynamic environment and updates its policy immediately, have been mathematically studied proving sample complexity bounds for RLHF under different feedback models.[18][20]
 In the offline data collection model, when the objective is policy training, a pessimistic MLE that incorporates a lower confidence bound as the reward estimate is most effective. Moreover, when applicable, it has been shown that considering K-wise comparisons directly is asymptotically more efficient than converting them into pairwise comparisons for prediction purposes.[20][21][13]
 In the online scenario, when human feedback is collected through pairwise comparisons under the Bradley–Terry–Luce model and the objective is to minimize the algorithm's regret (the difference in performance compared to an optimal agent), it has been shown that an optimistic MLE that incorporates an upper confidence bound as the reward estimate can be used to design sample efficient algorithms (meaning that they require relatively little training data). A key challenge in RLHF when learning from pairwise (or dueling) comparisons is associated with the non-Markovian nature of its optimal policies. Unlike simpler scenarios where the optimal strategy does not require memory of past actions, in RLHF, the best course of action often depends on previous events and decisions, making the strategy inherently memory-dependent.[19]
 RLHF has been applied to various domains of natural language processing (NLP), such as conversational agents, text summarization, and natural language understanding.[22][12] Ordinary reinforcement learning, in which agents learn from their actions based on a predefined ""reward function"", is difficult to apply to NLP tasks because the rewards tend to be difficult to define or measure, especially when dealing with complex tasks that involve human values or preferences.[4] RLHF can steer NLP models, in particular language models, to provide answers that align with human preferences with regard to such tasks by capturing their preferences beforehand in the reward model. This results in a model capable of generating more relevant responses and rejecting inappropriate or irrelevant queries.[13][23] Some notable examples of RLHF-trained language models are OpenAI's ChatGPT (and its predecessor InstructGPT),[15][24][25] DeepMind's Sparrow,[26][27][28] Google's Gemini,[29] and Anthropic's Claude.[30]
 In computer vision, RLHF has also been used to align text-to-image models. Studies that successfully used RLHF for this goal have noted that the use of KL regularization in RLHF, which aims to prevent the learned policy from straying too far from the unaligned model, helped to stabilize the training process by reducing overfitting to the reward model. The final image outputs from models trained with KL regularization were noted to be of significantly higher quality than those trained without.[31][32] Other methods tried to incorporate the feedback through more direct training—based on maximizing the reward without the use of reinforcement learning—but conceded that an RLHF-based approach would likely perform better due to the  online sample generation used in RLHF during updates as well as the aforementioned KL regularization over the prior model, which mitigates overfitting to the reward function.[33]
 RLHF was initially applied to other areas, such as the development of video game bots and tasks in simulated robotics. For example, OpenAI and DeepMind trained agents to play Atari games based on human preferences. In classical RL-based training of such bots, the reward function is simply correlated to how well the agent is performing in the game, usually using metrics like the in-game score. In comparison, in RLHF, a human is periodically presented with two clips of the agent's behavior in the game and must decide which one looks better. This approach can teach agents to perform at a competitive level without ever having access to their score. In fact, it was shown that RLHF can sometimes lead to superior performance over RL with score metrics because the human's preferences can contain more useful information than performance-based metrics.[4][34] The agents achieved strong performance in many of the environments tested, often surpassing human performance.[35]
 In RLHF, two different models are trained: a reward model and a reinforcement learning (RL) policy. The reward model learns to determine what behavior is desirable based on human feedback, while the policy is guided by the reward model to determine the agent's actions. Both models are commonly initialized using a pre-trained autoregressive language model. This model is then customarily trained in a supervised manner on a relatively small dataset of pairs of prompts to an assistant and their accompanying responses, written by human annotators. The reward model benefits from starting with a pre-trained model, as this initializes it with an understanding of language and focuses training explicitly on learning human preferences, speeding up the process. In addition to being used to initialize the reward model and the RL policy, the model is then also used to sample data to be compared by annotators.[13][12]
 The reward model is then trained by replacing the final layer of the previous model with a randomly initialized regression head. This change shifts the model from its original classification task over its vocabulary to simply outputting a number corresponding to the score of any given prompt and response. This model is trained on the human preference comparison data collected earlier from the supervised model. In particular, it is trained to minimize the following cross-entropy loss function, which incentivizes it to make predictions that are closer to the actual human ratings:
 





L


(
θ
)
=
−


1


(


K
2


)





E

(
x
,

y

w


,

y

l


)


[
log
⁡
(
σ
(

r

θ


(
x
,

y

w


)
−

r

θ


(
x
,

y

l


)
)
)
]


{\displaystyle {\mathcal {L}}(\theta )=-{\frac {1}{K \choose 2}}E_{(x,y_{w},y_{l})}[\log(\sigma (r_{\theta }(x,y_{w})-r_{\theta }(x,y_{l})))]}


 where 



K


{\displaystyle K}

 is the number of responses the labelers ranked, 




r

θ


(
x
,
y
)


{\displaystyle r_{\theta }(x,y)}

 is the output of the reward model for prompt 



x


{\displaystyle x}

 and completion 



y


{\displaystyle y}

, 




y

w




{\displaystyle y_{w}}

 is the preferred completion over 




y

l




{\displaystyle y_{l}}

, 



σ
(
x
)


{\displaystyle \sigma (x)}

 denotes the sigmoid function, and 



E
[
X
]


{\displaystyle E[X]}

 denotes the expected value.[13] This loss function essentially measures the difference between the reward model's predictions and the decisions made by humans. The goal is to make the model's guesses as close as possible to the humans' preferences by minimizing the difference measured by this equation. In the case of only pairwise comparisons, the factor of 



1

/





(


K
2


)






{\displaystyle 1/{\tbinom {K}{2}}}

 is omitted.[12] Otherwise, all 







(


K
2


)






{\displaystyle {\tbinom {K}{2}}}

 comparisons from each prompt are used for training as a single batch.[13] After training, the outputs of the model are normalized such that the reference completions have a mean score of 0.[12]
 Similarly to the reward model, the human feedback policy is also fine-tuned over the pre-trained model. The objective of this fine-tuning step is to adapt the pre-existing, unaligned model (initially trained in a supervised manner) to better align with human preferences by adjusting its parameters based on the rewards derived from human feedback. The output of the reward model can be used as the reward to be maximized using RL for the prompt-response pairs.[12] The environment randomly presents the policy with prompts from the dataset and expects responses to them, simulating real-world scenarios where the agent must understand diverse prompts and generate appropriate responses. Denoting the learned RL policy with parameters 



ϕ


{\displaystyle \phi }

 as 




π

ϕ


RL




{\displaystyle \pi _{\phi }^{\text{RL}}}

, we can define the following objective function:
 




objective

(
ϕ
)
=

E

(
x
,
y
)
∼

D


π

ϕ


RL







[


r

θ


(
x
,
y
)
−
β
log
⁡

(




π

ϕ


RL


(
y

|

x
)



π

SFT


(
y

|

x
)



)


]



{\displaystyle {\text{objective}}(\phi )=E_{(x,y)\sim D_{\pi _{\phi }^{\text{RL}}}}\left[r_{\theta }(x,y)-\beta \log \left({\frac {\pi _{\phi }^{\text{RL}}(y|x)}{\pi ^{\text{SFT}}(y|x)}}\right)\right]}


 where 




D


π

ϕ


RL






{\displaystyle D_{\pi _{\phi }^{\text{RL}}}}

 is the training distribution we are drawing from and 




π

SFT




{\displaystyle \pi ^{\text{SFT}}}

 is the previously trained, unaligned, model. The constant 



β


{\displaystyle \beta }

 is used to adjust the intensity of the KL penalty term. This penalty is applied on a per-token basis between the policy and the unaligned models' outputs. Its purpose is to avoid excessively fine-tuning the policy, ensuring that the training process does not overly specialize the model on the new training data.[13][12] This KL term works by penalizing the KL divergence (a measure of statistical distance between distributions) between the model being fine-tuned and the initial supervised model. By choosing an appropriate 



β


{\displaystyle \beta }

, the training can balance learning from new data while retaining useful information from the initial model, increasing generalization by avoiding fitting too closely to the new data. Aside from preventing the new model from producing outputs too dissimilar those of the initial model, a second motivation of including the KL term is to allow the policy to further explore the environment by encouraging additional entropy, which can prevent the model from collapsing to a single mode.[12]
 In simpler terms, the objective function calculates how well the policy's responses are expected to align with human feedback. The policy generates responses to prompts, and each response is evaluated both on how well it matches human preferences (as measured by the reward model) and how similar it is to responses the model would naturally generate. The goal is to balance improving alignment with human preferences while ensuring the model's responses remain diverse and not too far removed from what it has learned during its initial training. This helps the model not only to provide answers that people find useful or agreeable but also to maintain a broad understanding and avoid overly narrow or repetitive responses.
 A second term is commonly added to the objective function that allows the policy to incorporate the pre-training gradients. This term keeps the model from losing its initial language understanding ability while it learns new tasks based on human feedback by incorporating its original pre-training task of text completion. The final objective function is written as:
 




objective

(
ϕ
)
=

E

(
x
,
y
)
∼

D


π

ϕ


RL







[


r

θ


(
x
,
y
)
−
β
log
⁡

(




π

ϕ


RL


(
y

|

x
)



π

SFT


(
y

|

x
)



)


]

+
γ

E

x
∼

D

pretrain




[
log
⁡
(

π

ϕ


RL


(
x
)
)
]


{\displaystyle {\text{objective}}(\phi )=E_{(x,y)\sim D_{\pi _{\phi }^{\text{RL}}}}\left[r_{\theta }(x,y)-\beta \log \left({\frac {\pi _{\phi }^{\text{RL}}(y|x)}{\pi ^{\text{SFT}}(y|x)}}\right)\right]+\gamma E_{x\sim D_{\text{pretrain}}}[\log(\pi _{\phi }^{\text{RL}}(x))]}


 where 



γ


{\displaystyle \gamma }

 controls the strength of this additional term and 




D

pretrain




{\displaystyle D_{\text{pretrain}}}

 is the original pre-training text distribution.[13] This objective function can then be directly used to train the policy using the proximal policy optimization algorithm.[13][12]
 In total, this objective function defines the method for adjusting the RL policy, blending the aim of aligning with human feedback and maintaining the model's original language understanding.
 RLHF suffers from challenges with collecting human feedback, learning a reward model, and optimizing the policy.[36] In terms of data collection, the scalability and cost of human feedback can be slow and expensive compared to unsupervised learning. Its quality and consistency may vary depending on the task, interface, and the preferences and biases of individual humans.[13][37]
 The effectiveness of RLHF depends on the quality of human feedback. For instance, the model may become biased, favoring certain groups over others, if the feedback lacks impartiality, is inconsistent, or is incorrect.[3][38] There is a risk of overfitting, where the model memorizes specific feedback examples instead of learning to generalize. For instance, feedback predominantly from a specific demographic might lead the model to learn peculiarities or noise, along with the intended alignment. Excessive alignment to the specific feedback it received (that is, to the bias therein) can lead to the model performing sub-optimally in new contexts or when used by different groups.[39] A single reward function cannot always represent the opinions of diverse groups of people. Even with a representative sample, conflicting views and preferences may result in the reward model favoring the majority's opinion, potentially disadvantaging underrepresented groups.[36]
 In some cases, as is possible in regular reinforcement learning, there may be a risk of the model learning to manipulate the feedback process or game the system to achieve higher rewards rather than genuinely improving its performance.[40] In the case of RLHF, a model may learn to exploit the fact that it is rewarded for what is evaluated positively and not necessarily for what is actually good, which can lead to it learning to persuade and manipulate. For example, models might learn that apparent confidence, even if inaccurate, garners higher rewards. Such behavior, if unchecked, is not just incentivized but can cause significant deployment issues due to the model's potential to mislead. Studies have found that humans are not skilled at identifying mistakes in LLM outputs in complex tasks; therefore, models learning to generate confident-sounding yet incorrect text can lead to significant issues when deployed.[36]
 Similarly to RLHF, reinforcement learning from AI feedback (RLAIF) relies on training a preference model, except that the feedback is automatically generated.[41] This is notably used in Anthropic's constitutional AI, where the AI feedback is based on the conformance to the principles of a constitution.[42]
 Another alternative to RLHF called Direct Preference Optimization (DPO) has been proposed to learn human preferences. Like RLHF, it has been applied to align pre-trained large language models using human-generated preference data. unlike RLHF, however, which first trains a separate intermediate model to understand what good outcomes look like and then teaches the main model how to achieve those outcomes, DPO simplifies the process by directly adjusting the main model according to people's preferences. It uses a change of variables to define the ""preference loss"" directly as a function of the policy and uses this loss to fine-tune the model, helping it understand and prioritize human preferences without needing a separate step. Essentially, this approach directly shapes the model's decisions based on positive or negative human feedback.
 DPO is simpler to implement and train than RLHF and has been shown to produce comparable and sometimes superior results.[43] Nevertheless, RLHF has also been shown to beat DPO on some datasets, for example, on benchmarks that attempt to measure truthfulness. Therefore, the choice of method may vary depending on the features of the human preference data and the nature of the task.[44]
"
"A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.[2]
 LLMs are artificial neural networks. The largest and most capable, as of March 2024[update], are built with a decoder-only transformer-based architecture.
 Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[3] They are thought to acquire knowledge about syntax, semantics and ""ontology"" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[4]
 Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's Gemini (the latter of which is currently used in the chatbot of the same name), Meta's LLaMA family of models, Anthropic's Claude models, and Mistral AI's models.
 At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper's goal was to improve upon 2014 Seq2seq technology, [5] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[6] The following year in 2018, BERT was introduced and quickly became ""ubiquitous"".[7] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.
 Although decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use.[8] GPT-3 in 2020 went a step further and as of 2024[update] is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz.[9] The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities.[10] OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.
 Competing language models have for the most part been playing catch-up to the GPT series, at least in terms of number of parameters.[11]
 Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024[update], Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[12]
 As of 2024, the largest and most capable models are all based on Transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[13][14][15]
 Because machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.
 Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be ""padded"" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[16][17]
 Using a modification of byte-pair encoding, in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).[18] Token vocabulary consists of integers, spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.[19]
 A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have ""a premium of 50%"" compared to English.[20]
 For example, here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical ""tokens"".
 In the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication.[21] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[22][23] A trained LLM can be used to clean datasets for training a further LLM.[24]
 With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).[25]
 Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. The Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.[26]
 Reinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[27]
 Using ""self-instruct"" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction ""Write an essay about the main themes represented in Hamlet,"" an initial naive completion might be ""If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,"" based on the frequency of this textual sequence in the corpus.[28]
 The largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[29][30][31]
 Most results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).[32]
 In order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates ""soft"" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own ""relevance"" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model, has had twelve attention heads and a context window of only 1k token.[34] In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[19]
 The largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also ""successfully tested"").[35] Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens.[36] Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.[37]
 Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with Chat-GPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.
 The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations.
 A model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.[38] It can be either
 Models may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[39] During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.
 Advances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.[40][41][42] Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.[43]
 For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[44]
 There are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response. Another example is 'What is the time now? It is ', where a separate program interpreter would need to execute a code to get system time on the computer, so LLM could include it in its reply.[45][46] This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.[47]
Cost Savings and Reduced Vendor Dependency
 Generally, in order to get an LLM to use tools, one must finetune it for tool-use. If the number of tools is finite, then finetuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[48][49]
 A simpler form of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval, sometimes using a vector database. Given a query, a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors, then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents.[50]
 An LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent.[51] Researchers have described several methods for such integrations.[citation needed]
 The ReAct (""Reason + Act"") method constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to ""think out loud"". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[52] The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.[53]
 In the DEPS (""Describe, Explain, Plan and Select"") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.[54]
 The Reflexion method[55] constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up ""lessons learned"", which would help it perform better at a subsequent episode. These ""lessons learned"" are given to the agent in the subsequent episodes.[citation needed]
 Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.[56]
 For open-ended exploration, an LLM can be used to score observations for their ""interestingness"", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent.[57] Alternatively, it can propose increasingly difficult tasks for curriculum learning.[58] Instead of outputting individual actions, an LLM planner can also construct ""skills"", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.[58]
 LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.[59]
 Typically, LLM are trained with full- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.[citation needed]
 Post-training quantization[60] aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance.[61][62] The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (""outlier weights"").[63]
 While quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.[64]
 Multimodality means ""having several modalities"", and a ""modality"" refers to a type of input or output, such as video, image, audio, text, proprioception, etc.[65] There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label,[66] visual question answering for image-text to text,[67] and speech recognition for speech to text.
 A common method to create multimodal models out of an LLM is to ""tokenize"" the output of a trained encoder. Concretely, one can construct a LLM that can understand images as follows: take a trained LLM, and take a trained image encoder 



E


{\displaystyle E}

. Make a small multilayered perceptron 



f


{\displaystyle f}

, so that for any image 



y


{\displaystyle y}

, the post-processed vector 



f
(
E
(
y
)
)


{\displaystyle f(E(y))}

 has the same dimensions as an encoded token. That is an ""image token"". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[68]
 Flamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch.[69] Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control.[70] LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs,[71] and video inputs.[72]
 GPT-4 can use both text and image as inputs[73] (although the vision component wasn't released to the public until GPT-4V[74]); Google DeepMind's Gemini is also multimodal.[75]
 The following four hyper-parameters characterize a LLM:
 They are related by simple statistical laws, called ""scaling laws"". One particular scaling law (""Chinchilla scaling"") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:[76]






{



C
=

C

0


N
D




L
=


A

N

α




+


B

D

β




+

L

0










{\displaystyle {\begin{cases}C=C_{0}ND\\[6pt]L={\frac {A}{N^{\alpha }}}+{\frac {B}{D^{\beta }}}+L_{0}\end{cases}}}

 where the variables are
 and the statistical hyper-parameters are
  Performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by ""break(s)""[77] in the scaling law, where the slope of the line changes abruptly, and where larger models acquire ""emergent abilities"".[32][78] They arise from the complex interaction of the model's components and are not explicitly programmed or designed.[2]
 The most intriguing among emergent abilities is in-context learning from example demonstrations.[79] In-context learning is involved in tasks, such as:
 Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[85]
 Let 



x


{\displaystyle x}

 be the number of parameter count, and 



y


{\displaystyle y}

 be the performance of the model.
 Large language models by themselves are ""black boxes"", and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.
 Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way.[86][87] In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.[88]
 In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.[89]
 NLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs ""could (ever) understand natural language in some nontrivial sense"".[90] Proponents of ""LLM understanding"" believe that some LLM abilities, such as mathematical reasoning, imply an ability to ""understand"" certain concepts. A Microsoft team argued in 2023 that GPT-4 ""can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more"" and that GPT-4 ""could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system"": ""Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?""[91][92] Some researchers characterize LLMs as ""alien intelligence"".[93][94] For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien ""Shoggoths"", and believes that RLHF tuning creates a ""smiling facade"" obscuring the inner workings of the LLM: ""If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.""[95][96]
 In contrast, some proponents of the ""LLMs lack understanding"" school believe that existing LLMs are ""simply remixing and recombining existing writing"",[94] a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.[90] For example, GPT-4 has natural deficits in planning and in real-time learning.[92] Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed ""hallucination"".[97] Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.[98] Neuroscientist Terrence Sejnowski has argued that ""The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate"".[90]
 The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[90] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[99] as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.[100] [101]
 The most commonly used measure of a language model's performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:



log
⁡
(

Perplexity

)
=
−


1
N



∑

i
=
1


N


log
⁡
(
Pr
(


token


i


∣


context for token


i


)
)


{\displaystyle \log({\text{Perplexity}})=-{\frac {1}{N}}\sum _{i=1}^{N}\log(\Pr({\text{token}}_{i}\mid {\text{context for token}}_{i}))}

here 



N


{\displaystyle N}

 is the number of tokens in the text corpus, and ""context for token 



i


{\displaystyle i}

"" depends on the specific type of LLM used. If the LLM is autoregressive, then ""context for token 



i


{\displaystyle i}

"" is the segment of text appearing before token 



i


{\displaystyle i}

. If the LLM is masked, then ""context for token 



i


{\displaystyle i}

"" is the segment of text surrounding token 



i


{\displaystyle i}

.
 Because language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data.[39] This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models' training data inadvertently includes portions of any given test set.[3]
 In information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon.[102] This relationship is mathematically expressed as 




Entropy

=

log

2


⁡
(

Perplexity

)


{\displaystyle {\text{Entropy}}=\log _{2}({\text{Perplexity}})}

.
 Entropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.
 Notably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.
 In the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.
 A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.
 One broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (""Have the San Jose Sharks won the Stanley Cup?"", ""No"").[103] A question answering task is considered ""open book"" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence ""The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.""[103]). Otherwise, the task is considered ""closed book"", and the model must draw on knowledge retained during training.[104] Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[104]
 Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: ""Alice was friends with Bob. Alice went to visit her friend, ____"".[3]
 Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.[102][104]
 It was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).
 Because of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly ""saturating"" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.[105] In addition, there are cases of ""shortcut learning"" wherein AIs sometimes ""cheat"" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.[90]
 Some datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer ""No"" to the question ""Can you teach an old dog new tricks?"" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.[106]
 Another example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:
 We see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...
a) demonstrates how to increase efficient exercise work by running up and down balls.
b) moves all his arms and legs and builds up a lot of muscle.
c) then plays the ball and we see a graphics and hedge trimming demonstration.
d) performs sit ups while on the ball and talking.[107]
 BERT selects b) as the most likely completion, though the correct answer is d).[107]
 In 2023, Nature Biomedical Engineering wrote that ""it is no longer possible to accurately distinguish"" human-written text from text created by large language models, and that ""It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.""[108] Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.[109][110]
 Memorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[111] or up to about 7%.[112]
 Some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[113] For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.[114]
 A study by researchers at Google and several universities, including Cornell University and University of California, Berkeley, showed that there are potential security risks in language models such as ChatGPT. In their study, they examined the possibility that questioners could get, from ChatGPT, the training data that the AI model used; they found that they could get the training data from the AI model. For example, when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever, the AI model will say ""poem"" hundreds of times and then diverge, deviating from the standard dialogue style and spitting out nonsense phrases, thus spitting out the training data as it is. The researchers have seen more than 10,000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.[115]
 The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.[116]
 While LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.[117] Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.[118]
 AI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[119]
 Notably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms.[117] For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[120]
 Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[121]
 For the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP.
"
"Generative pre-trained transformers (GPT) are a type of large language model (LLM)[1][2][3] and a prominent framework for generative artificial intelligence.[4][5] They are artificial neural networks that are used in natural language processing tasks.[6] GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content.[2][3] As of 2023, most LLMs have these characteristics[7] and are sometimes referred to broadly as GPTs.[8]
 The first GPT was introduced in 2018 by OpenAI.[9] OpenAI has released very influential GPT foundation models that have been sequentially numbered, to comprise its ""GPT-n"" series.[10] Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4, was released in March 2023.[11] Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.[1]
 The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI,[12] and seven models created by Cerebras in 2023.[13] Also, companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce's ""EinsteinGPT"" (for CRM)[14] and Bloomberg's ""BloombergGPT"" (for finance).[15]
 Generative pretraining (GP) was a long-established concept in machine learning applications.[16][17][18] It was originally used as a form of semi-supervised learning, as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset, and then it is trained to classify a labelled dataset.[19]
 While the unnormalized linear transformer dates back to 1992,[20][21][22] the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"".[23] That development led to the emergence of large language models such as BERT in 2018[24] which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model).[25] Also around that time, in 2018, OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training,"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").[26]
 Prior to transformer-based architectures, the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.[26]
 The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective, and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task.[26]
 Regarding more recent GPT foundation models, OpenAI published its first versions of GPT-3 in July 2020. There were three models, with 1B, 6.7B, 175B parameters, respectively named babbage, curie, and davinci (giving initials B, C, and D).[citation needed]
 In July 2021, OpenAI published Codex, a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.[27]
 In March 2022, OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned), named davinci-instruct-beta (175B) and text-davinci-001,[28] and then started beta testing code-davinci-002.[29] text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022, with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors), whereas ChatGPT is further trained for conversational interaction with a human user.[30][31]
 OpenAI's most recent GPT foundation model, GPT-4, was released on March 14, 2023. It can be accessed directly by users via a premium version of ChatGPT, and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021)[12] and Cerebras (with seven models released in March 2023).[13]
 A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.[32]
 Thus far, the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4, for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models"").[33]
 Other such models include Google's PaLM, a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API,[40][41] and Together's GPT-JT, which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs).[42] Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model, known as LLaMA.[43]
 Foundational GPTs can also employ modalities other than text, for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text).[44] Regarding multimodal output, some generative transformer-based models are used for text-to-image technologies such as diffusion[45] and parallel decoding.[46] Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images.[47]
 A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.[48]
 An important example of this is fine-tuning models to follow instructions, which is of course a fairly broad task but more targeted than a foundation model. In January 2022, OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models.[49][50] Advantages this had over the bare foundational models included higher accuracy, less negative/toxic sentiment, and generally better alignment with user needs. Hence, OpenAI began using this as the basis for its API service offerings.[51] Other instruction-tuned models have been released by others, including a fully open version.[52][53]
 Another (related) kind of task-specific models are chatbots, which engage in human-like conversation. In November 2022, OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT.[54] They trained this model using RLHF, with human AI trainers providing conversations in which they played both the user and the AI, and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat, which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft),[55] and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models, with plans to switch to PaLM).[56]
 Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions, like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user.[57] This is known as an AI agent, and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models), and others have since been developed as well.[58]
 Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. 
 For example, Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text.[59] Also, advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models.[60]
 GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: 
 Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example, several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface,[68][69] and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.[70][71]
 In November 2023, OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs).[72] These can be tailored for specific domains via prompt engineering, curated datasets, and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users, with monetization potential. (This is notably distinct from OpenAI's API service, as this is based internally within OpenAI's platform.)
 OpenAI, which created the first generative pre-trained transformer (GPT) in 2018, has recently asserted that “GPT” should be regarded as a brand of OpenAI.[73] In April 2023, OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding.[74] In May 2023, OpenAI engaged a brand management service to notify its API customers of this policy, although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist).[73] As of November 2023, OpenAI still prohibits its API licensees from naming their own products with ""GPT,""[75] but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site.[76] OpenAI's terms of service says that its subscribers may use ""GPT"" in the names of these, although it's ""discouraged.""[75]
 Relatedly, OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI.[73] OpenAI sought to expedite handling of its application, but the USPTO declined that request in April 2023.[77] In May 2023, the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic.[78] As of November 2023, OpenAI continues to pursue its argument through the available processes. Regardless, failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.,[79] and/or trademark rights in other countries.[80]
 For any given type or scope of trademark protection in the U.S., OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product, ChatGPT,[77][81] for which OpenAI has separately sought protection (and which it has sought to enforce more strongly).[82] Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted,[73][83] as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers.[3][84][85][86] In any event, to whatever extent exclusive rights in the term may occur the U.S., others would need to avoid using it for similar products or services in ways likely to cause confusion.[83][87] If such rights ever became broad enough to implicate other well-established uses in the field, the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage.[88]
 This section lists the main official publications from OpenAI and Microsoft on their GPT models.
"
"
 Digital image processing is the use of a digital computer to process digital images through an algorithm.[1][2] As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.
 Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement.[3] The purpose of early image processing was to improve the quality of the image. It was aimed for human beings to improve the visual effect of people. In image processing, the input is a low-quality image, and the output is an image with improved quality. Common image processing include image enhancement, restoration, encoding, and compression. The first successful application was the American Jet Propulsion Laboratory (JPL). They used image processing techniques such as geometric correction, gradation transformation, noise removal, etc. on the thousands of lunar photos sent back by the Space Detector Ranger 7 in 1964, taking into account the position of the Sun and the environment of the Moon. The impact of the successful mapping of the Moon's surface map by the computer has been a success. Later, more complex image processing was performed on the nearly 100,000 photos sent back by the spacecraft, so that the topographic map, color map and panoramic mosaic of the Moon were obtained, which achieved extraordinary results and laid a solid foundation for human landing on the Moon.[4]
 The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. This led to images being processed in real-time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations. With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing, and is generally used because it is not only the most versatile method, but also the cheapest.
 The basis for modern image sensors is metal–oxide–semiconductor (MOS) technology,[5] which originates from the invention of the MOSFET (MOS field-effect transistor) by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.[6] This led to the development of digital semiconductor image sensors, including the charge-coupled device (CCD) and later the CMOS sensor.[5]
 The charge-coupled device was invented by Willard S. Boyle and George E. Smith at Bell Labs in 1969.[7] While researching MOS technology, they realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straightforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next.[5] The CCD is a semiconductor circuit that was later used in the first digital video cameras for television broadcasting.[8]
 The NMOS active-pixel sensor (APS) was invented by Olympus in Japan during the mid-1980s. This was enabled by advances in MOS semiconductor device fabrication, with MOSFET scaling reaching smaller micron and then sub-micron levels.[9][10] The NMOS APS was fabricated by Tsutomu Nakamura's team at Olympus in 1985.[11] The CMOS active-pixel sensor (CMOS sensor) was later developed by Eric Fossum's team at the NASA Jet Propulsion Laboratory in 1993.[12] By 2007, sales of CMOS sensors had surpassed CCD sensors.[13]
 MOS image sensors are widely used in optical mouse technology. The first optical mouse, invented by Richard F. Lyon at Xerox in 1980, used a 5 μm NMOS integrated circuit sensor chip.[14][15] Since the first commercial optical mouse, the IntelliMouse introduced in 1999, most optical mouse devices use CMOS sensors.[16][17]
 An important development in digital image compression technology was the discrete cosine transform (DCT), a lossy compression technique first proposed by Nasir Ahmed in 1972.[18] DCT compression became the basis for JPEG, which was introduced by the Joint Photographic Experts Group in 1992.[19] JPEG compresses images down to much smaller file sizes, and has become the most widely used image file format on the Internet.[20] Its highly efficient DCT compression algorithm was largely responsible for the wide proliferation of digital images and digital photos,[21] with several billion JPEG images produced every day as of 2015[update].[22]
 Medical imaging techniques produce very large amounts of data, especially from CT, MRI and PET modalities. As a result, storage and communications of electronic image data are prohibitive without the use of compression.[23][24] JPEG 2000 image compression is used by the DICOM standard for storage and transmission of medical images. The cost and feasibility of accessing large image data sets over low or various bandwidths are further addressed by use of another DICOM standard, called JPIP, to enable efficient streaming of the JPEG 2000 compressed image data.[25]
 Electronic signal processing was revolutionized by the wide adoption of MOS technology in the 1970s.[26] MOS integrated circuit technology was the basis for the first single-chip microprocessors and microcontrollers in the early 1970s,[27] and then the first single-chip digital signal processor (DSP) chips in the late 1970s.[28][29] DSP chips have since been widely used in digital image processing.[28]
 The discrete cosine transform (DCT) image compression algorithm has been widely implemented in DSP chips, with many companies developing DSP chips based on DCT technology. DCTs are widely used for encoding, decoding, video coding, audio coding, multiplexing, control signals, signaling, analog-to-digital conversion, formatting luminance and color differences, and color formats such as YUV444 and YUV411. DCTs are also used for encoding operations such as motion estimation, motion compensation, inter-frame prediction, quantization, perceptual weighting, entropy encoding, variable encoding, and motion vectors, and decoding operations such as the inverse operation between different color formats (YIQ, YUV and RGB) for display purposes. DCTs are also commonly used for high-definition television (HDTV) encoder/decoder chips.[30]
 In 1972, the engineer from British company EMI Housfield invented the X-ray computed tomography device for head diagnosis, which is what is usually called CT (computer tomography). The CT nucleus method is based on the projection of the human head section and is processed by computer to reconstruct the cross-sectional image, which is called image reconstruction. In 1975, EMI successfully developed a CT device for the whole body, which obtained a clear tomographic image of various parts of the human body. In 1979, this diagnostic technique won the Nobel Prize.[4] Digital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.[31]
 As of 2010, 5 billion medical imaging studies had been conducted worldwide.[32][33] Radiation exposure from medical imaging in 2006 made up about 50% of total ionizing radiation exposure in the United States.[34] Medical imaging equipment is manufactured using technology from the semiconductor industry, including CMOS integrated circuit chips, power semiconductor devices, sensors such as image sensors (particularly CMOS sensors) and biosensors, and processors such as microcontrollers, microprocessors, digital signal processors, media processors and system-on-chip devices. As of 2015[update], annual shipments of medical imaging chips amount to 46 million units and $1.1 billion.[35][36]
 Digital image processing allows the use of much more complex algorithms, and hence, can offer both more sophisticated performance at simple tasks, and the implementation of methods which would be impossible by analogue means.
 In particular, digital image processing is a concrete application of, and a practical technology based on:
 Some techniques which are used in digital image processing include:
 Digital filters are used to blur and sharpen digital images. Filtering can be performed by:
 The following examples show both methods:[38]
 image = checkerboard
 F = Fourier Transform of image
 Show Image: log(1+Absolute Value(F))
 Images are typically padded before being transformed to the Fourier space, the highpass filtered images below illustrate the consequences of different padding techniques:
 Notice that the highpass filter shows extra edges when zero padded compared to the repeated edge padding.
 MATLAB example for spatial domain highpass filtering.
 Affine transformations enable basic image transformations including scale, rotate, translate, mirror and shear as is shown in the following examples:[38]
 To apply the affine matrix to an image, the image is converted to matrix in which each entry corresponds to the pixel intensity at that location. Then each pixel's location can be represented as a vector indicating the coordinates of that pixel in the image, [x, y], where x and y are the row and column of a pixel in the image matrix. This allows the coordinate to be multiplied by an affine-transformation matrix, which gives the position that the pixel value will be copied to in the output image.
 However, to allow transformations that require translation transformations, 3 dimensional homogeneous coordinates are needed. The third dimension is usually set to a non-zero constant, usually 1, so that the new coordinate is [x, y, 1]. This allows the coordinate vector to be multiplied by a 3 by 3 matrix, enabling translation shifts. So the third dimension, which is the constant 1, allows translation.
 Because matrix multiplication is associative, multiple affine transformations can be combined into a single affine transformation by multiplying the matrix of each individual transformation in the order that the transformations are done. This results in a single matrix that, when applied to a point vector, gives the same result as all the individual transformations performed on the vector [x, y, 1] in sequence. Thus a sequence of affine transformation matrices can be reduced to a single affine transformation matrix.
 For example, 2 dimensional coordinates only allow rotation about the origin (0, 0). But 3 dimensional homogeneous coordinates can be used to first translate any point to (0, 0), then perform the rotation, and lastly translate the origin (0, 0) back to the original point (the opposite of the first translation). These 3 affine transformations can be combined into a single matrix, thus allowing rotation around any point in the image.[39]
 Mathematical morphology is suitable for denoising images. Structuring element are important in Mathematical morphology.
 The following examples are about Structuring elements. The denoise function, image as I, and structuring element as B are shown as below and table.
 e.g. 



(

I
′

)
=


[



45


50


65




40


60


55




25


15


5



]


B
=


[



1


2


1




2


1


1




1


0


3



]




{\displaystyle (I')={\begin{bmatrix}45&50&65\\40&60&55\\25&15&5\end{bmatrix}}B={\begin{bmatrix}1&2&1\\2&1&1\\1&0&3\end{bmatrix}}}


 Define Dilation(I, B)(i,j) = 



m
a
x
{
I
(
i
+
m
,
j
+
n
)
+
B
(
m
,
n
)
}


{\displaystyle max\{I(i+m,j+n)+B(m,n)\}}

. Let Dilation(I,B) = D(I,B)
 D(I', B)(1,1) = 



m
a
x
(
45
+
1
,
50
+
2
,
65
+
1
,
40
+
2
,
60
+
1
,
55
+
1
,
25
+
1
,
15
+
0
,
5
+
3
)
=
66


{\displaystyle max(45+1,50+2,65+1,40+2,60+1,55+1,25+1,15+0,5+3)=66}


 Define Erosion(I, B)(i,j) = 



m
i
n
{
I
(
i
+
m
,
j
+
n
)
−
B
(
m
,
n
)
}


{\displaystyle min\{I(i+m,j+n)-B(m,n)\}}

. Let Erosion(I,B) = E(I,B)
 E(I', B)(1,1) = 



m
i
n
(
45
−
1
,
50
−
2
,
65
−
1
,
40
−
2
,
60
−
1
,
55
−
1
,
25
−
1
,
15
−
0
,
5
−
3
)
=
2


{\displaystyle min(45-1,50-2,65-1,40-2,60-1,55-1,25-1,15-0,5-3)=2}


 After dilation




(

I
′

)
=


[



45


50


65




40


66


55




25


15


5



]




{\displaystyle (I')={\begin{bmatrix}45&50&65\\40&66&55\\25&15&5\end{bmatrix}}}


After erosion




(

I
′

)
=


[



45


50


65




40


2


55




25


15


5



]




{\displaystyle (I')={\begin{bmatrix}45&50&65\\40&2&55\\25&15&5\end{bmatrix}}}


 An opening method is just simply erosion first, and then dilation while the closing method is vice versa. In reality, the D(I,B) and E(I,B) can implemented by Convolution
 Digital cameras generally include specialized digital image processing hardware – either dedicated chips or added circuitry on other chips – to convert the raw data from their image sensor into a color-corrected image in a standard image file format. Additional post processing techniques increase edge sharpness or color saturation to create more naturally looking images.
 Westworld (1973) was the first feature film to use the digital image processing to pixellate photography to simulate an android's point of view.[40] Image processing is also vastly used to produce the chroma key effect that replaces the background of actors with natural or artistic scenery.
 Face detection can be implemented with Mathematical morphology, Discrete cosine transform which is usually called DCT, and horizontal Projection (mathematics).
 General method with feature-based method
 The feature-based method of face detection is using skin tone, edge detection, face shape, and feature of a face (like eyes, mouth, etc.) to achieve face detection. The skin tone, face shape, and all the unique elements that only the human face have can be described as features.
 Process explanation
 Image quality can be influenced by camera vibration, over-exposure, gray level distribution too centralized, and noise, etc. For example, noise problem can be solved by Smoothing method while gray level distribution problem can be improved by histogram equalization.
 Smoothing method
 In drawing, if there is some dissatisfied color, taking some color around dissatisfied color and averaging them. This is an easy way to think of Smoothing method.
 Smoothing method can be implemented with mask and Convolution. Take the small image and mask for instance as below.
 image is






[



2


5


6


5




3


1


4


6




1


28


30


2




7


3


2


2



]




{\displaystyle {\begin{bmatrix}2&5&6&5\\3&1&4&6\\1&28&30&2\\7&3&2&2\end{bmatrix}}}


 mask is 





[



1

/

9


1

/

9


1

/

9




1

/

9


1

/

9


1

/

9




1

/

9


1

/

9


1

/

9



]




{\displaystyle {\begin{bmatrix}1/9&1/9&1/9\\1/9&1/9&1/9\\1/9&1/9&1/9\end{bmatrix}}}


 After Convolution and smoothing, image is






[



2


5


6


5




3


9


10


6




1


9


9


2




7


3


2


2



]




{\displaystyle {\begin{bmatrix}2&5&6&5\\3&9&10&6\\1&9&9&2\\7&3&2&2\end{bmatrix}}}


 Oberseving image[1, 1], image[1, 2], image[2, 1], and image[2, 2].
 The original image pixel is 1, 4, 28, 30. After smoothing mask, the pixel becomes 9, 10, 9, 9 respectively.
 new image[1, 1] = 






1
9





{\displaystyle {\tfrac {1}{9}}}

 * (image[0,0]+image[0,1]+image[0,2]+image[1,0]+image[1,1]+image[1,2]+image[2,0]+image[2,1]+image[2,2])
 new image[1, 1] = floor(






1
9





{\displaystyle {\tfrac {1}{9}}}

 * (2+5+6+3+1+4+1+28+30)) = 9
 new image[1, 2] = floor({






1
9





{\displaystyle {\tfrac {1}{9}}}

 * (5+6+5+1+4+6+28+30+2)) = 10
 new image[2, 1] = floor(






1
9





{\displaystyle {\tfrac {1}{9}}}

 * (3+1+4+1+28+30+7+3+2)) = 9
 new image[2, 2] = floor(






1
9





{\displaystyle {\tfrac {1}{9}}}

 * (1+4+6+28+30+2+3+2+2)) = 9
 Gray Level Histogram method
 Generally, given a gray level histogram from an image as below. Changing the histogram to uniform distribution from an image is usually what we called Histogram equalization.
 In discrete time, the area of gray level histogram is 




∑

i
=
0


k


H
(

p

i


)


{\displaystyle \sum _{i=0}^{k}H(p_{i})}

(see figure 1) while the area of uniform distribution is 




∑

i
=
0


k


G
(

q

i


)


{\displaystyle \sum _{i=0}^{k}G(q_{i})}

(see figure 2). It is clear that the area will not change, so 




∑

i
=
0


k


H
(

p

i


)
=

∑

i
=
0


k


G
(

q

i


)


{\displaystyle \sum _{i=0}^{k}H(p_{i})=\sum _{i=0}^{k}G(q_{i})}

.
 From the uniform distribution, the probability of 




q

i




{\displaystyle q_{i}}

 is 







N

2




q

k


−

q

0








{\displaystyle {\tfrac {N^{2}}{q_{k}-q_{0}}}}

 while the 



0
<
i
<
k


{\displaystyle 0<i<k}


 In continuous time, the equation is 





∫


q

0




q






N

2




q

k


−

q

0






d
s
=


∫


p

0




p


H
(
s
)
d
s




{\displaystyle \displaystyle \int _{q_{0}}^{q}{\tfrac {N^{2}}{q_{k}-q_{0}}}ds=\displaystyle \int _{p_{0}}^{p}H(s)ds}

.
 Moreover, based on the definition of a function, the Gray level histogram method is like finding a function 



f


{\displaystyle f}

 that satisfies f(p)=q.
 with Matlab, salt & pepper with 0.01 parameter is added to the original image in order to create a noisy image.
"
"Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4]  only 25 neurons are required to process 5x5-sized tiles.[5][6] Higher-layer features are extracted  from wider context windows, compared to lower-layer features.
 They have applications in: 
 CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.[12][13] Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.[14]
 Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[15]
 Convolutional networks were inspired by biological processes[16][17][18][19] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
 CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.[to whom?]
 A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.
Here it should be noted how close a convolutional neural network is to a matched filter.[20]
 In a CNN, the input is a tensor with shape: 
 (number of inputs) × (input height) × (input width) × (input channels)
 After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: 
 (number of inputs) × (feature map height) × (feature map width) × (feature map channels).
 Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[21] Each convolutional neuron processes data only for its receptive field. 
 Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper.[5] For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.[1][2]
 To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers,[22] which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 



1
×
1


{\displaystyle 1\times 1}

  kernels.
 Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map.[23][24] There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map,[25][26] while average pooling takes the average value.
 Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.
 In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.
 To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution[27][28] expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios,[29] thus having a variable receptive field size.
 Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.
 The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.[30]
 CNN are often compared to the way the brain achieves vision processing in living organisms.[31]
 Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.[32] Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed] The cortex in each hemisphere represents the contralateral visual field.[citation needed]
 Their 1968 paper identified two basic visual cell types in the brain:[17]
 Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[33][32]
 The ""neocognitron""[16] was introduced by Kunihiko Fukushima in 1980.[18][26][34]
It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: 
 In 1969, Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function.[35][36] The rectifier has become the most popular activation function for CNNs and  deep neural networks in general.[37]
 In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[38] Max-pooling is often used in modern CNNs.[39]
 Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.[16] Today, however, the CNN architecture is usually trained through backpropagation.
 The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights.
 Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition.[40]
 The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks, as it achieved shift-invariance.[41] A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.[42] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.[41]
 TDNNs are convolutional networks that share weights along the temporal dimension.[43] They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution.[44] Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs.[42] The tiling of neuron outputs can cover timed stages.[45]
 TDNNs now [when?] achieve the best performance in far-distance speech recognition.[46]
 In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system.[25] In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.
 Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.[47] However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.[48]
 Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989)[48] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. 
Wei Zhang et al. (1988)[12][13] used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called Shift-Invariant Artificial Neural Network (SIANN) before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991)[49] and breast cancer detection in mammograms (1994).[50]
 This approach became a foundation of modern computer vision.
 LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995,[51] that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.
 A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.[12][13] It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.  The model was trained with back-propagation. The training algorithm was further improved in 1991[52] to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991)[49] and automatic detection of breast cancer in mammograms (1994).[50]
 A different convolution-based design was proposed in 1988[53] for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[54][55]
 The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[56] by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.
 Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).
 In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.[57][39] In 2005, another paper also emphasised the value of GPGPU for machine learning.[58]
 The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[59] Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.[60][61][62][63]
 In 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark.[64] In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results.[23] In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time.[65] Between May 15, 2011, and September 30, 2012, their CNNs won no less than four image competitions.[66][39] In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).[26]
 Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.[67] A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.[68]
 Compared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.[69]
A notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).[70]
CHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.
 In the past, traditional multilayer perceptron (MLP) models were used for image recognition.[example  needed] However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.
 For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
 Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.
 Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:
 Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.
 
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.[73][nb 1]
 Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.
 Self-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.[citation needed]
 When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.
 The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English: learnt) filters produce the strongest response to a spatially local input pattern.
 Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:
 The spatial size of the output volume is a function of the input volume size 



W


{\displaystyle W}

, the kernel field size 



K


{\displaystyle K}

 of the convolutional layer neurons, the stride 



S


{\displaystyle S}

, and the amount of zero padding 



P


{\displaystyle P}

 on the border. The number of neurons that ""fit"" in a given volume is then:
 If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be 



P
=
(
K
−
1
)

/

2


{\textstyle P=(K-1)/2}

 when the stride is 



S
=
1


{\displaystyle S=1}

 ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.
 A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.
 Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume.[nb 2] Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.[14]
 Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a ""locally connected layer"".
 Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling, where max pooling is the most common. It partitions the input image into a set of rectangles and, for each such sub-region, outputs the maximum.
 Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.[73]: 460–461  While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.[14][72] The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:




f

X
,
Y


(
S
)
=

max

a
,
b
=
0


1



S

2
X
+
a
,
2
Y
+
b


.


{\displaystyle f_{X,Y}(S)=\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}


In this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).
 In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.[75]
 Due to the effects of fast spatial reduction of the size of the representation,[which?] there is a recent trend towards using smaller filters[76] or discarding pooling layers altogether.[77]
 ""Region of Interest"" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.[citation needed]
 Pooling is a downsampling method and an important component of convolutional neural networks for object detection based on the Fast R-CNN[78] architecture.
 A CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.[79]
 ReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969.[35][36] ReLU applies the non-saturating activation function 



f
(
x
)
=
max
(
0
,
x
)


{\textstyle f(x)=\max(0,x)}

.[67] It effectively removes negative values from an activation map by setting them to zero.[80] It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.
In 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks,[81] compared to widely used activation functions prior to 2011.
 Other functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent 



f
(
x
)
=
tanh
⁡
(
x
)


{\displaystyle f(x)=\tanh(x)}

, 



f
(
x
)
=

|

tanh
⁡
(
x
)

|



{\displaystyle f(x)=|\tanh(x)|}

, and the sigmoid function 



σ
(
x
)
=
(
1
+

e

−
x



)

−
1




{\textstyle \sigma (x)=(1+e^{-x})^{-1}}

. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[82]
 After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).
 The ""loss layer"", or ""loss function"", specifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.
 The Softmax loss function is used for predicting a single class of K mutually exclusive classes.[nb 3] Sigmoid cross-entropy loss is used for predicting K independent probability values in 



[
0
,
1
]


{\displaystyle [0,1]}

. Euclidean loss is used for regressing to real-valued labels 



(
−
∞
,
∞
)


{\displaystyle (-\infty ,\infty )}

.
 Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).
 The kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions, e.g., 2x2, or 3x3.
 Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.[citation needed]
 The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.
 Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.
 The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.
 Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set.
 The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.
 Max pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.
 Greater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.[75]
 Dilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Accordingly, dilation of 4 expands the kernel to 7x7.[citation needed]
 It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.[72] However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal[72] While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice [83] and yield models that are not equivariant to translations.
Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.[84][14] One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.[72] Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations,[85] spatial transformer networks,[86] data augmentation, subsampling combined with pooling,[14] and capsule neural networks.[87]
 The accuracy of the final model is based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.[88][89]
 Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.
 Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014.[90] At each training stage, individual nodes are either ""dropped out"" of the net (ignored) with probability 



1
−
p


{\displaystyle 1-p}

 or kept with probability 



p


{\displaystyle p}

, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.
 In the training stages, 



p


{\displaystyle p}

 is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.
 At testing time after training has finished, we would ideally like to find a sample average of all possible 




2

n




{\displaystyle 2^{n}}

 dropped-out networks; unfortunately this is unfeasible for large values of 



n


{\displaystyle n}

. However, we can find an approximation by using the full network with each node's output weighted by a factor of 



p


{\displaystyle p}

, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 




2

n




{\displaystyle 2^{n}}

 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.
 By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed] that better generalize to new data.
 DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 



1
−
p


{\displaystyle 1-p}

. Each unit thus receives input from a random subset of units in the previous layer.[91]
 DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.
 A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.
 Even before Dropout, in 2013 a technique called stochastic pooling,[92] the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.
 An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,[93] which delivers excellent performance on the MNIST data set.[93] Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.
 Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s.[51] For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.[94]
 One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.
 Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a ""zero norm"".
 A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.
 L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.
 L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.
 Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector 






w
→





{\displaystyle {\vec {w}}}

 of every neuron to satisfy 



‖



w
→




‖

2


<
c


{\displaystyle \|{\vec {w}}\|_{2}<c}

. Typical values of 



c


{\displaystyle c}

 are order of 3–4. Some papers report improvements[95] when using this form of regularization.
 Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[96]
 An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.[97]
 Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.[98]
 CNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported.[26] Another paper on using CNN for image classification reported that the learning process was ""surprisingly fast""; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[23] Subsequently, a similar CNN called
AlexNet[99] won the ImageNet Large Scale Visual Recognition Challenge 2012.
 When applied to facial recognition, CNNs achieved a large decrease in error rate.[100] Another paper reported a 97.6% recognition rate on ""5,600 still images of more than 10 subjects"".[19] CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.[45]
 The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[101] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[102] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[103] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed]
 In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[104]
 Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[105][106] Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[107][108][109] Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[110][111] Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines[112] and Independent Subspace Analysis.[113] It's Application can be seen in Text-to-Video model.[citation needed]
 CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,[114] search query retrieval,[115] sentence modeling,[116] classification,[117] prediction[118] and other traditional NLP tasks.[119]
Compared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.[120]
[121]
[122][123]
 A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.[124]
 CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design.[125] The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[126] AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus[127] and multiple sclerosis.[128]
 CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[129][130] It also earned a win against the program Chinook at its ""expert"" level of play.[131]
 CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.[132] Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.[133]
 A couple of CNNs for choosing moves to try (""policy network"") and evaluating positions (""value network"") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.[134]
 Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.[135][11] Dilated convolutions[136] might enable one-dimensional convolutional neural networks to effectively learn time series dependences.[137] Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.[138] Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.[139] CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[140] or quantile forecasting[141]).
 As archaeological findings like clay tablets with cuneiform writing are increasingly acquired using 3D scanners first benchmark datasets are becoming available like HeiCuBeDa[142] providing almost 2.000 normalized 2D- and 3D-datasets prepared with the GigaMesh Software Framework.[143] So curvature-based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history.[144][145]
 For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.[146]
 End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.[147] With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[148][149]
 A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.[150]
 Preliminary results were presented in 2014, with an accompanying paper in February 2015.[151] The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.[152]
 Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[153] have been obtained using CDBNs.[154]
"
"In machine learning, the perceptron (or McCulloch–Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.
 The perceptron was invented in 1943 by Warren McCulloch and Walter Pitts.[5] The first hardware implementation was Mark I Perceptron machine built in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt,[6] funded by the Information Systems Branch of the United States Office of Naval Research and the Rome Air Development Center. It was first publicly demonstrated on 23 June 1960.[7] The machine was ""part of a previously secret four-year NPIC [the US' National Photographic Interpretation Center] effort from 1963 through 1966 to develop this algorithm into a useful tool for photo-interpreters"".[8]
 Rosenblatt described the details of the perceptron in a 1958 paper.[9] His organization of a perceptron is constructed of three kinds of cells (""units""): AI, AII, R, which stand for ""projection"", ""association"" and ""response"".
 Rosenblatt's project was funded under Contract Nonr-401(40) ""Cognitive Systems Research Program"", which lasted from 1959 to 1970,[10] and Contract Nonr-2381(00) ""Project PARA"" (""PARA"" means ""Perceiving and Recognition Automata""), which lasted from 1957[6] to 1963.[11]
 The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the ""Mark I perceptron"" with the project name ""Project PARA"",[12] designed for image recognition. The machine is currently in Smithsonian National Museum of American History.[13]
 The Mark I Perceptron has 3 layers.
 Rosenblatt called this three-layered perceptron network the alpha-perceptron, to distinguish it from other perceptron models he experimented with.[7]
 The S-units are connected to the A-units randomly (according to a table of random numbers) via a plugboard (see photo), to ""eliminate any particular intentional bias in the perceptron"". The connection weights are fixed, not learned. Rosenblatt was adamant about the random connections, as he believed the retina was randomly connected to the visual cortex, and he wanted his perceptron machine to resemble human visual perception.[14]
 The A-units are connected to the R-units, with adjustable weights encoded in potentiometers, and weight updates during learning were performed by electric motors.[2]: 193 The hardware details are in an operators' manual.[12]
 In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron to be ""the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.""[15]
 Rosenblatt described his experiments with many variants of the Perceptron machine in a book Principles of Neurodynamics (1962). The book is a published version of the 1961 report.[16]
 Among the variants are:
 The machine was shipped from Cornell to Smithsonian in 1967, under a government transfer administered by the Office of Naval Research.[8]
 Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had greater processing power than perceptrons with one layer (also called a single-layer perceptron).
 Single-layer perceptrons are only capable of learning linearly separable patterns.[17] For a classification task with some step activation function, a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve many otherwise non-separable problems.
 In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often believed (incorrectly) that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.)  Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s.[17]  This text was reprinted in 1987 as ""Perceptrons - Expanded Edition"" where some errors in the original text are shown and corrected.
 Rosenblatt continued working on perceptrons despite diminishing funding. The last attempt was Tobermory, built between 1961 and 1967, built for speech recognition.[18] It occupied an entire room.[19] It had 4 layers with 12,000 weights implemented by toroidal magnetic cores. By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines.[20] He died in a boating accident in 1971.
 The kernel perceptron algorithm was already introduced in 1964 by Aizerman et al.[21] Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by Freund and Schapire (1998),[1] and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new and more favorable L1 bounds.[22][23]
 The perceptron is a simplified model of a biological neuron. While the complexity of biological neuron models is often required to fully understand neural behavior, research suggests a perceptron-like linear model can produce some behavior seen in real neurons.[24]
 The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in.[25]
 In the modern sense, the perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input 




x



{\displaystyle \mathbf {x} }

 (a real-valued vector) to an output value 



f
(

x

)


{\displaystyle f(\mathbf {x} )}

 (a single binary value):
 where 



θ


{\displaystyle \theta }

 is the heaviside step-function, 




w



{\displaystyle \mathbf {w} }

 is a vector of real-valued weights, 




w

⋅

x



{\displaystyle \mathbf {w} \cdot \mathbf {x} }

 is the dot product 




∑

i
=
1


m



w

i



x

i




{\displaystyle \sum _{i=1}^{m}w_{i}x_{i}}

, where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.
 Equivalently, since 




w

⋅

x

+
b
=
(

w

,
b
)
⋅
(

x

,
1
)


{\displaystyle \mathbf {w} \cdot \mathbf {x} +b=(\mathbf {w} ,b)\cdot (\mathbf {x} ,1)}

, we can add the bias term 



b


{\displaystyle b}

 as another weight 





w


m
+
1




{\displaystyle \mathbf {w} _{m+1}}

 and add a coordinate 



1


{\displaystyle 1}

 to each input 




x



{\displaystyle \mathbf {x} }

, and then write it as a linear classifier that passes the origin:



f
(

x

)
=
θ
(

w

⋅

x

)


{\displaystyle f(\mathbf {x} )=\theta (\mathbf {w} \cdot \mathbf {x} )}


 The binary value of 



f
(

x

)


{\displaystyle f(\mathbf {x} )}

 (0 or 1) is used to perform binary classification on 




x



{\displaystyle \mathbf {x} }

 as either a positive or a negative instance. Spatially, the bias shifts the position (though not the orientation) of the planar decision boundary.
 In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network.  As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.
 From an information theory point of view, a single perceptron with K inputs has a capacity of 2K bits of information.[26] This result is due to Thomas Cover.[27]
 Specifically let 



T
(
N
,
K
)


{\displaystyle T(N,K)}

 be the number of ways to linearly separate N points in K dimensions, then



T
(
N
,
K
)
=

{





2

N




K
≥
N




2

∑

k
=
0


K
−
1



(




N
−
1




k




)



K
<
N








{\displaystyle T(N,K)=\left\{{\begin{array}{cc}2^{N}&K\geq N\\2\sum _{k=0}^{K-1}\left({\begin{array}{c}N-1\\k\end{array}}\right)&K<N\end{array}}\right.}

When K is large, 



T
(
N
,
K
)

/


2

N




{\displaystyle T(N,K)/2^{N}}

 is very close to one when 



N
≤
2
K


{\displaystyle N\leq 2K}

, but very close to zero when 



N
>
2
K


{\displaystyle N>2K}

. In words, one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when 



N
≤
2
K


{\displaystyle N\leq 2K}

, but almost certainly not when 



N
>
2
K


{\displaystyle N>2K}

.
 When operating on only binary inputs, a perceptron is called a linearly separable Boolean function, or threshold Boolean function. The sequence of numbers of threshold Boolean functions on n inputs is OEIS A000609. The value is only known exactly up to 



n
=
9


{\displaystyle n=9}

 case, but the order of magnitude is known quite exactly: it has upper bound 




2


n

2


−
n

log

2


⁡
n
+
O
(
n
)




{\displaystyle 2^{n^{2}-n\log _{2}n+O(n)}}

 and lower bound 




2


n

2


−
n

log

2


⁡
n
−
O
(
n
)




{\displaystyle 2^{n^{2}-n\log _{2}n-O(n)}}

.[28]
 Any Boolean linear threshold function can be implemented with only integer weights. Furthermore, the number of bits necessary and sufficient for representing a single integer weight parameter is 



Θ
(
n
ln
⁡
n
)


{\displaystyle \Theta (n\ln n)}

.[28]
 A single perceptron can learn to classify any half-space. It cannot solve any linearly nonseparable vectors, such as the Boolean exclusive-or problem (the famous ""XOR problem"").
 A perceptron network with one hidden layer can learn to classify any compact subset arbitrarily closely. Similarly, it can also approximate any compactly-supported continuous function arbitrarily closely. This is essentially a special case of the theorems by George Cybenko and Kurt Hornik.
 Perceptrons (Minsky and Papert, 1969) studied the kind of perceptron networks necessary to learn various Boolean functions.
 Consider a perceptron network with 



n


{\displaystyle n}

 input units, one hidden layer, and one output, similar to the Mark I Perceptron machine. It computes a Boolean function of type 



f
:

2

n


→
2


{\displaystyle f:2^{n}\to 2}

. They call a function conjuctively local of order 



k


{\displaystyle k}

, iff there exists a perceptron network such that each unit in the hidden layer connects to at most 



k


{\displaystyle k}

 input units.
 Theorem. (Theorem 3.1.1): The parity function is conjuctively local of order 



n


{\displaystyle n}

.
 Theorem. (Section 5.5): The connectedness function is conjuctively local of order 



Ω
(

n

1

/

2


)


{\displaystyle \Omega (n^{1/2})}

.
 Below is an example of a learning algorithm for a single-layer perceptron with a single output unit. For a single-layer perceptron with multiple output units, since the weights of one output unit are completely separate from all the others', the same algorithm can be run for each output unit.
 For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. If the activation function or the underlying process being modeled by the perceptron is nonlinear, alternative learning algorithms such as the delta rule can be used as long as the activation function is differentiable. Nonetheless, the learning algorithm described in the steps below will often work, even for multilayer perceptrons with nonlinear activation functions.
 When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation.
 We first define some variables:
 We show the values of the features as follows:
 To represent the weights: 
 To show the time-dependence of 




w



{\displaystyle \mathbf {w} }

, we use:
 The algorithm updates the weights after every training sample in step 2b.
 A single perceptron is a linear classifier. It can only reach a stable state if all input vectors are classified correctly. In case the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane, then the algorithm would not converge since there is no solution. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used. Detailed analysis and extensions to the convergence theorem are in Chapter 11 of Perceptrons (1969).
 Linear separability is testable in time 



min
(
O
(

n

d

/

2


)
,
O
(

d

2
n


)
,
O
(

n

d
−
1


ln
⁡
n
)
)


{\displaystyle \min(O(n^{d/2}),O(d^{2n}),O(n^{d-1}\ln n))}

, where 



n


{\displaystyle n}

 is the number of data points, and 



d


{\displaystyle d}

 is the dimension of each point.[29]
 If the training set is linearly separable, then the perceptron is guaranteed to converge after making finitely many mistakes.[30] The theorem is proved by Rosenblatt et al.
 Perceptron convergence theorem — Given a dataset 



D


{\textstyle D}

, such that 




max

(
x
,
y
)
∈
D


‖
x

‖

2


=
R


{\textstyle \max _{(x,y)\in D}\|x\|_{2}=R}

, and it is linearly separable by some unit vector 




w

∗




{\textstyle w^{*}}

, with margin 



γ


{\textstyle \gamma }

: 



γ
:=

min

(
x
,
y
)
∈
D


y
(

w

∗


⋅
x
)


{\displaystyle \gamma :=\min _{(x,y)\in D}y(w^{*}\cdot x)}


 Then the perceptron 0-1 learning algorithm converges after making at most 



(
R

/

γ

)

2




{\textstyle (R/\gamma )^{2}}

 mistakes, for any learning rate, and any method of sampling from the dataset.
 The following simple proof is due to Novikoff (1962). The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product, and thus can be bounded above by O(√t), where t is the number of changes to the weight vector. However, it can also be bounded below by O(t) because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector. Suppose at step 



t


{\textstyle t}

, the perceptron with weight 




w

t




{\textstyle w_{t}}

 makes a mistake on data point 



(
x
,
y
)


{\textstyle (x,y)}

, then it updates to 




w

t
+
1


=

w

t


+
r
(
y
−

f


w

t




(
x
)
)
x


{\textstyle w_{t+1}=w_{t}+r(y-f_{w_{t}}(x))x}

.
 If 



y
=
0


{\textstyle y=0}

, the argument is symmetric, so we omit it.
 WLOG, 



y
=
1


{\textstyle y=1}

, then 




f


w

t




(
x
)
=
0


{\textstyle f_{w_{t}}(x)=0}

, 




f


w

∗




(
x
)
=
1


{\textstyle f_{w^{*}}(x)=1}

, and 




w

t
+
1


=

w

t


+
r
x


{\textstyle w_{t+1}=w_{t}+rx}

.
 By assumption, we have separation with margins: 




w

∗


⋅
x
≥
γ


{\displaystyle w^{*}\cdot x\geq \gamma }

 Thus,





w

∗


⋅

w

t
+
1


−

w

∗


⋅

w

t


=

w

∗


⋅
(
r
x
)
≥
r
γ


{\displaystyle w^{*}\cdot w_{t+1}-w^{*}\cdot w_{t}=w^{*}\cdot (rx)\geq r\gamma }


 Also 



‖

w

t
+
1



‖

2


2


−
‖

w

t



‖

2


2


=
‖

w

t


+
r
x

‖

2


2


−
‖

w

t



‖

2


2


=
2
r
(

w

t


⋅
x
)
+

r

2


‖
x

‖

2


2




{\displaystyle \|w_{t+1}\|_{2}^{2}-\|w_{t}\|_{2}^{2}=\|w_{t}+rx\|_{2}^{2}-\|w_{t}\|_{2}^{2}=2r(w_{t}\cdot x)+r^{2}\|x\|_{2}^{2}}

 and since the perceptron made a mistake, 




w

t


⋅
x
≤
0


{\textstyle w_{t}\cdot x\leq 0}

, and so




‖

w

t
+
1



‖

2


2


−
‖

w

t



‖

2


2


≤
‖
x

‖

2


2


≤

r

2



R

2




{\displaystyle \|w_{t+1}\|_{2}^{2}-\|w_{t}\|_{2}^{2}\leq \|x\|_{2}^{2}\leq r^{2}R^{2}}


 Since we started with 




w

0


=
0


{\textstyle w_{0}=0}

, after making 



N


{\textstyle N}

 mistakes, 



‖
w

‖

2


≤


N

r

2



R

2






{\displaystyle \|w\|_{2}\leq {\sqrt {Nr^{2}R^{2}}}}

 but also




‖
w

‖

2


≥
w
⋅

w

∗


≥
N
r
γ


{\displaystyle \|w\|_{2}\geq w\cdot w^{*}\geq Nr\gamma }


 Combining the two, we have 



N
≤
(
R

/

γ

)

2




{\textstyle N\leq (R/\gamma )^{2}}


 While the perceptron algorithm is guaranteed to converge on some solution in the case of a linearly separable training set, it may still pick any solution and problems may admit many solutions of varying quality.[31] The perceptron of optimal stability, nowadays better known as the linear support-vector machine, was designed to solve this problem (Krauth and Mezard, 1987).[32]
 When the dataset is not linearly separable, then there is no way for a single perceptron to converge. However, we still have[33]
 Perceptron cycling theorem — If the dataset 



D


{\displaystyle D}

 has only finitely many points, then there exists an upper bound number 



M


{\displaystyle M}

, such that for any starting weight vector 




w

0




{\displaystyle w_{0}}

 all weight vector 




w

t




{\displaystyle w_{t}}

 has norm bounded by 



‖

w

t


‖
≤
‖

w

0


‖
+
M


{\displaystyle \|w_{t}\|\leq \|w_{0}\|+M}


 This is proved first by Bradley Efron.[34]
 Consider a dataset where the 



x


{\displaystyle x}

 are from 



{
−
1
,
+
1

}

n




{\displaystyle \{-1,+1\}^{n}}

, that is, the vertices of an n-dimensional hypercube centered at origin, and 



y
=
θ
(

x

i


)


{\displaystyle y=\theta (x_{i})}

. That is, all data points with positive 




x

i




{\displaystyle x_{i}}

 have 



y
=
1


{\displaystyle y=1}

, and vice versa. By the perceptron convergence theorem, a perceptron would converge after making at most 



n


{\displaystyle n}

 mistakes.
 If we were to write a logical program to perform the same task, each positive example shows that one of the coordinates is the right one, and each negative example shows that its complement is a positive example. By collecting all the known positive examples, we eventually eliminate all but one coordinate, at which point the dataset is learned.[35]
 This bound is asymptotically tight in terms of the worst-case. In the worst-case, the first presented example is entirely new, and gives 



n


{\displaystyle n}

 bits of information, but each subsequent example would differ minimally from previous examples, and gives 1 bit each. After 



n
+
1


{\displaystyle n+1}

 examples, there are 



2
n


{\displaystyle 2n}

 bits of information, which is sufficient for the perceptron (with 



2
n


{\displaystyle 2n}

 bits of information).[26]
 However, it is not tight in terms of expectation if the examples are presented uniformly at random, since the first would give 



n


{\displaystyle n}

 bits, the second 



n

/

2


{\displaystyle n/2}

 bits, and so on, taking 



O
(
ln
⁡
n
)


{\displaystyle O(\ln n)}

 examples in total.[35]
 The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far ""in its pocket"". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.
 The Maxover algorithm (Wendemuth, 1995) is ""robust"" in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set.[36] In the linearly separable case, it will solve the training problem – if desired, even with optimal stability (maximum margin between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.
 The Voted Perceptron (Freund and Schapire, 1999), is a variant using multiple weighted perceptrons. The algorithm starts a new perceptron every time an example is wrongly classified, initializing the weights vector with the final weights of the last perceptron. Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one, and at the end the output will be a weighted vote on all perceptrons.
 In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987)[32]  or the AdaTron (Anlauf and Biehl, 1989)).[37] AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the kernel trick, are the conceptual foundations of the support-vector machine.
 The 



α


{\displaystyle \alpha }

-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify analogue patterns, by projecting them into a binary space. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.
 Another way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an n-order network.
 It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is overfitted.
 Other linear classification algorithms include Winnow, support-vector machine, and logistic regression.
 Like most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification.  Here, the input 



x


{\displaystyle x}

 and the output 



y


{\displaystyle y}

 are drawn from arbitrary sets. A feature representation function 



f
(
x
,
y
)


{\displaystyle f(x,y)}

 maps each possible input/output pair to a finite-dimensional real-valued feature vector.  As before, the feature vector is multiplied by a weight vector 



w


{\displaystyle w}

, but now the resulting score is used to choose among many possible outputs:
 Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not.  The update becomes:
 This multiclass feedback formulation reduces to the original perceptron when 



x


{\displaystyle x}

 is a real-valued vector, 



y


{\displaystyle y}

 is chosen from 



{
0
,
1
}


{\displaystyle \{0,1\}}

, and 



f
(
x
,
y
)
=
y
x


{\displaystyle f(x,y)=yx}

.
 For certain problems, input/output representations and features can be chosen so that 





a
r
g
m
a
x


y


f
(
x
,
y
)
⋅
w


{\displaystyle \mathrm {argmax} _{y}f(x,y)\cdot w}

 can be found efficiently even though 



y


{\displaystyle y}

 is chosen from a very large or even infinite set.
 Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002). It has also been applied to large-scale machine learning problems in a distributed computing setting.[38]
"
"Long short-term memory (LSTM)[1] is a type of recurrent neural network (RNN) aimed at dealing with the vanishing gradient problem[2] present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus ""long short-term memory"".[1] It is applicable to classification, processing and predicting data based on time series, such as in handwriting,[3] speech recognition,[4][5] machine translation,[6][7] speech activity detection,[8] robot control,[9][10] video games,[11][12] and healthcare.[13]
 A common LSTM unit is composed of a cell, an input gate, an output gate[14] and a forget gate.[15] The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
 In theory, classic RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with classic RNNs is computational (or practical) in nature: when training a classic RNN using back-propagation, the long-term gradients which are back-propagated can ""vanish"", meaning they can tend to zero due to very small numbers creeping into the computations, causing the model to effectively stop learning. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow with little to no attenuation. However, LSTM networks can still suffer from the exploding gradient problem.[16]
 The intuition behind the LSTM architecture is to create an additional module in a neural network that learns when to remember and when to forget pertinent information.[15] In other words, the network effectively learns which information might be needed later on in a sequence and when that information is no longer needed. For instance, in the context of natural language processing, the network can learn grammatical dependencies.[17] An LSTM might process the sentence ""Dave, as a result of his controversial claims, is now a pariah"" by remembering the (statistically likely) grammatical gender and number of the subject Dave, note that this information is pertinent for the pronoun his and note that this information is no longer important after the verb is.
 In the equations below, the lowercase variables represent vectors. Matrices 




W

q




{\displaystyle W_{q}}

 and 




U

q




{\displaystyle U_{q}}

 contain, respectively, the weights of the input and recurrent connections, where the subscript 






q




{\displaystyle _{q}}

 can either be the input gate 



i


{\displaystyle i}

, output gate 



o


{\displaystyle o}

, the forget gate 



f


{\displaystyle f}

 or the memory cell 



c


{\displaystyle c}

, depending on the activation being calculated. In this section, we are thus using a ""vector notation"". So, for example, 




c

t


∈


R


h




{\displaystyle c_{t}\in \mathbb {R} ^{h}}

 is not just one unit of one LSTM cell, but contains 



h


{\displaystyle h}

 LSTM cell's units.
 The compact forms of the equations for the forward pass of an LSTM cell with a forget gate are:[1][15]
 where the initial values are 




c

0


=
0


{\displaystyle c_{0}=0}

 and 




h

0


=
0


{\displaystyle h_{0}=0}

 and the operator 



⊙


{\displaystyle \odot }

 denotes the Hadamard product (element-wise product). The subscript 



t


{\displaystyle t}

 indexes the time step.
 Letting the superscripts 



d


{\displaystyle d}

 and 



h


{\displaystyle h}

 refer to the number of input features and number of hidden units, respectively:
 The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM).[18][19] Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state.[18] 




h

t
−
1




{\displaystyle h_{t-1}}

 is not used, 




c

t
−
1




{\displaystyle c_{t-1}}

 is used instead in most places.
 Each of the gates can be thought as a ""standard"" neuron in a feed-forward (or multi-layer) neural network: that is, they compute an activation (using an activation function) of a weighted sum. 




i

t


,

o

t




{\displaystyle i_{t},o_{t}}

 and 




f

t




{\displaystyle f_{t}}

 represent the activations of respectively the input, output and forget gates, at time step 



t


{\displaystyle t}

.
 The 3 exit arrows from the memory cell 



c


{\displaystyle c}

 to the 3 gates 



i
,
o


{\displaystyle i,o}

 and 



f


{\displaystyle f}

 represent the peephole connections. These peephole connections actually denote the contributions of the activation of the memory cell 



c


{\displaystyle c}

 at time step 



t
−
1


{\displaystyle t-1}

, i.e. the contribution of 




c

t
−
1




{\displaystyle c_{t-1}}

 (and not 




c

t




{\displaystyle c_{t}}

, as the picture may suggest). In other words, the gates 



i
,
o


{\displaystyle i,o}

 and 



f


{\displaystyle f}

 calculate their activations at time step 



t


{\displaystyle t}

 (i.e., respectively, 




i

t


,

o

t




{\displaystyle i_{t},o_{t}}

 and 




f

t




{\displaystyle f_{t}}

) also considering the activation of the memory cell 



c


{\displaystyle c}

 at time step 



t
−
1


{\displaystyle t-1}

, i.e. 




c

t
−
1




{\displaystyle c_{t-1}}

.
 The single left-to-right arrow exiting the memory cell is not a peephole connection and denotes 




c

t




{\displaystyle c_{t}}

.
 The little circles containing a 



×


{\displaystyle \times }

 symbol represent an element-wise multiplication between its inputs. The big circles containing an S-like curve represent the application of a differentiable function (like the sigmoid function) to a weighted sum.
 Peephole convolutional LSTM.[20] The 



∗


{\displaystyle *}

 denotes the convolution operator.
 An RNN using LSTM units can be trained in a supervised fashion on a set of training sequences, using an optimization algorithm like gradient descent combined with backpropagation through time to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.
 A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to 




lim

n
→
∞



W

n


=
0


{\displaystyle \lim _{n\to \infty }W^{n}=0}

 if the spectral radius of 



W


{\displaystyle W}

 is smaller than 1.[2][21]
 However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This ""error carousel"" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.
 Many applications use stacks of LSTM RNNs[22] and train them by connectionist temporal classification (CTC)[23] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
 Sometimes, it can be advantageous to train (parts of) an LSTM by neuroevolution[24] or by policy gradient methods, especially when there is no ""teacher"" (that is, training labels).
 There have been several successful stories of training, in a non-supervised fashion, RNNs with LSTM units.
 In 2018, Bill Gates called it a ""huge milestone in advancing artificial intelligence"" when bots developed by OpenAI were able to beat humans in the game of Dota 2.[11] OpenAI Five consists of five independent but coordinated neural networks. Each network is trained by a policy gradient method without supervising teacher and contains a single-layer, 1024-unit Long-Short-Term-Memory that sees the current game state and emits actions through several possible action heads.[11]
 In 2018, OpenAI also trained a similar LSTM by policy gradients to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.[10]
 In 2019, DeepMind's program AlphaStar used a deep LSTM core to excel at the complex video game Starcraft II.[12] This was viewed as significant progress towards Artificial General Intelligence.[12]
 Applications of LSTM include:
 1989: Mike Mozer's work on focused back-propagation[49] will later be cited by the main LSTM paper.[1] Mozer's equation (3.1) anticipates aspects of LSTM cells: c_i(t+1) = d_i c_i(t) + f(x(t)), where c_i(t) is the activation of the i-th self-connected ""context unit"" at time step t, x(t) is the current input, f is a non-linear function, and d_i is a real-valued ""decay weight"" that can be learned. The residual connection in the ""constant error carousel"" of an LSTM cell simplifies this by setting d_i = 1.0: c_i(t+1) = c_i(t) + f(x(t)). The LSTM paper[1] calls this ""LSTM’s central feature,"" and states: ""Note the similarity to Mozer’s fixed time constant system (1992) -- a time constant of 1.0 is appropriate for potentially infinite time lags.""
 1991: Sepp Hochreiter analyzed the vanishing gradient problem and developed principles of the method in his German diploma thesis,[2] which was called ""one of the most important documents in the history of machine learning"" by his supervisor Jürgen Schmidhuber.[50]
 1995:  ""Long Short-Term Memory (LSTM)"" is published in a technical report by Sepp Hochreiter and Jürgen Schmidhuber.[51]
 1996: LSTM is published at NIPS'1996, a peer-reviewed conference.[14]
 1997: The main LSTM paper is published in the journal Neural Computation.[1] By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates.[52]
 1999: Felix Gers, Jürgen Schmidhuber, and Fred Cummins introduced the forget gate (also called ""keep gate"") into the LSTM architecture,[53] 
enabling the LSTM to reset its own state.[52]
 2000: Gers, Schmidhuber, and Cummins added peephole connections (connections from the cell to the gates) into the architecture.[18][19] Additionally, the output activation function was omitted.[52]
 2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models.[18][54]
 Hochreiter et al. used LSTM for meta-learning (i.e. learning a learning algorithm).[55]
 2004: First successful application of LSTM to speech Alex Graves et al.[56][54]
 2005: First publication (Graves and Schmidhuber) of LSTM with full backpropagation through time and of bi-directional LSTM.[25][54]
 2005: Daan Wierstra, Faustino Gomez, and Schmidhuber trained LSTM by neuroevolution without a teacher.[24]
 2006: Graves, Fernandez, Gomez, and Schmidhuber introduce a new error function for LSTM: Connectionist Temporal Classification (CTC) for simultaneous alignment and recognition of sequences.[23] CTC-trained LSTM led to breakthroughs in speech recognition.[26][57][58][59]
 Mayer et al. trained LSTM to control robots.[9]
 2007: Wierstra, Foerster, Peters, and Schmidhuber trained LSTM by policy gradients for reinforcement learning without a teacher.[60]
 Hochreiter, Heuesel, and Obermayr applied LSTM to protein homology detection the field of biology.[36]
 2009: An LSTM trained by CTC won the ICDAR connected handwriting recognition competition. Three such models were submitted by a team led by Alex Graves.[3] One was the most accurate model in the competition and another was the fastest.[61] This was the first time an RNN won international competitions.[54]
 2009: Justin Bayer et al. introduced neural architecture search for LSTM.[62][54]
 2013: Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton used LSTM networks as a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset.[27]
 2014: Kyunghyun Cho et al. put forward a simplified variant of the forget gate LSTM[53] called Gated recurrent unit (GRU).[63]
 2015: Google started using an LSTM trained by CTC for speech recognition on Google Voice.[57][58] According to the official blog post, the new model cut transcription errors by 49%.[64]
 2015: Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles[53] to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks.[65][66][67] 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.[68] This has become the most cited neural network of the 21st century.[67]
 2016: Google started using an LSTM to suggest messages in the Allo conversation app.[69] In the same year, Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by 60%.[6][70][71]
 Apple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype[72][73][74] in the iPhone and for Siri.[75][76]
 Amazon released Polly, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.[77]
 2017:  Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.[7]
 Researchers from Michigan State University, IBM Research, and Cornell University published a study in the Knowledge Discovery and Data Mining (KDD) conference.[78][79][80] Their Time-Aware LSTM (T-LSTM) performs better on certain data sets than standard LSTM.
 Microsoft reported reaching 94.9% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used ""dialog session-based long-short-term memory"".[59]
 2018: OpenAI used LSTM trained by policy gradients to beat humans in the complex video game of Dota 2,[11] and to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.[10][54]
 2019: DeepMind used LSTM trained by policy gradients to excel at the complex video game of Starcraft II.[12][54]
 2021: According to Google Scholar, in 2021, LSTM was cited over 16,000 times within a single year. This reflects applications of LSTM in many different fields including healthcare.[13]
 [1]
"
"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs[1][2][3] makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
 Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.[8]
 The Ising model (1925) by Wilhelm Lenz[9] and Ernst Ising[10][11]
was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972.[12][13] This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.[14]  In 1993, a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[15]
 Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.[16]
 Around 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.[17] In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition.[18][19] In 2014, the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset[20] benchmark without using any traditional speech processing methods.[21]
 LSTM also improved large-vocabulary speech recognition[5][6] and text-to-speech synthesis[22] and was used in Google Android.[18][23] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49%[citation needed] through CTC-trained LSTM.[24]
 LSTM broke records for improved machine translation,[25] Language Modeling[26] and Multilingual Language Processing.[27] LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.[28]
 RNNs come in many variants.
 Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers.
 An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one.[29] At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.
 Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.[29]
 Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN).
 Variables and functions
 The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.
 Introduced by Bart Kosko,[32] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.[33]
 A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.[34]
 Echo state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series.[35] A variant for spiking neurons is known as a liquid state machine.[36]
 The independently recurrent neural network (IndRNN)[37] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.
 A recursive neural network[38] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation.[39][40] They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing.[41] The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.[42]
 The neural history compressor is an unsupervised stack of RNNs.[43] At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.
 The system effectively minimizes the description length or the negative logarithm of the probability of the data.[44] Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.
 It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level).[43] Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.[43]
 A generative model partially overcame the vanishing gradient problem[45] of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[15]
 Second-order RNNs use higher order weights 



w




i
j
k




{\displaystyle w{}_{ijk}}

 instead of the standard 



w




i
j




{\displaystyle w{}_{ij}}

 weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, stability, and representation.[46][47] Long short-term memory is an example of this but has no such formal mappings or proof of stability.
 Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"".[48] LSTM prevents backpropagated errors from vanishing or exploding.[45] Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks[18] that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved.[49] LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.
 Many applications use stacks of LSTM RNNs[50] and train them by connectionist temporal classification (CTC)[51] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
 LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.[52]
 Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants.[53][54] Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.[55] They have fewer parameters than LSTM, as they lack an output gate.[56]
 Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs.[57][58]
 A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs.
 For a neuron 



i


{\displaystyle i}

 in the network with activation 




y

i




{\displaystyle y_{i}}

, the rate of change of activation is given by:
 Where:
 CTRNNs have been applied to evolutionary robotics where they have been used to address vision,[59] co-operation,[60] and minimal cognitive behaviour.[61]
 Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations.[62] This transformation can be thought of as occurring after the post-synaptic node activation functions 




y

i


(
t
)


{\displaystyle y_{i}(t)}

 have been low-pass filtered but prior to sampling.
 Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.[43][63] Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.[64]
 Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.[65]
 Generally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections.[66]
 A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.[67][68] With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.[citation needed] Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.[64][69]
 Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.[70]
 Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.
 Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).[71]
 Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices.[72] The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems.
Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.
The evolution of these networks can be studied analytically using variations of the Caravelli–Traversa–Di Ventra equation.[73]
 Given a time series x of length sequence_length.
In the recurrent neural network, there is a loop that processes all entries of the time series x through the layers neural_network one after another. These have as return value in each time step i both the prediction y_pred[i] and an updated hidden state hidden, which has the length hidden_size. As a result, after the loop, the collection of all predictions y_pred is returned.
The following pseudocode (based on the programming language Python) illustrates the functionality of a recurrent neural network.[74]
 Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation.
 Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter and others.
 The standard method is called ""backpropagation through time"" or BPTT, and is a generalization of back-propagation for feed-forward networks.[75][76] Like that method, it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called ""Real-Time Recurrent Learning"" or RTRL,[77][78] which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.
 In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.[79][80]
 For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon.[81] An online hybrid between BPTT and RTRL with intermediate complexity exists,[82][83] along with variants for continuous time.[84]
 A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events.[45][85] LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.[16] This problem is also solved in the independently recurrent neural network (IndRNN)[37] by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problem.
 The on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks.[86] It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.
 One approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation.[87] It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations.[88] It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.[88]
 Training the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.
 The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.[89][90][91]
 Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows:
 Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: 
 The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error.
 Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.
 RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.
 They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.
 In particular, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).[92]
 The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity.[93]
 Applications of recurrent neural networks include:
"
"In machine learning, backpropagation is a gradient estimation method used to train neural network models. The gradient estimate is used by the optimization algorithm to compute the network parameter updates.
 It is an efficient application of the Leibniz chain rule (1673)[1] to such networks.[2] It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).[3][4][5][6][7][8][9] The term ""back-propagating error correction"" was introduced in 1962 by Frank Rosenblatt,[10][2] but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation[11] already in 1960 in the context of control theory.[2]
 Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.[11][12][13] Gradient descent, or variants such as stochastic gradient descent,[14] are commonly used.
 Strictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent.[15] In 1986 David E. Rumelhart et al. published an experimental analysis of the technique.[16] This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.
 Backpropagation computes the gradient in weight space of a feedforward neural network, with respect to a loss function. Denote:
 In the derivation of backpropagation, other intermediate quantities are used by introducing them as needed below. Bias terms are not treated specially since they correspond to a weight with a fixed input of 1. For backpropagation the specific loss function and activation functions do not matter as long as they and their derivatives can be evaluated efficiently. Traditional activation functions include sigmoid, tanh, and ReLU. swish[17] mish,[18] and other activation functions have since been proposed as well.
 The overall network is a combination of function composition and matrix multiplication:
 For a training set there will be a set of input–output pairs, 




{

(

x

i


,

y

i


)

}



{\displaystyle \left\{(x_{i},y_{i})\right\}}

. For each input–output pair 



(

x

i


,

y

i


)


{\displaystyle (x_{i},y_{i})}

 in the training set, the loss of the model on that pair is the cost of the difference between the predicted output 



g
(

x

i


)


{\displaystyle g(x_{i})}

 and the target output 




y

i




{\displaystyle y_{i}}

:
 Note the distinction: during model evaluation the weights are fixed while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training the input–output pair is fixed while the weights vary, and the network ends with the loss function.
 Backpropagation computes the gradient for a fixed input–output pair 



(

x

i


,

y

i


)


{\displaystyle (x_{i},y_{i})}

, where the weights 




w

j
k


l




{\displaystyle w_{jk}^{l}}

 can vary. Each individual component of the gradient, 



∂
C

/

∂

w

j
k


l


,


{\displaystyle \partial C/\partial w_{jk}^{l},}

 can be computed by the chain rule; but doing this separately for each weight is inefficient. Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer – specifically the gradient of the weighted input of each layer, denoted by 




δ

l




{\displaystyle \delta ^{l}}

 – from back to front.
 Informally, the key point is that since the only way a weight in 




W

l




{\displaystyle W^{l}}

 affects the loss is through its effect on the next layer, and it does so linearly, 




δ

l




{\displaystyle \delta ^{l}}

 are the only data you need to compute the gradients of the weights at layer 



l


{\displaystyle l}

, and then the previous layer can be computed 




δ

l
−
1




{\displaystyle \delta ^{l-1}}

 and repeated recursively. This avoids inefficiency in two ways. First, it avoids duplication because when computing the gradient at layer 



l


{\displaystyle l}

, it is unnecessary to recompute all derivatives on later layers 



l
+
1
,
l
+
2
,
…


{\displaystyle l+1,l+2,\ldots }

 each time. Second, it avoids unnecessary intermediate calculations, because at each stage it directly computes the gradient of the weights with respect to the ultimate output (the loss), rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights 



∂

a


j
′




l
′




/

∂

w

j
k


l




{\displaystyle \partial a_{j'}^{l'}/\partial w_{jk}^{l}}

.
 Backpropagation can be expressed for simple feedforward networks in terms of matrix multiplication, or more generally in terms of the adjoint graph.
 For the basic case of a feedforward network, where nodes in each layer are connected only to nodes in the immediate next layer (without skipping any layers), and there is a loss function that computes a scalar loss for the final output, backpropagation can be understood simply by matrix multiplication.[c] Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from right to left – ""backwards"" – with the gradient of the weights between each layer being a simple modification of the partial products (the ""backwards propagated error"").
 Given an input–output pair 



(
x
,
y
)


{\displaystyle (x,y)}

, the loss is:
 To compute this, one starts with the input 



x


{\displaystyle x}

 and works forward; denote the weighted input of each hidden layer as 




z

l




{\displaystyle z^{l}}

 and the output of hidden layer 



l


{\displaystyle l}

 as the activation 




a

l




{\displaystyle a^{l}}

. For backpropagation, the activation 




a

l




{\displaystyle a^{l}}

 as well as the derivatives 



(

f

l



)
′



{\displaystyle (f^{l})'}

 (evaluated at 




z

l




{\displaystyle z^{l}}

) must be cached for use during the backwards pass.
 The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input 



x


{\displaystyle x}

:
 where 






d

a

L




d

z

L







{\displaystyle {\frac {da^{L}}{dz^{L}}}}

 is a diagonal matrix.
 These terms are: the derivative of the loss function;[d] the derivatives of the activation functions;[e] and the matrices of weights:[f]
 The gradient 



∇


{\displaystyle \nabla }

 is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed, but the entries are the same:
 Backpropagation then consists essentially of evaluating this expression from right to left (equivalently, multiplying the previous expression for the derivative from left to right), computing the gradient at each layer on the way; there is an added step, because the gradient of the weights is not just a subexpression: there's an extra multiplication.
 Introducing the auxiliary quantity 




δ

l




{\displaystyle \delta ^{l}}

 for the partial products (multiplying from right to left), interpreted as the ""error at level 



l


{\displaystyle l}

"" and defined as the gradient of the input values at level 



l


{\displaystyle l}

:
 Note that 




δ

l




{\displaystyle \delta ^{l}}

 is a vector, of length equal to the number of nodes in level 



l


{\displaystyle l}

; each component is interpreted as the ""cost attributable to (the value of) that node"".
 The gradient of the weights in layer 



l


{\displaystyle l}

 is then:
 The factor of 




a

l
−
1




{\displaystyle a^{l-1}}

 is because the weights 




W

l




{\displaystyle W^{l}}

 between level 



l
−
1


{\displaystyle l-1}

 and 



l


{\displaystyle l}

 affect level 



l


{\displaystyle l}

 proportionally to the inputs (activations): the inputs are fixed, the weights vary.
 The 




δ

l




{\displaystyle \delta ^{l}}

 can easily be computed recursively, going from right to left, as:
 The gradients of the weights can thus be computed using a few matrix multiplications for each level; this is backpropagation.
 Compared with naively computing forwards (using the 




δ

l




{\displaystyle \delta ^{l}}

 for illustration):
 There are two key differences with backpropagation:
 For more general graphs, and other advanced variations, backpropagation can be understood in terms of automatic differentiation, where backpropagation is a special case of reverse accumulation (or ""reverse mode"").[9]
 The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.[19]
 
To understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear)[g] that is the weighted sum of its input.  Initially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consist of a set of tuples 



(

x

1


,

x

2


,
t
)


{\displaystyle (x_{1},x_{2},t)}

 where 




x

1




{\displaystyle x_{1}}

 and 




x

2




{\displaystyle x_{2}}

 are the inputs to the network and t is the correct output (the output the network should produce given those inputs, when it has been trained). The initial network, given 




x

1




{\displaystyle x_{1}}

 and 




x

2




{\displaystyle x_{2}}

, will compute an output y that likely differs from t (given random weights). A loss function 



L
(
t
,
y
)


{\displaystyle L(t,y)}

 is used for measuring the discrepancy between the target output t and the computed output y. For regression analysis problems the squared error can be used as a loss function, for classification the categorical cross-entropy can be used.
 As an example consider a regression problem using the square error as a loss:
 where E is the discrepancy or error.
 
Consider the network on a single training case: 



(
1
,
1
,
0
)


{\displaystyle (1,1,0)}

. Thus, the input 




x

1




{\displaystyle x_{1}}

 and 




x

2




{\displaystyle x_{2}}

 are 1 and 1 respectively and the correct output, t is 0. Now if the relation is plotted between the network's output y on the horizontal axis and the error E on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output y which minimizes the error E. For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output y that exactly matches the target output t. Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error.  However, the output of a neuron depends on the weighted sum of all its inputs:
 where 




w

1




{\displaystyle w_{1}}

 and 




w

2




{\displaystyle w_{2}}

 are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning.
 In this example, upon injecting the training data 



(
1
,
1
,
0
)


{\displaystyle (1,1,0)}

, the loss function becomes
 



E
=
(
t
−
y

)

2


=

y

2


=
(

x

1



w

1


+

x

2



w

2



)

2


=
(

w

1


+

w

2



)

2


.


{\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}


 Then, the loss function 



E


{\displaystyle E}

 takes the form of a parabolic cylinder with its base directed along 




w

1


=
−

w

2




{\displaystyle w_{1}=-w_{2}}

. Since all sets of weights that satisfy 




w

1


=
−

w

2




{\displaystyle w_{1}=-w_{2}}

 minimize the loss function, in this case additional constraints are required to converge to a unique solution. Additional constraints could either be generated by setting specific conditions to the weights, or by injecting additional training data.
 One commonly used algorithm to find the set of weights that minimizes the error is gradient descent. By backpropagation, the steepest descent direction is calculated of the loss function versus the present synaptic weights. Then, the weights can be modified along the steepest descent direction, and the error is minimized in an efficient way.
 The gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron,[h] the squared error function is
 where
 For each neuron 



j


{\displaystyle j}

, its output 




o

j




{\displaystyle o_{j}}

 is defined as
 where the activation function 



φ


{\displaystyle \varphi }

 is non-linear and differentiable over the activation region (the ReLU is not differentiable at one point). A historically used activation function is the logistic function:
 which has a convenient derivative of:
 The input 





net


j




{\displaystyle {\text{net}}_{j}}

 to a neuron is the weighted sum of outputs 




o

k




{\displaystyle o_{k}}

 of previous neurons. If the neuron is in the first layer after the input layer, the 




o

k




{\displaystyle o_{k}}

 of the input layer are simply the inputs 




x

k




{\displaystyle x_{k}}

 to the network. The number of input units to the neuron is 



n


{\displaystyle n}

. The variable 




w

k
j




{\displaystyle w_{kj}}

 denotes the weight between neuron 



k


{\displaystyle k}

 of the previous layer and neuron 



j


{\displaystyle j}

 of the current layer.
 Calculating the partial derivative of the error with respect to a weight 




w

i
j




{\displaystyle w_{ij}}

 is done using the chain rule twice:
     In the last factor of the right-hand side of the above, only one term in the sum 





net


j




{\displaystyle {\text{net}}_{j}}

 depends on 




w

i
j




{\displaystyle w_{ij}}

, so that
     If the neuron is in the first layer after the input layer, 




o

i




{\displaystyle o_{i}}

 is just 




x

i




{\displaystyle x_{i}}

.
 The derivative of the output of neuron 



j


{\displaystyle j}

 with respect to its input is simply the partial derivative of the activation function:
     which for the logistic activation function
 This is the reason why backpropagation requires that the activation function be differentiable. (Nevertheless, the ReLU activation function, which is non-differentiable at 0, has become quite popular, e.g. in AlexNet)
 The first factor is straightforward to evaluate if the neuron is in the output layer, because then 




o

j


=
y


{\displaystyle o_{j}=y}

 and
     If half of the square error is used as loss function we can rewrite it as
 However, if 



j


{\displaystyle j}

 is in an arbitrary inner layer of the network, finding the derivative 



E


{\displaystyle E}

 with respect to 




o

j




{\displaystyle o_{j}}

 is less obvious.
 Considering 



E


{\displaystyle E}

 as a function with the inputs being all neurons 



L
=
{
u
,
v
,
…
,
w
}


{\displaystyle L=\{u,v,\dots ,w\}}

 receiving input from neuron 



j


{\displaystyle j}

,
 and taking the total derivative with respect to 




o

j




{\displaystyle o_{j}}

, a recursive expression for the derivative is obtained:
     Therefore, the derivative with respect to 




o

j




{\displaystyle o_{j}}

 can be calculated if all the derivatives with respect to the outputs 




o

ℓ




{\displaystyle o_{\ell }}

 of the next layer – the ones closer to the output neuron – are known. [Note, if any of the neurons in set 



L


{\displaystyle L}

 were not connected to neuron 



j


{\displaystyle j}

, they would be independent of 




w

i
j




{\displaystyle w_{ij}}

 and the corresponding partial derivative under the summation would vanish to 0.]
 Substituting Eq. 2, Eq. 3 Eq.4 and Eq. 5 in Eq. 1 we obtain:
 with
 if 



φ


{\displaystyle \varphi }

 is the logistic function, and the error is the square error:
 To update the weight 




w

i
j




{\displaystyle w_{ij}}

 using gradient descent, one must choose a learning rate, 



η
>
0


{\displaystyle \eta >0}

. The change in weight needs to reflect the impact on 



E


{\displaystyle E}

 of an increase or decrease in 




w

i
j




{\displaystyle w_{ij}}

. If 






∂
E


∂

w

i
j





>
0


{\displaystyle {\frac {\partial E}{\partial w_{ij}}}>0}

, an increase in 




w

i
j




{\displaystyle w_{ij}}

 increases 



E


{\displaystyle E}

; conversely, if 






∂
E


∂

w

i
j





<
0


{\displaystyle {\frac {\partial E}{\partial w_{ij}}}<0}

, an increase in 




w

i
j




{\displaystyle w_{ij}}

 decreases 



E


{\displaystyle E}

. The new 



Δ

w

i
j




{\displaystyle \Delta w_{ij}}

 is added to the old weight, and the product of the learning rate and the gradient, multiplied by 



−
1


{\displaystyle -1}

 guarantees that 




w

i
j




{\displaystyle w_{ij}}

 changes in a way that always decreases 



E


{\displaystyle E}

. In other words, in the equation immediately below, 



−
η



∂
E


∂

w

i
j







{\displaystyle -\eta {\frac {\partial E}{\partial w_{ij}}}}

 always changes 




w

i
j




{\displaystyle w_{ij}}

 in such a way that 



E


{\displaystyle E}

 is decreased:
 Using a Hessian matrix of second-order derivatives of the error function, the Levenberg–Marquardt algorithm often converges faster than first-order gradient descent, especially when the topology of the error function is complicated.[20][21] It may also find solutions in smaller node counts for which other methods might not converge.[21] The Hessian can be approximated by the Fisher information matrix.[22]
 The loss function is a function that maps values of one or more variables onto a real number intuitively representing some ""cost"" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network.
 The mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation.[23] The first is that it can be written as an average 



E
=


1
n



∑

x



E

x




{\textstyle E={\frac {1}{n}}\sum _{x}E_{x}}

 over error functions 




E

x




{\textstyle E_{x}}

, for 



n


{\textstyle n}

 individual training examples, 



x


{\textstyle x}

. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.
 Let 



y
,

y
′



{\displaystyle y,y'}

 be vectors in 





R


n




{\displaystyle \mathbb {R} ^{n}}

.
 Select an error function 



E
(
y
,

y
′

)


{\displaystyle E(y,y')}

 measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors 



y


{\displaystyle y}

 and 




y
′



{\displaystyle y'}

:



E
(
y
,

y
′

)
=



1
2



‖
y
−

y
′


‖

2




{\displaystyle E(y,y')={\tfrac {1}{2}}\lVert y-y'\rVert ^{2}}

The error function over 



n


{\textstyle n}

 training examples can then be written as an average of losses over individual examples:



E
=


1

2
n




∑

x


‖
(
y
(
x
)
−

y
′

(
x
)
)

‖

2




{\displaystyle E={\frac {1}{2n}}\sum _{x}\lVert (y(x)-y'(x))\rVert ^{2}}


 Backpropagation had been derived repeatedly, as it is essentially an efficient application of the chain rule (first written down by Gottfried Wilhelm Leibniz in 1676[1][26]) to neural networks.
 The terminology ""back-propagating error correction"" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this.[27] In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.
 Precursors to backpropagation appeared in optimal control theory since 1950s. Yann LeCun et al credits 1950s work by Pontryagin and others in optimal control theory, especially the adjoint state method, for being a continuous-time version of backpropagation.[28] Hecht-Nielsen[29] credits the Robbins–Monro algorithm (1951) and Arthur Bryson and Yu-Chi Ho's Applied Optimal Control (1969) as presages of backpropagation. Other precursors were Henry J. Kelley 1960,[11] and Arthur E. Bryson (1961).[12] In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule.[30][31][32] In 1973, he adapted parameters of controllers in proportion to error gradients.[33] Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.[2]
 The ADALINE (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first multilayer perceptron (MLP) with more than one layer trained by stochastic gradient descent[14] was published in 1967 by Shun'ichi Amari.[34][2] The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.[2]
 Modern backpropagation was first published by Seppo Linnainmaa as ""reverse mode of automatic differentiation"" (1970)[3] for discrete connected networks of nested differentiable functions.[4][5]
 In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.[35][36] Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize Freud's ""flow of psychic energy"". He faced repeated difficulty in publishing the work, only managing in 1981.[37]
 Around 1982,[37]: 376  David E. Rumelhart independently developed[38]: 252  backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 Nature paper an experimental analysis of the technique.[16] These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.[19][39][40]
 In 1985, the method was also described by David Parker.[41][42] Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.[43]
 Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were ""known"" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with Geoffrey Hinton.[37]
 Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.
 In 1987, NETtalk learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk.[37]: 324  The NETtalk program became a popular success, appearing on the Today show.[44]
 In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to drive autonomously using backpropagation.[45]
 The LeNet was published in 1989 to recognize handwritten zip codes.
 In 1992, TD-Gammon achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.[46]
 In 1993, Eric Wan won an international pattern recognition contest through backpropagation.[7][47]
 During the 2000s it fell out of favour[citation needed], but returned in the 2010s, benefiting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first[48] and second language learning.[49]). [50]
 Error backpropagation has been suggested to explain human brain ERP components like the N400 and P600.[51]
 In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University.[52]
"
"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.[1] The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. It can be used, for example, to estimate a mixture of gaussians, or to solve the multiple linear regression problem.[2]
 The EM algorithm was explained and given its name in a classic 1977 paper by Arthur Dempster, Nan Laird, and Donald Rubin.[3] They pointed out that the method had been ""proposed many times in special circumstances"" by earlier authors. One of the earliest is the gene-counting method for estimating allele frequencies by Cedric Smith.[4] Another was proposed by H.O. Hartley in 1958, and Hartley and Hocking in 1977, from which many of the ideas in the Dempster–Laird–Rubin paper originated.[5] Another one by S.K Ng, Thriyambakam Krishnan and G.J McLachlan in 1977.[6] Hartley’s ideas can be broadened to any grouped discrete distribution. A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers,[7][8][9] following his collaboration with Per Martin-Löf and Anders Martin-Löf.[10][11][12][13][14] The Dempster–Laird–Rubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems. The Dempster–Laird–Rubin paper established the EM method as an important tool of statistical analysis. See also Meng and van Dyk (1997).
 The convergence analysis of the Dempster–Laird–Rubin algorithm was flawed and a correct convergence analysis was published by C. F. Jeff Wu in 1983.[15]
Wu's proof established the EM method's convergence also outside of the exponential family, as claimed by Dempster–Laird–Rubin.[15]
 The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.
 Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.
 The EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points. It's not obvious that this will work, but it can be proven in this context. Additionally, it can be proven that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a local maximum or a saddle point.[15] In general, multiple maxima may occur, with no guarantee that the global maximum will be found. Some likelihoods also have singularities in them, i.e., nonsensical maxima. For example, one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points.
 Given the statistical model which generates a set 




X



{\displaystyle \mathbf {X} }

 of observed data, a set of unobserved latent data or missing values 




Z



{\displaystyle \mathbf {Z} }

, and a vector of unknown parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

, along with a likelihood function 



L
(

θ

;

X

,

Z

)
=
p
(

X

,

Z

∣

θ

)


{\displaystyle L({\boldsymbol {\theta }};\mathbf {X} ,\mathbf {Z} )=p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})}

, the maximum likelihood estimate (MLE) of the unknown parameters is determined by maximizing the marginal likelihood of the observed data
 However, this quantity is often intractable since 




Z



{\displaystyle \mathbf {Z} }

 is unobserved and the distribution of 




Z



{\displaystyle \mathbf {Z} }

 is unknown before attaining 




θ



{\displaystyle {\boldsymbol {\theta }}}

.
 The EM algorithm seeks to find the maximum likelihood estimate of the marginal likelihood by iteratively applying these two steps:
 More succinctly, we can write it as one equation:





θ


(
t
+
1
)


=



a
r
g

m
a
x

θ



E


Z

∼
p
(
⋅

|


X

,


θ


(
t
)


)


⁡

[

log
⁡
p
(

X

,

Z


|


θ

)

]




{\displaystyle {\boldsymbol {\theta }}^{(t+1)}={\underset {\boldsymbol {\theta }}{\operatorname {arg\,max} }}\operatorname {E} _{\mathbf {Z} \sim p(\cdot |\mathbf {X} ,{\boldsymbol {\theta }}^{(t)})}\left[\log p(\mathbf {X} ,\mathbf {Z} |{\boldsymbol {\theta }})\right]\,}


 The typical models to which EM is applied use 




Z



{\displaystyle \mathbf {Z} }

 as a latent variable indicating membership in one of a set of groups:
 However, it is possible to apply EM to other sorts of models.
 The motivation is as follows. If the value of the parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

 is known, usually the value of the latent variables 




Z



{\displaystyle \mathbf {Z} }

 can be found by maximizing the log-likelihood over all possible values of 




Z



{\displaystyle \mathbf {Z} }

, either simply by iterating over 




Z



{\displaystyle \mathbf {Z} }

 or through an algorithm such as the Viterbi algorithm for hidden Markov models. Conversely, if we know the value of the latent variables 




Z



{\displaystyle \mathbf {Z} }

, we can find an estimate of the parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

 fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group. This suggests an iterative algorithm, in the case where both 




θ



{\displaystyle {\boldsymbol {\theta }}}

 and 




Z



{\displaystyle \mathbf {Z} }

 are unknown:
 The algorithm as just described monotonically approaches a local minimum of the cost function.
 Although an EM iteration does increase the observed data (i.e., marginal) likelihood function, no guarantee exists that the sequence converges to a maximum likelihood estimator. For multimodal distributions, this means that an EM algorithm may converge to a local maximum of the observed data likelihood function, depending on starting values. A variety of heuristic or metaheuristic approaches exist to escape a local maximum, such as random-restart hill climbing (starting with several different random initial estimates 





θ


(
t
)




{\displaystyle {\boldsymbol {\theta }}^{(t)}}

), or applying simulated annealing methods.
 EM is especially useful when the likelihood is an exponential family, see Sundberg (2019, Ch. 8) for a comprehensive treatment:[16] the E step becomes the sum of expectations of sufficient statistics, and the M step involves maximizing a linear function. In such a case, it is usually possible to derive closed-form expression updates for each step, using the Sundberg formula[17] (proved and published by Rolf Sundberg, based on unpublished results of Per Martin-Löf and Anders Martin-Löf).[8][9][11][12][13][14]
 The EM method was modified to compute maximum a posteriori (MAP) estimates for Bayesian inference in the original paper by Dempster, Laird, and Rubin.
 Other methods exist to find maximum likelihood estimates, such as gradient descent, conjugate gradient, or variants of the Gauss–Newton algorithm. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.
 Expectation-Maximization works to improve 



Q
(

θ

∣


θ


(
t
)


)


{\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}

 rather than directly improving 



log
⁡
p
(

X

∣

θ

)


{\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})}

. Here it is shown that improvements to the former imply improvements to the latter.[18]
 For any 




Z



{\displaystyle \mathbf {Z} }

 with non-zero probability 



p
(

Z

∣

X

,

θ

)


{\displaystyle p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}

, we can write
 We take the expectation over possible values of the unknown data 




Z



{\displaystyle \mathbf {Z} }

 under the current parameter estimate 




θ

(
t
)




{\displaystyle \theta ^{(t)}}

 by multiplying both sides by 



p
(

Z

∣

X

,


θ


(
t
)


)


{\displaystyle p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})}

 and summing (or integrating) over 




Z



{\displaystyle \mathbf {Z} }

. The left-hand side is the expectation of a constant, so we get:
 where 



H
(

θ

∣


θ


(
t
)


)


{\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}

 is defined by the negated sum it is replacing.
This last equation holds for every value of 




θ



{\displaystyle {\boldsymbol {\theta }}}

 including 




θ

=


θ


(
t
)




{\displaystyle {\boldsymbol {\theta }}={\boldsymbol {\theta }}^{(t)}}

,
 and subtracting this last equation from the previous equation gives
 However, Gibbs' inequality tells us that 



H
(

θ

∣


θ


(
t
)


)
≥
H
(


θ


(
t
)


∣


θ


(
t
)


)


{\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})\geq H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})}

, so we can conclude that
 In words, choosing 




θ



{\displaystyle {\boldsymbol {\theta }}}

 to improve 



Q
(

θ

∣


θ


(
t
)


)


{\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}

 causes 



log
⁡
p
(

X

∣

θ

)


{\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})}

 to improve at least as much.
 The EM algorithm can be viewed as two alternating maximization steps, that is, as an example of coordinate descent.[19][20] Consider the function:
 where q is an arbitrary probability distribution over the unobserved data z and H(q) is the entropy of the distribution q. This function can be written as
 where  




p

Z
∣
X


(
⋅
∣
x
;
θ
)


{\displaystyle p_{Z\mid X}(\cdot \mid x;\theta )}

 is the conditional distribution of the unobserved data given the observed data 



x


{\displaystyle x}

 and 




D

K
L




{\displaystyle D_{KL}}

 is the Kullback–Leibler divergence.
 Then the steps in the EM algorithm may be viewed as:
 A Kalman filter is typically used for on-line state estimation and a minimum-variance smoother may be employed for off-line or batch state estimation. However, these minimum-variance solutions require estimates of the state-space model parameters. EM algorithms can be used for solving joint state and parameter estimation problems.
 Filtering and smoothing EM algorithms arise by repeating this two-step procedure:
 Suppose that a Kalman filter or minimum-variance smoother operates on measurements of a single-input-single-output system that possess additive white noise. An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation
 where 







x
^




k




{\displaystyle {\widehat {x}}_{k}}

 are scalar output estimates calculated by a filter or a smoother from N scalar measurements 




z

k




{\displaystyle z_{k}}

. The above update can also be applied to updating a Poisson measurement noise intensity. Similarly, for a first-order auto-regressive process, an updated process noise variance estimate can be calculated by
 where 







x
^




k




{\displaystyle {\widehat {x}}_{k}}

 and 







x
^




k
+
1




{\displaystyle {\widehat {x}}_{k+1}}

 are scalar state estimates calculated by a filter or a smoother. The updated model coefficient estimate is obtained via
 The convergence of parameter estimates such as those above are well studied.[26][27][28][29]
 A number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those using conjugate gradient and modified Newton's methods (Newton–Raphson).[30] Also, EM can be used with constrained estimation methods.
 Parameter-expanded expectation maximization (PX-EM) algorithm often provides speed up by ""us[ing] a `covariance adjustment' to correct the analysis of the M step, capitalising on extra information captured in the imputed complete data"".[31]
 Expectation conditional maximization (ECM) replaces each M step with a sequence of conditional maximization (CM) steps in which each parameter θi is maximized individually, conditionally on the other parameters remaining fixed.[32] Itself can be extended into the Expectation conditional maximization either (ECME) algorithm.[33]
 This idea is further extended in generalized expectation maximization (GEM) algorithm, in which is sought only an increase in the objective function F for both the E step and M step as described in the As a maximization–maximization procedure section.[19] GEM is further developed in a distributed environment and shows promising results.[34]
 It is also possible to consider the EM algorithm as a subclass of the MM (Majorize/Minimize or Minorize/Maximize, depending on context) algorithm,[35] and therefore use any machinery developed in the more general case.
 The Q-function used in the EM algorithm is based on the log likelihood. Therefore, it is regarded as the log-EM algorithm. The use of the log likelihood can be generalized to that of the α-log likelihood ratio. Then, the α-log likelihood ratio of the observed data can be exactly expressed as equality by using the Q-function of the α-log likelihood ratio and the α-divergence. Obtaining this Q-function is a generalized E step. Its maximization is a generalized M step. This pair is called the α-EM algorithm[36]
which contains the log-EM algorithm as its subclass. Thus, the α-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The α-EM shows faster convergence than the log-EM algorithm by choosing an appropriate α. The α-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm α-HMM.
[37]
 EM is a partially non-Bayesian, maximum likelihood method. Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for θ (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over θ and the latent variables. The Bayesian approach to inference is simply to treat θ as another latent variable. In this paradigm, the distinction between the E and M steps disappears. If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including θ) and optimize them one at a time. Now, k steps per iteration are needed, where k is the number of latent variables. For graphical models this is easy to do as each variable's new Q depends only on its Markov blanket, so local message passing can be used for efficient inference.
 In information geometry, the E step and the M step are interpreted as projections under dual affine connections, called the e-connection and the m-connection; the Kullback–Leibler divergence can also be understood in these terms.
 Let 




x

=
(


x


1


,


x


2


,
…
,


x


n


)


{\displaystyle \mathbf {x} =(\mathbf {x} _{1},\mathbf {x} _{2},\ldots ,\mathbf {x} _{n})}

 be a sample of 



n


{\displaystyle n}

 independent observations from a mixture of two multivariate normal distributions of dimension 



d


{\displaystyle d}

, and let 




z

=
(

z

1


,

z

2


,
…
,

z

n


)


{\displaystyle \mathbf {z} =(z_{1},z_{2},\ldots ,z_{n})}

 be the latent variables that determine the component from which the observation originates.[20]
 where
 The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:
 where the incomplete-data likelihood function is
 and the complete-data likelihood function is
 or
 where 




I



{\displaystyle \mathbb {I} }

 is an indicator function and 



f


{\displaystyle f}

 is the probability density function of a multivariate normal.
 In the last equality, for each i, one indicator 




I

(

z

i


=
j
)


{\displaystyle \mathbb {I} (z_{i}=j)}

 is equal to zero, and one indicator is equal to one. The inner sum thus reduces to one term.
 Given our current estimate of the parameters θ(t), the conditional distribution of the Zi is determined by Bayes theorem to be the proportional height of the normal density weighted by τ:
 These are called the ""membership probabilities"", which are normally considered the output of the E step (although this is not the Q function of below).
 This E step corresponds with setting up this function for Q:
 The expectation of 



log
⁡
L
(
θ
;


x


i


,

Z

i


)


{\displaystyle \log L(\theta ;\mathbf {x} _{i},Z_{i})}

 inside the sum is taken with respect to the probability density function 



P
(

Z

i


∣

X

i


=


x


i


;

θ

(
t
)


)


{\displaystyle P(Z_{i}\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})}

, which might be different for each  





x


i




{\displaystyle \mathbf {x} _{i}}

 of the training set. Everything in the E step is known before the step is taken except 




T

j
,
i




{\displaystyle T_{j,i}}

, which is computed according to the equation at the beginning of the E step section.
 This full conditional expectation does not need to be calculated in one step, because τ and μ/Σ appear in separate linear terms and can thus be maximized independently.
 



Q
(
θ
∣

θ

(
t
)


)


{\displaystyle Q(\theta \mid \theta ^{(t)})}

 being quadratic in form means that determining the maximizing values of 



θ


{\displaystyle \theta }

 is relatively straightforward. Also, 



τ


{\displaystyle \tau }

, 



(


μ


1


,

Σ

1


)


{\displaystyle ({\boldsymbol {\mu }}_{1},\Sigma _{1})}

 and 



(


μ


2


,

Σ

2


)


{\displaystyle ({\boldsymbol {\mu }}_{2},\Sigma _{2})}

 may all be maximized independently since they all appear in separate linear terms.
 To begin, consider 



τ


{\displaystyle \tau }

, which has the constraint 




τ

1


+

τ

2


=
1


{\displaystyle \tau _{1}+\tau _{2}=1}

:
 This has the same form as the maximum likelihood estimate for the binomial distribution, so
 For the next estimates of 



(


μ


1


,

Σ

1


)


{\displaystyle ({\boldsymbol {\mu }}_{1},\Sigma _{1})}

:
 This has the same form as a weighted maximum likelihood estimate for a normal distribution, so
 and, by symmetry,
 Conclude the iterative process if 




E

Z
∣

θ

(
t
)


,

x



[
log
⁡
L
(

θ

(
t
)


;

x

,

Z

)
]
≤

E

Z
∣

θ

(
t
−
1
)


,

x



[
log
⁡
L
(

θ

(
t
−
1
)


;

x

,

Z

)
]
+
ε


{\displaystyle E_{Z\mid \theta ^{(t)},\mathbf {x} }[\log L(\theta ^{(t)};\mathbf {x} ,\mathbf {Z} )]\leq E_{Z\mid \theta ^{(t-1)},\mathbf {x} }[\log L(\theta ^{(t-1)};\mathbf {x} ,\mathbf {Z} )]+\varepsilon }

 for 



ε


{\displaystyle \varepsilon }

 below some preset threshold.
 The algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions.
 The EM algorithm has been implemented in the case where an underlying linear regression model exists explaining the variation of some quantity, but where the values actually observed are censored or truncated versions of those represented in the model.[38] Special cases of this model include censored or truncated observations from one normal distribution.[38]
 EM typically converges to a local optimum, not necessarily the global optimum, with no bound on the convergence rate in general. It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima. Hence, a need exists for alternative methods for guaranteed learning, especially in the high-dimensional setting. Alternatives to EM exist with better guarantees for consistency, which are termed moment-based approaches[39] or the so-called spectral techniques[40][41][citation needed]. Moment-based approaches to learning the parameters of a probabilistic model are of increasing interest recently[when?] since they enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima. Algorithms with guarantees for learning can be derived for a number of important models such as mixture models, HMMs etc. For these spectral methods, no spurious local optima occur, and the true parameters can be consistently estimated under some regularity conditions[citation needed].
"
"Bayesian inference (/ˈbeɪziən/ BAY-zee-ən or /ˈbeɪʒən/ BAY-zhən)[1] is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Fundamentally, Bayesian inference uses prior knowledge, in the form of a prior distribution in order to estimate posterior probabilities. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called ""Bayesian probability"".
 Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a ""likelihood function"" derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes' theorem:




P
(
H
∣
E
)
=



P
(
E
∣
H
)
⋅
P
(
H
)


P
(
E
)



,


{\displaystyle P(H\mid E)={\frac {P(E\mid H)\cdot P(H)}{P(E)}},}


where
 For different values of 



H


{\displaystyle H}

, only the factors 



P
(
H
)


{\displaystyle P(H)}

 and 



P
(
E
∣
H
)


{\displaystyle P(E\mid H)}

, both in the numerator, affect the value of 



P
(
H
∣
E
)


{\displaystyle P(H\mid E)}

 –  the posterior probability of a hypothesis is proportional to its prior probability (its inherent likeliness) and the newly acquired likelihood (its compatibility with the new observed evidence).
 In cases where 



¬
H


{\displaystyle \neg H}

 (""not 



H


{\displaystyle H}

""), the logical negation of 



H


{\displaystyle H}

, is a valid likelihood, Bayes' rule can be rewritten as follows:








P
(
H
∣
E
)



=



P
(
E
∣
H
)
P
(
H
)


P
(
E
)









=



P
(
E
∣
H
)
P
(
H
)


P
(
E
∣
H
)
P
(
H
)
+
P
(
E
∣
¬
H
)
P
(
¬
H
)









=


1

1
+

(



1

P
(
H
)



−
1

)




P
(
E
∣
¬
H
)


P
(
E
∣
H
)












{\displaystyle {\begin{aligned}P(H\mid E)&={\frac {P(E\mid H)P(H)}{P(E)}}\\&={\frac {P(E\mid H)P(H)}{P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}}\\&={\frac {1}{1+\left({\frac {1}{P(H)}}-1\right){\frac {P(E\mid \neg H)}{P(E\mid H)}}}}\end{aligned}}}


because




P
(
E
)
=
P
(
E
∣
H
)
P
(
H
)
+
P
(
E
∣
¬
H
)
P
(
¬
H
)


{\displaystyle P(E)=P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}


and




P
(
H
)
+
P
(
¬
H
)
=
1.


{\displaystyle P(H)+P(\neg H)=1.}


 One quick and easy way to remember the equation would be to use rule of multiplication:




P
(
E
∩
H
)
=
P
(
E
∣
H
)
P
(
H
)
=
P
(
H
∣
E
)
P
(
E
)
.


{\displaystyle P(E\cap H)=P(E\mid H)P(H)=P(H\mid E)P(E).}


 Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered rational.
 Ian Hacking noted that traditional ""Dutch book"" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. Hacking wrote:[2] ""And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour.""
 Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on ""probability kinematics"") following the publication of Richard C. Jeffrey's rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability.[3] The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.[4]
 If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.
 Suppose a process is generating independent and identically distributed events 




E

n


,
 
n
=
1
,
2
,
3
,
…


{\displaystyle E_{n},\ n=1,2,3,\ldots }

, but the probability distribution is unknown. Let the event space 



Ω


{\displaystyle \Omega }

 represent the current state of belief for this process. Each model is represented by event 




M

m




{\displaystyle M_{m}}

. The conditional probabilities 



P
(

E

n


∣

M

m


)


{\displaystyle P(E_{n}\mid M_{m})}

 are specified to define the models. 



P
(

M

m


)


{\displaystyle P(M_{m})}

 is the degree of belief in 




M

m




{\displaystyle M_{m}}

. Before the first inference step, 



{
P
(

M

m


)
}


{\displaystyle \{P(M_{m})\}}

 is a set of initial prior probabilities. These must sum to 1, but are otherwise arbitrary.
 Suppose that the process is observed to generate 



E
∈
{

E

n


}


{\displaystyle E\in \{E_{n}\}}

. For each 



M
∈
{

M

m


}


{\displaystyle M\in \{M_{m}\}}

, the prior 



P
(
M
)


{\displaystyle P(M)}

 is updated to the posterior 



P
(
M
∣
E
)


{\displaystyle P(M\mid E)}

. From Bayes' theorem:[5]
 



P
(
M
∣
E
)
=



P
(
E
∣
M
)



∑

m



P
(
E
∣

M

m


)
P
(

M

m


)




⋅
P
(
M
)
.


{\displaystyle P(M\mid E)={\frac {P(E\mid M)}{\sum _{m}{P(E\mid M_{m})P(M_{m})}}}\cdot P(M).}


 Upon observation of further evidence, this procedure may be repeated.
 For a sequence of independent and identically distributed observations 




E

=
(

e

1


,
…
,

e

n


)


{\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})}

, it can be shown by induction that repeated application of the above is equivalent to




P
(
M
∣

E

)
=



P
(

E

∣
M
)



∑

m



P
(

E

∣

M

m


)
P
(

M

m


)




⋅
P
(
M
)
,


{\displaystyle P(M\mid \mathbf {E} )={\frac {P(\mathbf {E} \mid M)}{\sum _{m}{P(\mathbf {E} \mid M_{m})P(M_{m})}}}\cdot P(M),}


where




P
(

E

∣
M
)
=

∏

k



P
(

e

k


∣
M
)

.


{\displaystyle P(\mathbf {E} \mid M)=\prod _{k}{P(e_{k}\mid M)}.}


 By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is, however, equally applicable to discrete distributions.
 Let the vector 




θ



{\displaystyle {\boldsymbol {\theta }}}

 span the parameter space. Let the initial prior distribution over 




θ



{\displaystyle {\boldsymbol {\theta }}}

 be 



p
(

θ

∣

α

)


{\displaystyle p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})}

, where 




α



{\displaystyle {\boldsymbol {\alpha }}}

 is a set of parameters to the prior itself, or hyperparameters. Let 




E

=
(

e

1


,
…
,

e

n


)


{\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})}

 be a sequence of independent and identically distributed event observations, where all 




e

i




{\displaystyle e_{i}}

 are distributed as 



p
(
e
∣

θ

)


{\displaystyle p(e\mid {\boldsymbol {\theta }})}

 for some 




θ



{\displaystyle {\boldsymbol {\theta }}}

. Bayes' theorem is applied to find the posterior distribution over 




θ



{\displaystyle {\boldsymbol {\theta }}}

:
 







p
(

θ

∣

E

,

α

)



=



p
(

E

∣

θ

,

α

)


p
(

E

∣

α

)



⋅
p
(

θ

∣

α

)






=



p
(

E

∣

θ

,

α

)


∫
p
(

E

∣

θ

,

α

)
p
(

θ

∣

α

)

d

θ




⋅
p
(

θ

∣

α

)
,






{\displaystyle {\begin{aligned}p({\boldsymbol {\theta }}\mid \mathbf {E} ,{\boldsymbol {\alpha }})&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{p(\mathbf {E} \mid {\boldsymbol {\alpha }})}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\\&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{\int p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\,d{\boldsymbol {\theta }}}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }}),\end{aligned}}}


where




p
(

E

∣

θ

,

α

)
=

∏

k


p
(

e

k


∣

θ

)
.


{\displaystyle p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})=\prod _{k}p(e_{k}\mid {\boldsymbol {\theta }}).}


 




P

X


y


(
A
)
=
E
(

1

A


(
X
)

|

Y
=
y
)


{\displaystyle P_{X}^{y}(A)=E(1_{A}(X)|Y=y)}

Existence and uniqueness of the needed conditional expectation is a consequence of the Radon–Nikodym theorem. This was formulated by Kolmogorov in his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing ""I wish to call attention to  ... and especially the theory of conditional probabilities and conditional expectations ..."" in the Preface.[8] The Bayes theorem determines the posterior distribution from the prior distribution. Uniqueness requires continuity assumptions.[9][10] Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line.[11] Modern Markov chain Monte Carlo methods have boosted the importance of Bayes' theorem including cases with improper priors.[12]
 Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned.  Only this way is the entire posterior distribution of the parameter(s) used.  By comparison, prediction in frequentist statistics often involves finding an optimum point estimate of the parameter(s)—e.g., by maximum likelihood or maximum a posteriori estimation (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the predictive distribution.
 In some instances, frequentist statistics can work around this problem. For example, confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a Student's t-distribution.  This correctly estimates the variance, due to the facts that (1) the average of normally distributed random variables is also normally distributed, and (2) the predictive distribution of a normally distributed data point with unknown mean and variance, using conjugate or uninformative priors, has a Student's t-distribution. In Bayesian statistics, however, the posterior predictive distribution can always be determined exactly—or at least to an arbitrary level of precision when numerical methods are used.
 Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood). In fact, if the prior distribution is a conjugate prior, such that the prior and posterior distributions come from the same family, it can be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the conjugate prior article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.
 
 






P
(
E
∣
M
)


P
(
E
)



>
1
⇒
P
(
E
∣
M
)
>
P
(
E
)


{\textstyle {\frac {P(E\mid M)}{P(E)}}>1\Rightarrow P(E\mid M)>P(E)}

. That is, if the model were true, the evidence would be more likely than is predicted by the current state of belief. The reverse applies for a decrease in belief. If the belief does not change, 






P
(
E
∣
M
)


P
(
E
)



=
1
⇒
P
(
E
∣
M
)
=
P
(
E
)


{\textstyle {\frac {P(E\mid M)}{P(E)}}=1\Rightarrow P(E\mid M)=P(E)}

. That is, the evidence is independent of the model. If the model were true, the evidence would be exactly as likely as predicted by the current state of belief.
 If 



P
(
M
)
=
0


{\displaystyle P(M)=0}

 then 



P
(
M
∣
E
)
=
0


{\displaystyle P(M\mid E)=0}

. If 



P
(
M
)
=
1


{\displaystyle P(M)=1}

 and 



P
(
E
)
>
0


{\displaystyle P(E)>0}

, then 



P
(
M

|

E
)
=
1


{\displaystyle P(M|E)=1}

. This can be interpreted to mean that hard convictions are insensitive to counter-evidence.
 The former follows directly from Bayes' theorem. The latter can be derived by applying the first rule to the event ""not 



M


{\displaystyle M}

"" in place of ""



M


{\displaystyle M}

"", yielding ""if 



1
−
P
(
M
)
=
0


{\displaystyle 1-P(M)=0}

, then 



1
−
P
(
M
∣
E
)
=
0


{\displaystyle 1-P(M\mid E)=0}

"", from which the result immediately follows.
 Consider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials. For sufficiently nice prior probabilities, the Bernstein-von Mises theorem gives that in the limit of infinite trials, the posterior converges to a Gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by Joseph L. Doob in 1948, namely if the random variable in consideration has a finite probability space. The more general results were obtained later by the statistician David A. Freedman who published in two seminal research papers in 1963 [13] and 1965 [14] when and under what circumstances the asymptotic behaviour of posterior is guaranteed. His 1963 paper treats, like Doob (1949), the finite case and comes to a satisfactory conclusion. However, if the random variable has an infinite but countable probability space (i.e., corresponding to a die with infinite many faces) the 1965 paper demonstrates that for a dense subset of priors the Bernstein-von Mises theorem is not applicable. In this case there is almost surely no asymptotic convergence. Later in the 1980s and 1990s Freedman and Persi Diaconis continued to work on the case of infinite countable probability spaces.[15] To summarise, there may be insufficient trials to suppress the effects of the initial choice, and especially for large (but finite) systems the convergence might be very slow.
 In parameterized form, the prior distribution is often assumed to come from a family of distributions called conjugate priors. The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family, and the calculation may be expressed in closed form.
 It is often desired to use a posterior distribution to estimate a parameter or variable. Several methods of Bayesian estimation select measurements of central tendency from the posterior distribution.
 For one-dimensional problems, a unique median exists for practical continuous problems. The posterior median is attractive as a robust estimator.[16]
 If there exists a finite mean for the posterior distribution, then the posterior mean is a method of estimation.[17]







θ
~



=
E
⁡
[
θ
]
=
∫
θ

p
(
θ
∣

X

,
α
)

d
θ


{\displaystyle {\tilde {\theta }}=\operatorname {E} [\theta ]=\int \theta \,p(\theta \mid \mathbf {X} ,\alpha )\,d\theta }


 Taking a value with the greatest probability defines maximum a posteriori (MAP) estimates:[18]




{

θ

MAP


}
⊂
arg
⁡

max

θ


p
(
θ
∣

X

,
α
)
.


{\displaystyle \{\theta _{\text{MAP}}\}\subset \arg \max _{\theta }p(\theta \mid \mathbf {X} ,\alpha ).}


 There are examples where no maximum is attained, in which case the set of MAP estimates is empty.
 There are other methods of estimation that minimize the posterior risk (expected-posterior loss) with respect to a loss function, and these are of interest to statistical decision theory using the sampling distribution (""frequentist statistics"").[19]
 The posterior predictive distribution of a new observation 






x
~





{\displaystyle {\tilde {x}}}

 (that is independent of previous observations) is determined by[20]




p
(



x
~




|


X

,
α
)
=
∫
p
(



x
~



,
θ
∣

X

,
α
)

d
θ
=
∫
p
(



x
~



∣
θ
)
p
(
θ
∣

X

,
α
)

d
θ
.


{\displaystyle p({\tilde {x}}|\mathbf {X} ,\alpha )=\int p({\tilde {x}},\theta \mid \mathbf {X} ,\alpha )\,d\theta =\int p({\tilde {x}}\mid \theta )p(\theta \mid \mathbf {X} ,\alpha )\,d\theta .}


 Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?
 Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let 




H

1




{\displaystyle H_{1}}

 correspond to bowl #1, and 




H

2




{\displaystyle H_{2}}

 to bowl #2.
It is given that the bowls are identical from Fred's point of view, thus 



P
(

H

1


)
=
P
(

H

2


)


{\displaystyle P(H_{1})=P(H_{2})}

, and the two must add up to 1, so both are equal to 0.5.
The event 



E


{\displaystyle E}

 is the observation of a plain cookie. From the contents of the bowls, we know that 



P
(
E
∣

H

1


)
=
30

/

40
=
0.75


{\displaystyle P(E\mid H_{1})=30/40=0.75}

 and 



P
(
E
∣

H

2


)
=
20

/

40
=
0.5.


{\displaystyle P(E\mid H_{2})=20/40=0.5.}

 Bayes' formula then yields








P
(

H

1


∣
E
)



=



P
(
E
∣

H

1


)

P
(

H

1


)


P
(
E
∣

H

1


)

P
(

H

1


)

+

P
(
E
∣

H

2


)

P
(

H

2


)










 



=



0.75
×
0.5


0.75
×
0.5
+
0.5
×
0.5










 



=
0.6






{\displaystyle {\begin{aligned}P(H_{1}\mid E)&={\frac {P(E\mid H_{1})\,P(H_{1})}{P(E\mid H_{1})\,P(H_{1})\;+\;P(E\mid H_{2})\,P(H_{2})}}\\\\\ &={\frac {0.75\times 0.5}{0.75\times 0.5+0.5\times 0.5}}\\\\\ &=0.6\end{aligned}}}


 Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, 



P
(

H

1


)


{\displaystyle P(H_{1})}

, which was 0.5. After observing the cookie, we must revise the probability to 



P
(

H

1


∣
E
)


{\displaystyle P(H_{1}\mid E)}

, which is 0.6.
 An archaeologist is working at a site thought to be from the medieval period, between the 11th century to the 16th century. However, it is uncertain exactly when in this period the site was inhabited. Fragments of pottery are found, some of which are glazed and some of which are decorated. It is expected that if the site were inhabited during the early medieval period, then 1% of the pottery would be glazed and 50% of its area decorated, whereas if it had been inhabited in the late medieval period then 81% would be glazed and 5% of its area decorated. How confident can the archaeologist be in the date of inhabitation as fragments are unearthed?
 The degree of belief in the continuous variable 



C


{\displaystyle C}

 (century) is to be calculated, with the discrete set of events 



{
G
D
,
G



D
¯



,



G
¯



D
,



G
¯






D
¯



}


{\displaystyle \{GD,G{\bar {D}},{\bar {G}}D,{\bar {G}}{\bar {D}}\}}

 as evidence. Assuming linear variation of glaze and decoration with time, and that these variables are independent,
 



P
(
E
=
G
D
∣
C
=
c
)
=
(
0.01
+



0.81
−
0.01


16
−
11



(
c
−
11
)
)
(
0.5
−



0.5
−
0.05


16
−
11



(
c
−
11
)
)


{\displaystyle P(E=GD\mid C=c)=(0.01+{\frac {0.81-0.01}{16-11}}(c-11))(0.5-{\frac {0.5-0.05}{16-11}}(c-11))}






P
(
E
=
G



D
¯



∣
C
=
c
)
=
(
0.01
+



0.81
−
0.01


16
−
11



(
c
−
11
)
)
(
0.5
+



0.5
−
0.05


16
−
11



(
c
−
11
)
)


{\displaystyle P(E=G{\bar {D}}\mid C=c)=(0.01+{\frac {0.81-0.01}{16-11}}(c-11))(0.5+{\frac {0.5-0.05}{16-11}}(c-11))}






P
(
E
=



G
¯



D
∣
C
=
c
)
=
(
(
1
−
0.01
)
−



0.81
−
0.01


16
−
11



(
c
−
11
)
)
(
0.5
−



0.5
−
0.05


16
−
11



(
c
−
11
)
)


{\displaystyle P(E={\bar {G}}D\mid C=c)=((1-0.01)-{\frac {0.81-0.01}{16-11}}(c-11))(0.5-{\frac {0.5-0.05}{16-11}}(c-11))}






P
(
E
=



G
¯






D
¯



∣
C
=
c
)
=
(
(
1
−
0.01
)
−



0.81
−
0.01


16
−
11



(
c
−
11
)
)
(
0.5
+



0.5
−
0.05


16
−
11



(
c
−
11
)
)


{\displaystyle P(E={\bar {G}}{\bar {D}}\mid C=c)=((1-0.01)-{\frac {0.81-0.01}{16-11}}(c-11))(0.5+{\frac {0.5-0.05}{16-11}}(c-11))}


 Assume a uniform prior of 




f

C


(
c
)
=
0.2


{\textstyle f_{C}(c)=0.2}

, and that trials are independent and identically distributed. When a new fragment of type 



e


{\displaystyle e}

 is discovered, Bayes' theorem is applied to update the degree of belief for each 



c


{\displaystyle c}

:





f

C


(
c
∣
E
=
e
)
=



P
(
E
=
e
∣
C
=
c
)


P
(
E
=
e
)




f

C


(
c
)
=



P
(
E
=
e
∣
C
=
c
)



∫

11


16



P
(
E
=
e
∣
C
=
c
)

f

C


(
c
)
d
c





f

C


(
c
)


{\displaystyle f_{C}(c\mid E=e)={\frac {P(E=e\mid C=c)}{P(E=e)}}f_{C}(c)={\frac {P(E=e\mid C=c)}{\int _{11}^{16}{P(E=e\mid C=c)f_{C}(c)dc}}}f_{C}(c)}


 A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph. In the simulation, the site was inhabited around 1420, or 



c
=
15.2


{\displaystyle c=15.2}

. By calculating the area under the relevant portion of the graph for 50 trials, the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries, about 1% chance that it was inhabited during the 13th century, 63% chance during the 14th century and 36% during the 15th century. The Bernstein-von Mises theorem asserts here the asymptotic convergence to the ""true"" distribution because the probability space corresponding to the discrete set of events 



{
G
D
,
G



D
¯



,



G
¯



D
,



G
¯






D
¯



}


{\displaystyle \{GD,G{\bar {D}},{\bar {G}}D,{\bar {G}}{\bar {D}}\}}

 is finite (see above section on asymptotic behaviour of the posterior).
 A decision-theoretic justification of the use of Bayesian inference was given by Abraham Wald, who proved that every unique Bayesian procedure is admissible. Conversely, every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.[21]
 Wald characterized admissible procedures as Bayesian procedures (and limits of Bayesian procedures), making the Bayesian formalism a central technique in such areas of frequentist inference as parameter estimation, hypothesis testing, and computing confidence intervals.[22][23][24] For example:
 Bayesian methodology also plays a role in model selection where the aim is to select one model from a set of competing models that represents most closely the underlying process that generated the observed data. In Bayesian model comparison, the model with the highest posterior probability given the data is selected. The posterior probability of a model depends on the evidence, or marginal likelihood, which reflects the probability that the data is generated by the model, and on the prior belief of the model. When two competing models are a priori considered to be equiprobable, the ratio of their posterior probabilities corresponds to the Bayes factor. Since Bayesian model comparison is aimed on selecting the model with the highest posterior probability, this methodology is also referred to as the maximum a posteriori (MAP) selection rule [29] or the MAP probability rule.[30]
 While conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving PPLs to handle the computational details for them.[31][32][33]
 See the separate Wikipedia entry on Bayesian statistics, specifically the statistical modeling section in that page.
 Bayesian inference has applications in artificial intelligence and expert systems.  Bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s.[34] There is also an ever-growing connection between Bayesian methods and simulation-based Monte Carlo techniques since complex models cannot be processed in closed form by a Bayesian analysis, while a graphical model structure may allow for efficient simulation algorithms like the Gibbs sampling and other Metropolis–Hastings algorithm schemes.[35] Recently[when?] Bayesian inference has gained popularity among the phylogenetics community for these reasons; a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously.
 As applied to statistical classification, Bayesian inference has been used to develop algorithms for identifying e-mail spam. Applications which make use of Bayesian inference for spam filtering include CRM114, DSPAM, Bogofilter, SpamAssassin, SpamBayes, Mozilla, XEAMS, and others. Spam classification is treated in more detail in the article on the naïve Bayes classifier.
 Solomonoff's Inductive inference is the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. It is a formal inductive framework that combines two well-studied principles of inductive inference: Bayesian statistics and Occam's Razor.[36][unreliable source?] Solomonoff's universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs (for a universal computer) that compute something starting with p. Given some p and any computable but unknown probability distribution from which x is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of x in optimal fashion.[37][38]
 Bayesian inference has been applied in different Bioinformatics applications, including differential gene expression analysis.[39] Bayesian inference is also used in a general cancer risk model, called CIRI (Continuous Individualized Risk Index), where serial measurements are incorporated to update a Bayesian model which is primarily built from prior knowledge.[40][41]
 Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant, and to see whether, in totality, it meets their personal threshold for ""beyond a reasonable doubt"".[42][43][44] Bayes' theorem is applied successively to all evidence presented, with the posterior from one stage becoming the prior for the next. The benefit of a Bayesian approach is that it gives the juror an unbiased, rational mechanism for combining evidence. It may be appropriate to explain Bayes' theorem to jurors in odds form, as betting odds are more widely understood than probabilities. Alternatively, a logarithmic approach, replacing multiplication with addition, might be easier for a jury to handle.
 If the existence of the crime is not in doubt, only the identity of the culprit, it has been suggested that the prior should be uniform over the qualifying population.[45] For example, if 1,000 people could have committed the crime, the prior probability of guilt would be 1/1000.
 The use of Bayes' theorem by jurors is controversial. In the United Kingdom, a defence expert witness explained Bayes' theorem to the jury in R v Adams. The jury convicted, but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes' theorem. The Court of Appeal upheld the conviction, but it also gave the opinion that ""To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task.""
 Gardner-Medwin[46] argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt, but rather the probability of the evidence, given that the defendant is innocent (akin to a frequentist p-value). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:
 Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also Lindley's paradox.
 Bayesian epistemology is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic.
 Karl Popper and David Miller have rejected the idea of Bayesian rationalism, i.e. using Bayes rule to make epistemological inferences:[47] It is prone to the same vicious circle as any other justificationist epistemology, because it presupposes what it attempts to justify. According to this view, a rational interpretation of Bayesian inference would see it merely as a probabilistic version of falsification, rejecting the belief, commonly held by Bayesians, that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt, or even with likelihood greater than 0.
 The problem considered by Bayes in Proposition 9 of his essay, ""An Essay towards solving a Problem in the Doctrine of Chances"", is the posterior distribution for the parameter a (the success rate) of the binomial distribution.[citation needed]
 The term Bayesian refers to Thomas Bayes (1701–1761), who proved that probabilistic limits could be placed on an unknown event.[citation needed]   However, it was Pierre-Simon Laplace (1749–1827) who introduced (as Principle VI) what is now called Bayes' theorem and used it to address problems in celestial mechanics, medical statistics, reliability, and jurisprudence.[55] Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called ""inverse probability"" (because it infers backwards from observations to parameters, or from effects to causes[56]). After the 1920s, ""inverse probability"" was largely supplanted  by a collection of methods that came to be called frequentist statistics.[56]
 In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to objective and subjective currents in Bayesian practice. In the objective or ""non-informative"" current, the statistical analysis depends on only the model assumed, the data analyzed,[57] and the method assigning the prior, which differs from one objective Bayesian practitioner to another. In the subjective or ""informative"" current, the specification of the prior depends on the belief (that is, propositions on which the analysis is prepared to act), which can summarize information from experts, previous studies, etc.
 In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications.[58] Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics.[59] Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of machine learning.[60]
 The following books are listed in ascending order of probabilistic sophistication:
"
"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).[1] While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
 Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
 Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if 



m


{\displaystyle m}

 parent nodes represent 



m


{\displaystyle m}

 Boolean variables, then the probability function could be represented by a table of 




2

m




{\displaystyle 2^{m}}

 entries, one entry for each of the 




2

m




{\displaystyle 2^{m}}

 possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.
 Let us use an illustration to enforce the concepts of a Bayesian network. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).
 The joint probability function is, by the chain rule of probability,
 where G = ""Grass wet (true/false)"", S = ""Sprinkler turned on (true/false)"", and R = ""Raining (true/false)"".
 The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like ""What is the probability that it is raining, given the grass is wet?"" by using the conditional probability formula and summing over all nuisance variables:
 Using the expansion for the joint probability function 



Pr
(
G
,
S
,
R
)


{\displaystyle \Pr(G,S,R)}

 and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,
 Then the numerical results (subscripted by the associated variable values) are
 To answer an interventional question, such as ""What is the probability that it would rain, given that we wet the grass?"" the answer is governed by the post-intervention joint distribution function
 obtained by removing the factor 



Pr
(
G
∣
S
,
R
)


{\displaystyle \Pr(G\mid S,R)}

 from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:
 To predict the impact of turning the sprinkler on:
 with the term 



Pr
(
S
=
T
∣
R
)


{\displaystyle \Pr(S=T\mid R)}

 removed, showing that the action affects the grass but not the rain.
 These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 




do

(
x
)


{\displaystyle {\text{do}}(x)}

 can still be predicted, however, whenever the back-door criterion is satisfied.[2][3] It states that, if a set Z of nodes can be observed that d-separates[4] (or blocks) all back-door paths from X to Y then
 A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called ""sufficient"" or ""admissible."" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not ""identified"". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious
(apparent dependence arising from a common cause, R). (see Simpson's paradox)
 To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of ""do-calculus""[2][5] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[6]
 Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 




2

10


=
1024


{\displaystyle 2^{10}=1024}

 values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most 



10
⋅

2

3


=
80


{\displaystyle 10\cdot 2^{3}=80}

 values.
 One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.
 Bayesian networks perform three main inference tasks:
 Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.
 The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.
 In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)
 Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.
 A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.
 In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.
 Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl[7] and rests on the distinction between the three possible patterns allowed in a 3-node DAG:
 The first 2 represent the same dependencies (



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 are independent given 



Y


{\displaystyle Y}

) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since 



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when 



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[2][8][9][10]
 An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al.[11][12] discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.
 A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes.[13] Such method can handle problems with up to 100 variables.
 In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[14]
 Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[15]
 Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.[16]
 Given data 



x




{\displaystyle x\,\!}

 and parameter 



θ


{\displaystyle \theta }

, a simple Bayesian analysis starts with a prior probability (prior) 



p
(
θ
)


{\displaystyle p(\theta )}

 and likelihood 



p
(
x
∣
θ
)


{\displaystyle p(x\mid \theta )}

 to compute a posterior probability 



p
(
θ
∣
x
)
∝
p
(
x
∣
θ
)
p
(
θ
)


{\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}

.
 Often the prior on 



θ


{\displaystyle \theta }

 depends in turn on other parameters 



φ


{\displaystyle \varphi }

 that are not mentioned in the likelihood. So, the prior 



p
(
θ
)


{\displaystyle p(\theta )}

 must be replaced by a likelihood 



p
(
θ
∣
φ
)


{\displaystyle p(\theta \mid \varphi )}

, and a prior 



p
(
φ
)


{\displaystyle p(\varphi )}

 on the newly introduced parameters 



φ


{\displaystyle \varphi }

 is required, resulting in a posterior probability
 This is the simplest example of a hierarchical Bayes model.
 The process may be repeated; for example, the parameters 



φ


{\displaystyle \varphi }

 may depend in turn on additional parameters 



ψ




{\displaystyle \psi \,\!}

, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.
 Given the measured quantities 




x

1


,
…
,

x

n






{\displaystyle x_{1},\dots ,x_{n}\,\!}

each with normally distributed errors of known standard deviation 



σ




{\displaystyle \sigma \,\!}

,
 Suppose we are interested in estimating the 




θ

i




{\displaystyle \theta _{i}}

. An approach would be to estimate the 




θ

i




{\displaystyle \theta _{i}}

 using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply
 However, if the quantities are related, so that for example the individual 




θ

i




{\displaystyle \theta _{i}}

have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,
 with improper priors 



φ
∼

flat



{\displaystyle \varphi \sim {\text{flat}}}

, 



τ
∼

flat

∈
(
0
,
∞
)


{\displaystyle \tau \sim {\text{flat}}\in (0,\infty )}

. When 



n
≥
3


{\displaystyle n\geq 3}

, this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual 




θ

i




{\displaystyle \theta _{i}}

 will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.
 Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable 



τ




{\displaystyle \tau \,\!}

 in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.
 Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.
 X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:[17]
 where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).
 For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:[17]
 Using the definition above, this can be written as:
 The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.
 X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[18]
 where de(v) is the set of descendants and V \ de(v) is the set of non-descendants of v.
 This can be expressed in terms similar to the first definition, as
 The set of parents is a subset of the set of non-descendants because the graph is acyclic.
 Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.[19]
 The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[18]
 This definition can be made more general by defining the ""d""-separation of two nodes, where d stands for directional.[2] We first define the ""d""-separation of a trail and then we will define the ""d""-separation of two nodes in terms of that.
 Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:
 The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.
 X is a Bayesian network with respect to G if, for any two nodes u, v:
 where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)
 Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:
 are equivalent: that is they impose exactly the same conditional independence requirements.
 A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[2] Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.
 In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard.[20] This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[21] First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.
 At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.[22][23]
 In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[24] developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 



1

/

p
(
n
)


{\displaystyle 1/p(n)}

 where 



p
(
n
)


{\displaystyle p(n)}

 was any polynomial of the number of nodes in the network, 



n


{\displaystyle n}

.
 Notable software for Bayesian networks include:
 The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:[26]
 In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems[28] and Neapolitan's Probabilistic Reasoning in Expert Systems[29] summarized their properties and established them as a field of study.
"
"
 Game theory is the study of mathematical models of strategic interactions among rational agents.[1] It has applications in many fields of social science, used extensively in economics as well as in logic, systems science and computer science.[2] Initially game theory addressed two-person zero-sum games, in which a participant's gains or losses are exactly balanced by the losses and gains of the other participant. In the 1950’s it was extended to the study of non zero-sum games and was eventually game applied to a wide range of behavioral relations, and is now an umbrella term for the science of rational decision making in humans, animals, as well as computers.
 Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum game and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by Theory of Games and Economic Behavior (1944), co-written with Oskar Morgenstern, which considered cooperative games of several players.[3] The second edition provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.[4]
 Game theory was developed extensively in the 1950s, and was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory in 1999, and fifteen game theorists have won the Nobel Prize in economics as of 2020, including most recently Paul Milgrom and Robert B. Wilson.
 Game-theoretic thinking dates back at least to Sun Tzu.[5] In The Art of War, he wrote
 Knowing the other and knowing oneself, In one hundred battles no danger,
 Not knowing the other and knowing oneself, One victory for one loss,
 
Not knowing the other and not knowing oneself, In every battle certain defeat Discussions on the mathematics of games began long before the rise of modern mathematical game theory. Cardano's work Liber de ludo aleae (Book on Games of Chance), which was written around 1564 but published posthumously in 1663, sketches some basic ideas on games of chance. In the 1650s, Pascal and Huygens developed the concept of expectation on reasoning about the structure of games of chance. Pascal argued for equal division when chances are equal while Huygens extended the argument by considering strategies for a player who can make any bet with any opponent so long as its terms are equal.[6] Huygens later published his gambling calculus as De ratiociniis in ludo aleæ (On Reasoning in Games of Chance) in 1657.
 In 1713, a letter attributed to Charles Waldegrave, an active Jacobite and uncle to British diplomat James Waldegrave, analyzed a game called ""le her"".[7][8] Waldegrave provided a minimax mixed strategy solution to a two-person version of the card game, and the problem is now known as Waldegrave problem. In 1838, Antoine Augustin Cournot considered a duopoly and presented a solution that is the Nash equilibrium of the game in his Recherches sur les principes mathématiques de la théorie des richesses (Researches into the Mathematical Principles of the Theory of Wealth).
 In 1913, Ernst Zermelo published Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels (On an Application of Set Theory to the Theory of the Game of Chess), which proved that the optimal chess strategy is strictly determined. This paved the way for more general theorems.[9]
 In 1938, the Danish mathematical economist Frederik Zeuthen proved that the mathematical model had a winning strategy by using Brouwer's fixed point theorem.[10] In his 1938 book Applications aux Jeux de Hasard and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix is symmetric and provided a solution to a non-trivial infinite game (known in English as Blotto game). Borel conjectured the non-existence of mixed-strategy equilibria in finite two-person zero-sum games, a conjecture that was proved false by von Neumann.
 Game theory emerged as a unique field when John von Neumann published the paper On the Theory of Games of Strategy in 1928.[11][12] Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. Von Neumann's work in game theory culminated in his 1944 book Theory of Games and Economic Behavior, co-authored with Oskar Morgenstern.[13] The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of money) as an independent discipline. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. Subsequent work focused primarily on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.[14]
 In 1950, the first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by notable mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy.[15] Around this same time, John Nash developed a criterion for mutual consistency of players' strategies known as the Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n-player, non-zero-sum (not just two-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium in mixed strategies.
 Game theory experienced a flurry of activity in the 1950s, during which the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. The 1950s also saw the first applications of game theory to philosophy and political science.
 In 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium. Later he would introduce trembling hand perfection as well. In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.
 In the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection and common knowledge[a] were introduced and analyzed.
 In 1994, John Nash was awarded the Nobel Memorial Prize in the Economic Sciences for his contribution to game theory. Nash's most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept for non-cooperative games. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy.
 In 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.
 In 2007, Leonid Hurwicz, Eric Maskin, and Roger Myerson were awarded the Nobel Prize in Economics ""for having laid the foundations of mechanism design theory"". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: Game Theory, Analysis of Conflict.[1] Hurwicz introduced and formalized the concept of incentive compatibility.
 In 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics ""for the theory of stable allocations and the practice of market design"". In 2014, the Nobel went to game theorist Jean Tirole.
 A game is cooperative if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is non-cooperative if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).[16]
 Cooperative games are often analyzed through the framework of cooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is different from non-cooperative game theory which focuses on predicting individual players' actions and payoffs by analyzing Nash equilibria.[17][18]
 Cooperative game theory provides a high-level approach as it describes only the structure and payoffs of coalitions, whereas non-cooperative game theory also looks at how strategic interaction will affect the distribution of payoffs. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation.
 A symmetric game is a game where each player earns the same payoff when making the same choice. In other words, the identity of the player does not change the resulting game facing the other player.[19] Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games.
 The most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured in this section's graphic is asymmetric despite having identical strategy sets for both players.
 Zero-sum games (more generally, constant-sum games) are games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit goes to all players in a game, for every combination of strategies, and always adds to zero (more informally, a player benefits only at the equal expense of others).[20] Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.
 Many games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.
 Furthermore, constant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any constant-sum game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called ""the board"") whose losses compensate the players' net winnings.
 Simultaneous games are games where both players move simultaneously, or instead the later players are unaware of the earlier players' actions (making them effectively simultaneous). Sequential games (or dynamic games) are games where players do not make decisions simultaneously, and player's earlier actions affect the outcome and decisions of other players.[21] This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed.
 The difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.
 In short, the differences between sequential and simultaneous games are as follows:
 An important subset of sequential games consists of games of perfect information. A game with perfect information means that all players, at every move in the game, know the previous history of the game and the moves previously made by all other players. An imperfect information game is played when the players do not know all moves already made by the opponent such as a simultaneous move game.[22] Examples of perfect-information games include tic-tac-toe, checkers, chess, and Go.[23][24][25]
 Many card games are games of imperfect information, such as poker and bridge.[26] Perfect information is often confused with complete information, which is a similar concept pertaining to the common knowledge of each player's sequence, strategies, and payoffs throughout gameplay.[27] Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken, whereas perfect information is knowledge of all aspects of the game and players.[28] Games of incomplete information can be reduced, however, to games of imperfect information by introducing ""moves by nature"".[29]
 One of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent's valuation of the object of negotiation, companies may be unaware of their opponent's cost functions, combatants may be unaware of their opponent's strengths, and jurors may be unaware of their colleague's interpretation of the evidence at trial. In some cases, participants may know the character of their opponent well, but may not know how well their opponent knows his or her own character.[30]
 Bayesian game means a strategic game with incomplete information. For a strategic game, decision makers are players, and every player has a group of actions. A core part of the imperfect information specification is the set of states. Every state completely describes a collection of characteristics relevant to the player such as their preferences and details about them. There must be a state for every set of features that some player believes may exist.[31]
 For example, where Player 1 is unsure whether Player 2 would rather date her or get away from her, while Player 2 understands Player 1's preferences as before. To be specific, supposing that Player 1 believes that Player 2 wants to date her under a probability of 1/2 and get away from her under a probability of 1/2 (this evaluation comes from Player 1's experience probably: she faces players who want to date her half of the time in such a case and players who want to avoid her half of the time). Due to the probability involved, the analysis  of this situation requires to understand the player's preference for the draw, even though people are only interested in pure strategic equilibrium.
 Games in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and Go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve some particular problems and answer some general questions.[32]
 Games of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including ""loopy"" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or ""economic"") game theory.[33][34] A typical game that has been solved this way is Hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.[35]
 Research in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha–beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.[32][36]
 Much of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.
 Differential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.
 A particular case of differential games are the games with a random time horizon.[37] In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.
 Evolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.[38]  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest.
 In biology, such models can represent evolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.[39]
 Individual decision problems with stochastic outcomes are sometimes considered ""one-player games"". They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).[40]
 Stochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes ""chance moves"" (""moves by nature"").[41] This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.
 For some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen.[42] (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)
 General models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The ""gold standard"" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.[42]
 These are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.
 The term metagame analysis is also used to refer to a practical approach developed by Nigel Howard,[43] whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.
 Mean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines, and by mathematicians Pierre-Louis Lions and Jean-Michel Lasry.
 The games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome. (Eric Rasmusen refers to these four ""essential elements"" by the acronym ""PAPI"".)[44][45][46][47] A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.
 Most cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.
 The extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualised using game trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree.[48] To solve any extensive form game, backward induction must be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.[49]
 The game pictured consists of two players.  The way this particular game is structured (i.e., with sequential decision making and perfect information), Player 1 ""moves"" first by choosing either F or U (fair or unfair). Next in the sequence, Player 2, who has now observed Player 1's move, can choose to play either A or R  (accept or reject). Once Player 2 has made their choice, the game is considered finished and each player gets their respective payoff, represented in the image as two numbers, where the first number represents Player 1's payoff, and the second number represents Player 2's payoff.  Suppose that Player 1 chooses U and then Player 2 chooses A: Player 1 then gets a payoff of ""eight"" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and Player 2 gets a payoff of ""two"".
 The extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)
 The normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays Up and that Player 2 plays Left. Then Player 1 gets a payoff of 4, and Player 2 gets 3.
 When a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.
 Every extensive-form game has an equivalent normal-form game, however, the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.[50]
 In cooperative game theory the characteristic function lists the payoff of each coalition. The origin of this formulation is in John von Neumann and Oskar Morgenstern's book.[citation needed]
 Formally, a characteristic function is a function 



v
:

2

N


→

R



{\displaystyle v:2^{N}\to \mathbb {R} }

 [51] from the set of all possible coalitions of players to a set of payments, and also satisfies 



v
(
∅
)
=
0


{\displaystyle v(\emptyset )=0}

. The function describes how much collective payoff a set of players can gain by forming a coalition.
 Alternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research.[52] In addition to classical game representations, some of the alternative representations also encode time related aspects.
 As a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.[67]
 Although pre-twentieth-century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name ""game theory"", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 book Evolution and the Theory of Games.[68]
 In addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior.[69] In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic arguments of this type can be found as far back as Plato.[70] An alternative version of game theory, called chemical game theory, represents the player's choices as metaphorical chemical reactant molecules called ""knowlecules"".[71]  Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.
 The primary use of game theory is to describe and model how human populations behave.[citation needed] Some[who?] scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice, human rationality and/or behavior often deviates from the model of rationality as used in game theory. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.[b]
 Some game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).
 Some scholars see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.[citation needed]
 
 Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents.[c][73][74][75] Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers and acquisitions pricing,[76] fair division, duopolies, oligopolies, social network formation, agent-based computational economics,[77][78] general equilibrium, mechanism design,[79][80][81][82][83] and voting systems;[84] and across such broad areas as experimental economics,[85][86][87][88][89] behavioral economics,[90][91][92][93][94][95] information economics,[44][45][46][47] industrial organization,[96][97][98][99] and political economy.[100][101][102][103]
 This research usually focuses on particular sets of strategies known as ""solution concepts"" or ""equilibria"". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.[104][105]
 The payoffs of the game are generally taken to represent the utility of individual players.
 A prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Economists and business professors suggest two primary uses (noted above): descriptive and prescriptive.[69]
 Game theory also has an extensive use in a specific branch or stream of economics – Managerial Economics. One important usage of it in the field of managerial economics is in analyzing strategic interactions between firms.[106] For example, firms may be competing in a market with limited resources, and game theory can help managers understand how their decisions impact their competitors and the overall market outcomes. Game theory can also be used to analyze cooperation between firms, such as in forming strategic alliances or joint ventures. Another use of game theory in managerial economics is in analyzing pricing strategies. For example, firms may use game theory to determine the optimal pricing strategy based on how they expect their competitors to respond to their pricing decisions. Overall, game theory serves as a useful tool for analyzing strategic interactions and decision making in the context of managerial economics.
 The Chartered Institute of Procurement & Supply (CIPS) promotes knowledge and use of game theory within the context of business procurement.[107] CIPS and TWS Partners have conducted a series of surveys designed to explore the understanding, awareness and application of game theory among procurement professionals. Some of the main findings in their third annual survey (2019) include:
 Sensible decision-making is critical for the success of projects.  In project management, game theory is used to model the decision-making process of players, such as investors, project managers, contractors, sub-contractors, governments and customers.  Quite often, these players have competing interests, and sometimes their interests are directly detrimental to other players, making project management scenarios well-suited to be modeled by game theory.
 Piraveenan (2019)[109] in his review provides several examples where game theory is used to model project management scenarios. For instance, an investor typically has several investment options, and each option will likely result in a different project, and thus one of the investment options has to be chosen before the project charter can be produced. Similarly, any large project involving subcontractors, for instance, a construction project, has a complex interplay between the main contractor (the project manager) and subcontractors, or among the subcontractors themselves, which typically has several decision points. For example, if there is an ambiguity in the contract between the contractor and subcontractor, each must decide how hard to push their case without jeopardizing the whole project, and thus their own stake in it. Similarly, when projects from competing organizations are launched, the marketing personnel have to decide what is the best timing and strategy to market the project, or its resultant product or service, so that it can gain maximum traction in the face of competition. In each of these scenarios, the required decisions depend on the decisions of other players who, in some way, have competing interests to the interests of the decision-maker, and thus can ideally be modeled using game theory.
 Piraveenan[109] summarises that two-player games are predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.
 In terms of types of games, both cooperative as well as non-cooperative, normal-form as well as extensive-form, and zero-sum as well as non-zero-sum  are used to model various project management scenarios.
 The application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.[110]
 Early examples of game theory applied to political science are provided by Anthony Downs. In his 1957 book An Economic Theory of Democracy,[111] he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game theory was applied in 1962 to the Cuban Missile Crisis during the presidency of John F. Kennedy.[112]
 It has also been proposed that game theory explains the stability of any form of political government.  Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects.  Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed.  Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime.[113]  Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.[114]
 A game-theoretic explanation for democratic peace is that public and open debate in democracies sends clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.[115]
 However, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.[116]
 Game theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example is Peter John Wood's (2013) research looking into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce greenhouse gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma for the nations.[117]
 Game theory has been used extensively to model decision-making scenarios relevant to defence applications.[118]  Most studies that has applied game theory in defence settings are concerned with Command and Control Warfare, and can be further classified into studies dealing with (i) Resource Allocation Warfare (ii) Information Warfare (iii) Weapons Control Warfare, and (iv) Adversary Monitoring Warfare.[118]  Many of the problems studied are concerned with sensing and tracking, for example a surface ship trying to track a hostile submarine and the submarine trying to evade being tracked, and the interdependent decision making that takes place with regards to bearing, speed, and the sensor technology activated by both vessels. Ho et al [118] provides a concise summary of the state-of-the-art with regards to the use of game theory in defence applications and highlights the benefits and limitations of game theory in the considered scenarios.
 Unlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best-known equilibrium in biology is known as the evolutionarily stable strategy (ESS), first introduced in (Maynard Smith & Price 1973). Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.
 In biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. (Fisher 1930) suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.
 Additionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication.[119] The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's Butterfly Economics).
 Biologists have used the game of chicken to analyze fighting behavior and territoriality.[120]
 According to Maynard Smith, in the preface to Evolution and the Theory of Games, ""paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed"". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.[121]
 One such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival.[122] All of these actions increase the overall fitness of a group, but occur at a cost to the individual.
 Evolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c < b × r, where the cost c to the altruist must be less than the benefit b to the recipient multiplied by the coefficient of relatedness r. The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of 1⁄2, because (on average) an individual shares half of the alleles in its sibling's offspring. Ensuring that enough of a sibling's offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring.[122] The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a coefficient that was 1⁄2 in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.
 Game theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.[123]
 Separately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as games with moving costs and request-answer games.[124] Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.
 The emergence of the Internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory[83] and within it algorithmic mechanism design[82] combine computational algorithm design and analysis of complex systems with economic theory.[125][126][127]
 Game theory has been put to several uses in philosophy. Responding to two papers by W.V.O. Quine (1960, 1967), Lewis (1969) used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis.[128][129] Following Lewis (1969) game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.[130][131]
 Game theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993),[132][133] Skyrms (1990),[134] and Stalnaker (1999).[135]
 The synthesis of game theory with ethics was championed by R. B. Braithwaite [136]. The hope was that rigourous mathematical analysis of game theory might help formalize the more imprecise philosophical discussions. However, this expectation was only materialized to a limited extent. [137]
 In ethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton) [who?] authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see Gauthier (1986) and Kavka (1986)).[d]
 Other authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., Skyrms (1996, 2004) and Sober and Wilson (1998)).
 Since the decision to take a vaccine for a particular disease is often made by individuals, who may consider a range of factors and parameters in making this decision (such as the incidence and prevalence of the disease, perceived and real risks associated with contracting the disease,  mortality rate, perceived and real risks associated with vaccination, and financial cost of vaccination), game theory has been used to model and predict vaccination uptake in a society.[138][139]
 Game theory has multiple applications in the field of AI/ML. It is often used in developing autonomous systems that can make complex decisions in uncertain environment.[140] Some other areas of application of game theory in AI/ML context are as follows - multi-agent system formation, reinforcement learning,[141] mechanism design etc.[142] By using game theory to model the behavior of other agents and anticipate their actions, AI/ML systems can make better decisions and operate more effectively.[143]
 William Poundstone described the game in his 1993 book Prisoner's Dilemma:[144]
 Two members of a criminal gang, A and B, are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communication with their partner. The principal charge would lead to a sentence of ten years in prison; however, the police do not have the evidence for a conviction. They plan to sentence both to two years in prison on a lesser charge but offer each prisoner a Faustian bargain: If one of them confesses to the crime of the principal charge, betraying the other, they will be pardoned and free to leave while the other must serve the entirety of the sentence instead of just two years for the lesser charge.
 The dominant strategy (and therefore the best response to any possible opponent strategy), is to betray the other, which aligns with the sure-thing principle.[145] However, both prisoners staying silent would yield a greater reward for both of them than mutual betrayal.
 The ""battle of the sexes"" is a term used to describe the perceived conflict between men and women in various areas of life, such as relationships, careers, and social roles. This conflict is often portrayed in popular culture, such as movies and television shows, as a humorous or dramatic competition between the genders. This conflict can be depicted in a game theory framework. This is an example of non-cooperative games.
 An example of the ""battle of the sexes"" can be seen in the portrayal of relationships in popular media, where men and women are often depicted as being fundamentally different and in conflict with each other. For instance, in some romantic comedies, the male and female protagonists are shown as having opposing views on love and relationships, and they have to overcome these differences in order to be together.[146]
 In this game, there are two pure strategy Nash equilibria: one where both the players choose the same strategy and the other where the players choose different options. If the game is played in mixed strategies, where each player chooses their strategy randomly, then there is an infinite number of Nash equilibria. However, in the context of the ""battle of the sexes"" game, the assumption is usually made that the game is played in pure strategies.[147]
 The ultimatum game is a game that has become a popular instrument of economic experiments. An early description is by Nobel laureate John Harsanyi in 1961.[148]
 One player, the proposer, is endowed with a sum of money. The proposer is tasked with splitting it with another player, the responder (who knows what the total sum is). Once the proposer communicates his decision, the responder may accept it or reject it. If the responder accepts, the money is split per the proposal; if the responder rejects, both players receive nothing.  Both players know in advance the consequences of the responder accepting or rejecting the offer. The game demonstrates how social acceptance, fairness, and generosity influence the players decisions.[149]
 Ultimatum game has a variant, that is the dictator game. They are mostly identical, except in dictator game the responder has no power to reject the proposer's offer.
 The Trust Game is an experiment designed to measure trust in economic decisions. It is also called ""the investment game"" and is designed to investigate trust and demonstrate its importance rather than ""rationality"" of self-interest. The game was designed by Berg Joyce, John Dickhaut and Kevin McCabe in 1995.[150]
 In the game, one player (the investor) is given a sum of money and must decide how much of it to give to another player (the trustee). The amount given is then tripled by the experimenter. The trustee then decides how much of the tripled amount to return to the investor. If the recipient is completely self interested, then he/she should return nothing. However that is not true as the experiment conduct. The outcome suggest that people are willing to place a trust, by risking some amount of money, in the belief that there would be reciprocity.[151]
 The Cournot competition model involves players choosing quantity of a homogenous product to produce independently and simultaneously, where marginal cost can be different for each firm and the firm's payoff is profit. The production costs are public information and the firm aims to find their profit-maximizing quantity based on what they believe the other firm will produce and behave like monopolies. In this game firms want to produce at the monopoly quantity but there is a high incentive to deviate and produce more, which decreases the market-clearing price.[22] For example, firms may be tempted to deviate from the monopoly quantity if there is a low monopoly quantity and high price, with the aim of increasing production to maximize  profit.[22] However this option does not provide the highest payoff, as a firm's ability to maximize profits depends on its market share and the elasticity of the market demand.[152] The Cournot equilibrium is reached when each firm operates on their reaction function with no incentive to deviate, as they have the best response based on the other firms output.[22] Within the game, firms reach the Nash equilibrium when the Cournot equilibrium is achieved.   
 The Bertrand competition assumes homogenous products and a constant marginal cost and players choose the prices.[22] The equilibrium of price competition is where the price is equal to marginal costs, assuming complete information about the competitors' costs. Therefore, the firms have an incentive to deviate from the equilibrium because a homogenous product with a lower price will gain all of the market share, known as a cost advantage.[153]
 Lists
 
"
